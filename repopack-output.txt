This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2025-12-10T13:38:29.950Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
feature_cache/
  __init__.py
  extract.py
  load_hf.py
features/
  base.py
  bert_surprisal.py
  character_arcs.py
  emotional.py
  gc_academic.py
  gc_base.py
  gc_basic.py
  gc_char_diversity.py
  gc_concreteness.py
  gc_dialogue.py
  gc_discourse.py
  gc_extractor.py
  gc_narrative.py
  gc_polarity.py
  gc_pos.py
  gc_pronouns.py
  gc_punctuation.py
  gc_readability.py
  gc_syntax.py
  gc_temporal.py
  gc_wrapper.py
  ngram_surprisal.py
  ngram.py
  plot_shifts.py
  rst.py
  structure.py
  surprisal.py
data.py
pca_ensemble.py
pca_utils.py
README.md
requirements.txt
run_lr_base.sh
run_lr_bert.sh
run_lr_gpt.sh
run_lr_rfecv.sh
run_lr.sh
train_lr_rfecv.py
train_pca_ensemble.py

================================================================
Repository Files
================================================================

================
File: feature_cache/__init__.py
================
"""Feature cache package for incremental feature extraction and loading."""

from feature_cache.extract import (
    FEATURE_EXTRACTORS,
    GROUP_SHORTCUTS,
    extract_group,
    extract_multiple_groups,
)

from feature_cache.load_hf import (
    load_groups,
    load_feature_matrix,
    list_available_groups,
    print_cache_info,
)

__all__ = [
    # Extract
    "FEATURE_EXTRACTORS",
    "GROUP_SHORTCUTS",
    "extract_group",
    "extract_multiple_groups",
    # Load
    "load_groups",
    "load_feature_matrix",
    "list_available_groups",
    "print_cache_info",
]

================
File: feature_cache/extract.py
================
"""
Per-group incremental feature extraction for MENSA scene saliency.

This module extracts features one group at a time and saves them separately,
allowing incremental additions without recomputing existing groups.

Architecture:
    features/
        train/
            base.parquet
            narrative.parquet
            polarity.parquet
            ...
        validation/
            base.parquet
            narrative.parquet
            ...
        test/
            base.parquet
            ...
"""

from __future__ import annotations

from typing import Callable, Dict
from pathlib import Path
import pandas as pd
import numpy as np
from tqdm.auto import tqdm

from data import load_mensa_dataframe

# Import from organized features/ directory
from features.base import (
    add_ttr_features, 
    add_length_structure_features, 
    add_position_overlap_features,
    add_similarity_change_features,
    add_structural_position_features
)
from features.character_arcs import add_entity_character_features, add_character_arc_features
from features.surprisal import SurprisalComputer
from features.emotional import add_emotional_trajectory_features
from features.ngram import add_ngram_features
from features.rst import add_rst_features
from features.bert_surprisal import add_bert_surprisal_features
from features.ngram_surprisal import add_ngram_surprisal_features

# Genre Classifier features - now using features/gc_wrapper.py
from features.gc_wrapper import add_gc_features



def extract_base(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """
    Extract base features: TTR, readability, length, structure, position, entity.
    
    Returns DataFrame with movie_id, scene_index, and feature columns.
    """
    result = df[["movie_id", "scene_index"]].copy()
    
    # TTR
    temp = add_ttr_features(df, text_col="scene_text")
    result["ttr"] = temp["ttr"]
    
    # Readability
    temp = add_readability_features(df, text_col="scene_text")
    for col in ["flesch_reading_ease", "flesch_kincaid_grade", "gunning_fog", "smog_index", "automated_readability_index"]:
        if col in temp.columns:
            result[col] = temp[col]
    
    # Length and structure
    temp = add_length_structure_features(df, text_col="scene_text")
    for col in ["sentence_count", "token_count", "avg_sentence_len", "var_sentence_len", 
                "exclaim_rate", "question_rate", "uppercase_ratio", "dialogue_ratio"]:
        if col in temp.columns:
            result[col] = temp[col]
    
    # Position and overlap
    temp = add_position_overlap_features(df)
    for col in ["scene_index_norm", "overlap_prev", "overlap_next"]:
        if col in temp.columns:
            result[col] = temp[col]
    
    # Entity features
    temp = add_entity_character_features(df, text_col="scene_text")
    for col in ["unique_PERSON_count", "top_character_mention_rate", "pronoun_ratio", "name_repetition_rate"]:
        if col in temp.columns:
            result[col] = temp[col]
    
    return result


def extract_gc_polarity(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """Extract Genre Classifier polarity features."""
    temp = add_gc_features(df, text_col="scene_text", groups=["polarity"])
    # Direct extraction: just drop scene_text, keep everything else
    return temp.drop(columns=['scene_text']).copy()


def extract_gc_concreteness(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """Extract Genre Classifier concreteness features."""
    temp = add_gc_features(df, text_col="scene_text", groups=["concreteness"])
    # Direct extraction: just drop scene_text, keep everything else
    return temp.drop(columns=['scene_text']).copy()


def extract_gc_narrative(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """Extract Genre Classifier narrative features."""
    temp = add_gc_features(df, text_col="scene_text", groups=["narrative"])
    return temp.drop(columns=['scene_text']).copy()


def extract_gc_temporal(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """Extract Genre Classifier temporal features."""
    temp = add_gc_features(df, text_col="scene_text", groups=["temporal"])
    return temp.drop(columns=['scene_text']).copy()


def extract_gc_dialogue(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """Extract Genre Classifier dialogue features."""
    temp = add_gc_features(df, text_col="scene_text", groups=["dialogue"])
    return temp.drop(columns=['scene_text']).copy()


def extract_gc_discourse(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """Extract Genre Classifier discourse features."""
    temp = add_gc_features(df, text_col="scene_text", groups=["discourse"])
    return temp.drop(columns=['scene_text']).copy()


def extract_gc_pronouns(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """Extract Genre Classifier pronoun features."""
    temp = add_gc_features(df, text_col="scene_text", groups=["pronouns"])
    return temp.drop(columns=['scene_text']).copy()


def extract_gc_syntax(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """Extract Genre Classifier syntax features."""
    temp = add_gc_features(df, text_col="scene_text", groups=["syntax"])
    return temp.drop(columns=['scene_text']).copy()


def extract_gc_pos(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """Extract Genre Classifier POS features."""
    temp = add_gc_features(df, text_col="scene_text", groups=["pos"])
    return temp.drop(columns=['scene_text']).copy()


def extract_gc_basic(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """Extract Genre Classifier basic features (avg_sen_len, std_sen_len, lex_den)."""
    temp = add_gc_features(df, text_col="scene_text", groups=["basic"])
    return temp.drop(columns=['scene_text']).copy()


def extract_gc_char_diversity(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """Extract Genre Classifier character diversity features (TTR, MAAS, MSTTR, etc.)."""
    temp = add_gc_features(df, text_col="scene_text", groups=["char_diversity"])
    return temp.drop(columns=['scene_text']).copy()


def extract_gc_punctuation(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """Extract Genre Classifier punctuation features."""
    temp = add_gc_features(df, text_col="scene_text", groups=["punctuation"])
    return temp.drop(columns=['scene_text']).copy()


def extract_gc_academic(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """Extract Genre Classifier academic features (citations, numbers, passive voice)."""
    temp = add_gc_features(df, text_col="scene_text", groups=["academic"])
    return temp.drop(columns=['scene_text']).copy()


def extract_gc_readability(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """Extract Genre Classifier readability features (Flesch, Gunning Fog, etc.)."""
    temp = add_gc_features(df, text_col="scene_text", groups=["readability"])
    return temp.drop(columns=['scene_text']).copy()


def extract_surprisal(df: pd.DataFrame, lm_name: str = "distilgpt2", **kwargs) -> pd.DataFrame:
    """Extract surprisal features."""
    result = df[["movie_id", "scene_index"]].copy()
    
    surprisal = SurprisalComputer(model_name=lm_name)
    s_means, s_stds, s_cvs, s_p75s, s_maxs, s_slopes = [], [], [], [], [], []
    
    for text in tqdm(df["scene_text"].tolist(), desc="Surprisal"):
        feats = surprisal.scene_surprisal_features(text)
        s_means.append(feats.get("surprisal_mean", 0.0))
        s_stds.append(feats.get("surprisal_std", 0.0))
        s_cvs.append(feats.get("surprisal_cv", 0.0))
        s_p75s.append(feats.get("surprisal_p75", 0.0))
        s_maxs.append(feats.get("surprisal_max", 0.0))
        s_slopes.append(feats.get("surprisal_slope", 0.0))
    
    result["surprisal_mean"] = s_means
    result["surprisal_std"] = s_stds
    result["surprisal_cv"] = s_cvs
    result["surprisal_p75"] = s_p75s
    result["surprisal_max"] = s_maxs
    result["surprisal_slope"] = s_slopes
    
    return result


def extract_emotional(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """Extract emotional trajectory features."""
    result = df[["movie_id", "scene_index"]].copy()
    temp = add_emotional_trajectory_features(df, text_col="scene_text")
    emo_cols = [c for c in temp.columns if c.startswith("emo_")]
    for col in emo_cols:
        result[col] = temp[col]
    return result


def extract_ngram(df: pd.DataFrame, n: int = 3, **kwargs) -> pd.DataFrame:
    """Extract n-gram features."""
    result = df[["movie_id", "scene_index"]].copy()
    temp = add_ngram_features(df, text_col="scene_text", n=n)
    ngram_cols = [c for c in temp.columns if c.startswith("ngram_")]
    for col in ngram_cols:
        result[col] = temp[col]
    return result



def extract_character_arcs(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """
    Extract character transition features.
    
    Features:
    - char_new_introductions: First-time character appearances
    - char_returns: Characters returning after absence (payoff moments!)
    - char_callbacks: Characters from opening scene returning
    - char_turnover: Character lineup change rate
    """
    result = df[["movie_id", "scene_index"]].copy()
    
    # Need both entity and arc features
    temp = add_entity_character_features(df, text_col="scene_text")
    temp = add_character_arc_features(temp, text_col="scene_text")
    
    arc_cols = ["char_new_introductions", "char_returns", "char_callbacks", "char_turnover"]
    for col in arc_cols:
        if col in temp.columns:
            result[col] = temp[col]
    
    return result


def extract_plot_shifts(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """
    Extract similarity change features (plot shift detection).
    
    Features:
    - sim_change_magnitude: How different is current vs trend
    - vocab_novelty: New vocabulary appearing (reveals/twists)
    - dialogue_shift: Sudden dialogue amount changes
    """
    result = df[["movie_id", "scene_index"]].copy()
    
    temp = add_similarity_change_features(df, text_col="scene_text")
    
    shift_cols = ["sim_change_magnitude", "vocab_novelty", "dialogue_shift"]
    for col in shift_cols:
        if col in temp.columns:
            result[col] = temp[col]
    
    return result


def extract_structure(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """
    Extract structural position features.
    
    Features:
    - pos_edge_proximity: U-shaped importance (edges salient)
    - pos_act: Which act (1/2/3)
    - pos_within_act: Position within current act
    - callback_to_opening: Thematic callbacks to opening
    - callback_to_ending: Foreshadowing of ending
    """
    result = df[["movie_id", "scene_index"]].copy()
    
    temp = add_structural_position_features(df, text_col="scene_text")
    
    struct_cols = [
        "pos_edge_proximity", "pos_act", "pos_within_act",
        "callback_to_opening", "callback_to_ending"
    ]
    for col in struct_cols:
        if col in temp.columns:
            result[col] = temp[col]
    
    return result


def extract_rst(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """
    Extract RST (Rhetorical Structure Theory) structural features.
    
    Features:
    - Tree structure metrics (depth, nodes, branching)
    - Nuclearity distributions
    - Relation type frequencies (30 relations)
    - Complexity metrics (entropy, balance)
    """
    result = df[["movie_id", "scene_index"]].copy()
    
    temp = add_rst_features(df, text_col="scene_text")
    
    rst_cols = [c for c in temp.columns if c.startswith("rst_")]
    for col in rst_cols:
        result[col] = temp[col]
    
    return result


def extract_bert_surprisal(df: pd.DataFrame, **kwargs) -> pd.DataFrame:
    """
    Extract BERT-large surprisal features.
    
    Features:
    - Surprisal statistics (mean, median, std, IQR, etc.)
    - Perplexity and extremes
    - Position-based surprisal
    - Tokenization metrics
    """
    result = df[["movie_id", "scene_index"]].copy()
    
    temp = add_bert_surprisal_features(df, text_col="scene_text")
    
    # Get bert surprisal columns (not bert embeddings which start with bert_)
    bert_surp_cols = [c for c in temp.columns if c.startswith("bert_") and c not in result.columns]
    for col in bert_surp_cols:
        result[col] = temp[col]
    
    return result


def extract_ngram_surprisal(df: pd.DataFrame, model_paths: dict = None, max_order: int = 5, **kwargs) -> pd.DataFrame:
    """
    Extract n-gram language model surprisal features.
    
    Features:
    - For each n-gram order (1-5): 17 features
      - Sentence-level surprisal statistics (mean, median, std, var, cv)
      - Extremes (min, max, range, p25, p75, iqr)
      - Perplexity metrics
      - Position-based features (first/last quarter comparison)
      - Temporal trend (slope)
    - Total: 86 features (17 Ã— 5 orders + 1 count)
    """
    result = df[["movie_id", "scene_index"]].copy()
    
    temp = add_ngram_surprisal_features(df, text_col="scene_text", model_paths=model_paths, max_order=max_order)
    
    # Get ngram surprisal columns
    ngram_surp_cols = [c for c in temp.columns if c.startswith("ngram_") and c not in result.columns]
    for col in ngram_surp_cols:
        result[col] = temp[col]
    
    return result


# Registry of all feature extraction functions
FEATURE_EXTRACTORS: Dict[str, Callable] = {
    "base": extract_base,
    # Genre Classifier groups (all 14) - ordered starting with syntax
    "gc_syntax": extract_gc_syntax,
    "gc_pos": extract_gc_pos,
    "gc_basic": extract_gc_basic,
    "gc_char_diversity": extract_gc_char_diversity,
    "gc_polarity": extract_gc_polarity,
    "gc_concreteness": extract_gc_concreteness,
    "gc_narrative": extract_gc_narrative,
    "gc_temporal": extract_gc_temporal,
    "gc_dialogue": extract_gc_dialogue,
    "gc_discourse": extract_gc_discourse,
    "gc_pronouns": extract_gc_pronouns,
    "gc_punctuation": extract_gc_punctuation,
    "gc_academic": extract_gc_academic,
    "gc_readability": extract_gc_readability,
    # Advanced features
    "surprisal": extract_surprisal,
    "emotional": extract_emotional,
    "ngram": extract_ngram,
    # NEW: Narrative structure features
    "character_arcs": extract_character_arcs,
    "plot_shifts": extract_plot_shifts,
    "structure": extract_structure,
    # NEW: Advanced linguistic features
    "rst": extract_rst,
    "bert_surprisal": extract_bert_surprisal,
    "ngram_surprisal": extract_ngram_surprisal,
}


# Shortcut groups
GROUP_SHORTCUTS: Dict[str, list[str]] = {
    "gc_sentiment": ["gc_polarity", "gc_concreteness"],
    "gc_structural": ["gc_syntax", "gc_pos"],
    "gc_narrative_all": ["gc_narrative", "gc_temporal", "gc_dialogue"],
    "gc_linguistic": ["gc_basic", "gc_char_diversity", "gc_pos", "gc_syntax"],
    "gc_style": ["gc_pronouns", "gc_punctuation", "gc_discourse"],
    "gc_all": [
        "gc_syntax", "gc_pos", "gc_basic", "gc_char_diversity",
        "gc_polarity", "gc_concreteness", "gc_narrative", "gc_temporal",
        "gc_dialogue", "gc_discourse", "gc_pronouns", "gc_punctuation", 
        "gc_academic", "gc_readability"
    ],
    "core": ["base"],
    # NEW: Narrative structure shortcuts
    "narrative": ["character_arcs", "plot_shifts", "structure"],
    "plot_structure": ["character_arcs", "plot_shifts", "structure", "emotional"],
    "all_narrative": ["base", "character_arcs", "plot_shifts", "structure", "emotional"],
    "advanced": ["rst", "bert_surprisal", "ngram_surprisal"],
    "surprisal_all": ["surprisal", "bert_surprisal", "ngram_surprisal"],
    "fast": ["base", "gc_all", "emotional", "ngram", "character_arcs", "plot_shifts", "structure"],
    "all": list(FEATURE_EXTRACTORS.keys()),
}


def extract_group(
    df: pd.DataFrame,
    group_name: str,
    cache_dir: str | Path,
    split: str,
    **kwargs
) -> pd.DataFrame:
    """
    Extract features for a single group and save to cache.
    
    Args:
        df: DataFrame with scene_text, movie_id, scene_index, label
        group_name: Name of feature group to extract
        cache_dir: Base cache directory
        split: Dataset split (train, validation, test)
        **kwargs: Additional arguments for extractor (e.g., lm_name, n)
        
    Returns:
        DataFrame with movie_id, scene_index, and feature columns
    """
    cache_dir = Path(cache_dir)
    split_dir = cache_dir / split
    split_dir.mkdir(parents=True, exist_ok=True)
    
    output_path = split_dir / f"{group_name}.parquet"
    
    # Check if already exists
    if output_path.exists():
        print(f"âš  {group_name} already exists at {output_path}")
        print(f"  Delete it first if you want to regenerate")
        existing_df = pd.read_parquet(output_path)
        # Show info about existing file
        feature_cols = [c for c in existing_df.columns if c not in ['movie_id', 'scene_index', 'label']]
        print(f"  Existing features: {len(feature_cols)}")
        return existing_df
    
    # Extract features
    if group_name not in FEATURE_EXTRACTORS:
        raise ValueError(f"Unknown group: {group_name}. Available: {list(FEATURE_EXTRACTORS.keys())}")
    
    print(f"\n{'='*80}")
    print(f"Extracting {group_name} for {split}...")
    print(f"{'='*80}")
    
    extractor = FEATURE_EXTRACTORS[group_name]
    features_df = extractor(df, **kwargs)
    
    # Get feature columns (exclude movie_id, scene_index, and label)
    feature_cols = [c for c in features_df.columns if c not in ['movie_id', 'scene_index', 'label']]
    
    # Print detailed feature information
    print(f"\nâœ“ Extraction complete for {group_name}")
    print(f"  Samples: {len(features_df)}")
    print(f"  Total columns: {len(features_df.columns)}")
    print(f"  Feature columns: {len(feature_cols)}")
    
    # Show feature list
    if len(feature_cols) > 0:
        print(f"\n  Features extracted ({len(feature_cols)}):")
        # Group features for better display
        if len(feature_cols) <= 20:
            for feat in feature_cols:
                print(f"    â€¢ {feat}")
        else:
            # Show first 15 and last 5
            for feat in feature_cols[:15]:
                print(f"    â€¢ {feat}")
            print(f"    ... ({len(feature_cols) - 20} more features)")
            for feat in feature_cols[-5:]:
                print(f"    â€¢ {feat}")
    
    # IMPORTANT: Drop label column before saving - it should not be in feature cache
    if 'label' in features_df.columns:
        features_df = features_df.drop(columns=['label'])
    
    # Save to cache
    features_df.to_parquet(output_path, index=False, compression="snappy")
    
    size_mb = output_path.stat().st_size / (1024 * 1024)
    print(f"\n  ðŸ’¾ Saved to: {output_path}")
    print(f"  ðŸ“Š File size: {size_mb:.2f} MB")
    print(f"{'='*80}\n")
    
    return features_df


def extract_multiple_groups(
    split: str,
    groups: list[str],
    cache_dir: str | Path = "/scratch/ishaan.karan/features",
    **kwargs
) -> None:
    """
    Extract multiple feature groups for a split.
    
    Args:
        split: Dataset split name
        groups: List of group names to extract
        cache_dir: Base cache directory
        **kwargs: Additional arguments for extractors
    """
    print(f"\n{'='*80}")
    print(f"EXTRACTING GROUPS FOR {split.upper()} SPLIT")
    print(f"{'='*80}")
    print(f"Groups: {', '.join(groups)}")
    print(f"Cache dir: {cache_dir}/{split}/")
    print(f"{'='*80}\n")
    
    # Load data once
    print(f"Loading {split} data from MENSA...")
    df = load_mensa_dataframe(split)
    print(f"âœ“ Loaded {len(df)} samples\n")
    
    # Extract each group
    for i, group in enumerate(groups, 1):
        print(f"[{i}/{len(groups)}] {group}")
        try:
            extract_group(df, group, cache_dir, split, **kwargs)
        except Exception as e:
            print(f"âœ— Error extracting {group}: {e}")
        print()
    
    print(f"{'='*80}")
    print(f"EXTRACTION COMPLETE FOR {split.upper()}")
    print(f"{'='*80}\n")

================
File: feature_cache/load_hf.py
================
"""
Load cached features from Hugging Face Hub.

This module provides functions to load pre-extracted features from Hugging Face Hub.

Usage:
    from feature_cache.load_hf import load_groups
    
    # Load from HuggingFace
    X, y = load_groups(["base", "gc_polarity"], split="train", 
                       hf_repo="username/screenplay-features")
"""

from __future__ import annotations

import os
from pathlib import Path
from typing import List, Optional, Union
import pandas as pd


# Default Hugging Face repo
DEFAULT_HF_REPO = os.environ.get("HF_FEATURE_REPO", "Ishaank18/screenplay-features")

# Feature group shortcuts
GROUP_SHORTCUTS = {
    "gc_sentiment": ["gc_polarity", "gc_pronouns"],
    "all_gc": [
        "gc_academic", "gc_basic", "gc_char_diversity", "gc_concreteness",
        "gc_dialogue", "gc_discourse", "gc_narrative", "gc_polarity",
        "gc_pos", "gc_pronouns", "gc_punctuation", "gc_readability",
        "gc_syntax", "gc_temporal"
    ],
}


def load_groups(
    groups: List[str],
    split: str = "train",
    hf_repo: Optional[str] = None,
    include_label: bool = True,
    verbose: bool = True,
) -> Union[pd.DataFrame, tuple[pd.DataFrame, pd.Series]]:
    """
    Load and merge multiple feature groups from Hugging Face Hub.
    
    Args:
        groups: List of group names to load
        split: Dataset split (train, validation, test)
        hf_repo: Hugging Face repo name (e.g., "username/screenplay-features")
                If None, uses DEFAULT_HF_REPO
        include_label: Whether to include label column
        verbose: Print loading information
        
    Returns:
        If include_label=True: tuple of (X, y) where X is features and y is labels
        If include_label=False: DataFrame with features only
        
    Examples:
        >>> X, y = load_groups(["base", "gc_polarity"], split="train")
        >>> X = load_groups(["gc_sentiment"], split="test", include_label=False)
    """
    # Determine source
    if hf_repo is None:
        hf_repo = DEFAULT_HF_REPO
    
    if not hf_repo:
        raise ValueError("hf_repo must be specified. Provide a Hugging Face repo like 'username/screenplay-features'")
    
    # Expand shortcuts
    expanded_groups = []
    for g in groups:
        if g in GROUP_SHORTCUTS:
            expanded_groups.extend(GROUP_SHORTCUTS[g])
        else:
            expanded_groups.append(g)
    
    # Remove duplicates while preserving order
    expanded_groups = list(dict.fromkeys(expanded_groups))
    
    if verbose:
        print(f"Loading {len(expanded_groups)} feature groups for {split} split...")
    
    # Load from HuggingFace
    dfs = _load_from_hf(expanded_groups, split, hf_repo, verbose)
    
    if len(dfs) == 0:
        raise ValueError("No groups loaded")
    
    # Merge dataframes
    if len(dfs) == 1:
        result = dfs[0]
    else:
        result = _merge_dataframes(dfs, verbose)
    
    if verbose:
        print(f"âœ“ Loaded {len(result)} samples with {len(result.columns)-3} features")
    
    # Handle label
    if include_label:
        # Check if we have labels
        if 'label' not in result.columns:
            # Load labels from MENSA data
            if verbose:
                print("  Loading labels from MENSA dataset...")
            
            try:
                from data import load_mensa_dataframe
                mensa_df = load_mensa_dataframe(split)
                label_df = mensa_df[["movie_id", "scene_index", "label"]]
                
                # Merge labels
                result = result.merge(label_df, on=["movie_id", "scene_index"], how="left")
                
                if verbose:
                    print(f"  âœ“ Loaded labels for {len(result)} samples")
            except Exception as e:
                raise ValueError(f"Label column not found in data and failed to load from MENSA: {e}")
        
        # Return (X, y)
        feature_cols = [c for c in result.columns if c not in ["movie_id", "scene_index", "label"]]
        X = result[feature_cols]
        y = result["label"]
        return X, y
    else:
        # Drop label if present and return features only
        if 'label' in result.columns:
            result = result.drop(columns=['label'])
        
        feature_cols = [c for c in result.columns if c not in ["movie_id", "scene_index"]]
        return result[feature_cols]


def _load_from_hf(
    groups: List[str],
    split: str,
    hf_repo: str,
    verbose: bool = True,
) -> List[pd.DataFrame]:
    """Load feature groups from Hugging Face Hub."""
    
    try:
        from datasets import load_dataset
    except ImportError:
        raise ImportError("datasets library required for HuggingFace loading. Install with: pip install datasets")
    
    if verbose:
        print(f"  Source: Hugging Face ({hf_repo})")
    
    dfs = []
    for group in groups:
        try:
            # Construct file path
            file_path = f"{split}/{group}.parquet"
            
            # Load from HF
            ds = load_dataset(hf_repo, data_files=file_path, split="train")
            df = ds.to_pandas()
            dfs.append(df)
            
            if verbose:
                n_features = len([c for c in df.columns if c not in ["movie_id", "scene_index", "label"]])
                print(f"    âœ“ {group}: {len(df)} samples, {n_features} features")
                
        except Exception as e:
            print(f"    âœ— {group}: Error - {e}")
            raise
    
    return dfs


def _merge_dataframes(dfs: List[pd.DataFrame], verbose: bool = True) -> pd.DataFrame:
    """Merge multiple feature dataframes on movie_id + scene_index."""
    
    result = dfs[0]
    
    # Keep track of label from first df
    has_label = 'label' in result.columns
    if has_label:
        label_col = result['label'].copy()
        result = result.drop(columns=['label'])
    
    # Merge remaining dataframes
    for i, df in enumerate(dfs[1:], 1):
        # Drop label column to avoid duplicates
        if 'label' in df.columns:
            df = df.drop(columns=['label'])
        
        # Check for overlapping feature columns
        overlap = set(result.columns) & set(df.columns)
        overlap.discard('movie_id')
        overlap.discard('scene_index')
        
        if overlap:
            if verbose:
                print(f"  Warning: Dropping {len(overlap)} duplicate columns from group {i}")
            df = df.drop(columns=list(overlap))
        
        # Merge
        result = result.merge(df, on=["movie_id", "scene_index"], how="inner")
    
    # Add label back
    if has_label:
        result['label'] = label_col
    
    return result


def list_available_groups(
    split: str = "train",
    hf_repo: Optional[str] = None,
) -> List[str]:
    """
    List all available feature groups.
    
    Args:
        split: Dataset split
        hf_repo: Hugging Face repo (if None, uses DEFAULT_HF_REPO)
        
    Returns:
        List of available group names
        
    Note:
        This returns a hardcoded list of known groups. 
        For dynamic listing, you would need to use the HuggingFace Hub API.
    """
    # Return all known feature groups
    return [
        "base",
        "bert_surprisal",
        "character_arcs",
        "emotional",
        "gc_academic",
        "gc_basic",
        "gc_char_diversity",
        "gc_concreteness",
        "gc_dialogue",
        "gc_discourse",
        "gc_narrative",
        "gc_polarity",
        "gc_pos",
        "gc_pronouns",
        "gc_punctuation",
        "gc_readability",
        "gc_syntax",
        "gc_temporal",
        "ngram",
        "ngram_surprisal",
        "plot_shifts",
        "rst",
        "structure",
        "surprisal",
    ]


def print_cache_info(hf_repo: Optional[str] = None) -> None:
    """
    Print information about available cache.
    
    Args:
        hf_repo: Hugging Face repo (if None, uses DEFAULT_HF_REPO)
    """
    if hf_repo is None:
        hf_repo = DEFAULT_HF_REPO
    
    print(f"\n{'='*80}")
    print(f"FEATURE CACHE INFORMATION")
    print(f"{'='*80}")
    print(f"Source: Hugging Face Hub")
    print(f"Repo: {hf_repo}")
    print(f"{'='*80}\n")
    
    groups = list_available_groups()
    print(f"Available groups: {len(groups)}")
    print(f"  {', '.join(groups[:10])}")
    if len(groups) > 10:
        print(f"  ... and {len(groups) - 10} more")
    print(f"\n{'='*80}\n")


# Convenience function for backward compatibility
def load_feature_matrix(
    groups: List[str],
    split: str = "train",
    hf_repo: Optional[str] = None,
) -> tuple[pd.DataFrame, pd.Series]:
    """
    Load feature groups and return (X, y).
    
    This is an alias for load_groups with include_label=True.
    """
    return load_groups(groups, split, hf_repo, include_label=True)

================
File: features/base.py
================
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Any, List

import numpy as np
import pandas as pd

import nltk
from tqdm.auto import tqdm

try:
    from nltk.corpus import stopwords
except LookupError:
    nltk.download("stopwords")
    from nltk.corpus import stopwords

# Ensure punkt for sentence tokenization used later for surprisal consistency
try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")

EN_STOPWORDS = set(stopwords.words("english"))


def _tokenize_words(text: str) -> List[str]:
    return [w for w in nltk.word_tokenize(text) if any(c.isalpha() for c in w)]


def type_token_ratio(text: str, exclude_function_words: bool = False) -> float:
    tokens = _tokenize_words(text)
    if exclude_function_words:
        tokens = [t for t in tokens if t.lower() not in EN_STOPWORDS]
    if not tokens:
        return 0.0
    unique = set(t.lower() for t in tokens)
    return float(len(unique)) / float(len(tokens))


def add_ttr_features(df: pd.DataFrame, text_col: str = "scene_text") -> pd.DataFrame:
    df = df.copy()
    df["ttr"] = [
        type_token_ratio(x, exclude_function_words=False)
        for x in tqdm(df[text_col].tolist(), desc="TTR")
    ]
    # Drop ttr_no_func per pruning suggestion
    return df


def _sentence_tokenize(text: str) -> List[str]:
    try:
        return nltk.sent_tokenize(text)
    except Exception:
        return [text]


def add_length_structure_features(df: pd.DataFrame, text_col: str = "scene_text") -> pd.DataFrame:
    df = df.copy()
    texts = df[text_col].tolist()
    sent_counts: List[int] = []
    token_counts: List[int] = []
    avg_sent_len: List[float] = []
    var_sent_len: List[float] = []
    exclaim_rate: List[float] = []
    question_rate: List[float] = []
    uppercase_ratio: List[float] = []
    dialogue_ratio: List[float] = []
    for t in tqdm(texts, desc="Len/Struct"):
        sents = _sentence_tokenize(t or "")
        sents = [s for s in sents if s.strip()]
        sent_counts.append(len(sents))
        words = _tokenize_words(t or "")
        token_counts.append(len(words))
        slens = [len(_tokenize_words(s)) for s in sents] or [0]
        avg_sent_len.append(float(np.mean(slens)))
        var_sent_len.append(float(np.var(slens)))
        text_len = max(1, len(t))
        exclaim_rate.append(t.count("!") / text_len)
        question_rate.append(t.count("?") / text_len)
        upper = sum(1 for c in t if c.isupper())
        letters = sum(1 for c in t if c.isalpha())
        uppercase_ratio.append((upper / letters) if letters else 0.0)
        # simple dialogue heuristic: lines starting with uppercase word + colon, or quoted text
        lines = [ln.strip() for ln in t.splitlines() if ln.strip()]
        dialogue_lines = [ln for ln in lines if (":" in ln.split(" ")[0] if ln.split(" ") else False) or ("\"" in ln or "'" in ln)]
        dialogue_ratio.append((len(dialogue_lines) / len(lines)) if lines else 0.0)
    df["sentence_count"] = sent_counts
    df["token_count"] = token_counts
    df["avg_sentence_len"] = avg_sent_len
    df["var_sentence_len"] = var_sent_len
    df["exclaim_rate"] = exclaim_rate
    df["question_rate"] = question_rate
    df["uppercase_ratio"] = uppercase_ratio
    df["dialogue_ratio"] = dialogue_ratio
    return df


def add_position_overlap_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    # position
    if "scene_index" in df.columns:
        by_movie = df.groupby("movie_id")["scene_index"].transform("max").replace(0, 1)
        df["scene_index_norm"] = df["scene_index"] / by_movie
    else:
        df["scene_index_norm"] = 0.0
    # lexical overlap with neighbors (Jaccard)
    def jaccard(a: set, b: set) -> float:
        if not a and not b:
            return 0.0
        return float(len(a & b)) / float(len(a | b))
    prev_overlap: List[float] = []
    next_overlap: List[float] = []
    for i in tqdm(range(len(df)), desc="Overlap"):
        cur = set(w.lower() for w in _tokenize_words(df.loc[i, "scene_text"]) if w)
        # find prev/next within same movie
        prev_set: set = set()
        next_set: set = set()
        if i > 0 and df.loc[i - 1, "movie_id"] == df.loc[i, "movie_id"]:
            prev_set = set(w.lower() for w in _tokenize_words(df.loc[i - 1, "scene_text"]))
        if i + 1 < len(df) and df.loc[i + 1, "movie_id"] == df.loc[i, "movie_id"]:
            next_set = set(w.lower() for w in _tokenize_words(df.loc[i + 1, "scene_text"]))
        prev_overlap.append(jaccard(cur, prev_set))
        next_overlap.append(jaccard(cur, next_set))
    df["overlap_prev"] = prev_overlap
    df["overlap_next"] = next_overlap
    return df


def add_similarity_change_features(df: pd.DataFrame, text_col: str = "scene_text") -> pd.DataFrame:
    """
    Detect plot shifts through changes in similarity patterns.
    
    NEW FEATURES:
    - sim_change_magnitude: How different is this vs the trend?
    - vocab_novelty: New words appearing (reveals/plot twists)
    - dialogue_shift: Sudden change in dialogue amount
    
    Key insight: Salient scenes break from the pattern.
    """
    df = df.copy()
    
    # Initialize columns
    df["sim_change_magnitude"] = 0.0
    df["vocab_novelty"] = 0.0
    df["dialogue_shift"] = 0.0
    
    # Process each movie separately
    for movie_id, movie_group in tqdm(df.groupby("movie_id"), desc="Similarity changes"):
        movie_df = movie_group.sort_values("scene_index").copy()
        indices = movie_df.index.tolist()
        
        # Pre-tokenize all texts
        all_texts = []
        all_words = []
        all_dialogue_counts = []
        
        for idx in indices:
            text = movie_df.loc[idx, text_col]
            words = set(w.lower() for w in _tokenize_words(text))
            dialogue_count = text.count('"') + text.count("'")
            
            all_texts.append(text)
            all_words.append(words)
            all_dialogue_counts.append(dialogue_count)
        
        # Process each scene with context
        for i, idx in enumerate(indices):
            curr_words = all_words[i]
            curr_text = all_texts[i]
            curr_dialogue = all_dialogue_counts[i]
            
            # 1. SIMILARITY CHANGE MAGNITUDE
            # Compare current similarity with baseline trend
            if i >= 2:
                prev_words = all_words[i-1]
                prev_prev_words = all_words[i-2]
                
                # What was the "normal" similarity between prev scenes?
                baseline_sim = _jaccard_similarity(prev_words, prev_prev_words)
                
                # How similar is current to previous?
                current_sim = _jaccard_similarity(curr_words, prev_words)
                
                # Big change = plot shift / reveal moment
                change = abs(current_sim - baseline_sim)
                df.loc[idx, "sim_change_magnitude"] = float(change)
            
            # 2. VOCABULARY NOVELTY
            # New words = new concepts = reveals/twists
            if i > 0:
                prev_words = all_words[i-1]
                new_words = curr_words - prev_words
                novelty = len(new_words) / max(1, len(curr_words))
                df.loc[idx, "vocab_novelty"] = float(novelty)
            
            # 3. DIALOGUE SHIFT
            # Sudden change in dialogue amount
            if i > 0:
                prev_dialogue = all_dialogue_counts[i-1]
                text_length = max(1, len(curr_text.split()))
                shift = abs(curr_dialogue - prev_dialogue) / text_length
                df.loc[idx, "dialogue_shift"] = float(shift)
    
    return df


def add_structural_position_features(df: pd.DataFrame, text_col: str = "scene_text") -> pd.DataFrame:
    """
    Add three-act structure and thematic callback features.
    
    NEW FEATURES:
    - pos_edge_proximity: U-shaped importance (beginning/end salient)
    - pos_act: Which act (1=setup, 2=conflict, 3=resolution)
    - pos_within_act: Position within current act
    - callback_to_opening: Thematic return to opening
    - callback_to_ending: Foreshadowing of ending
    
    Key insight: Position matters. Edges and callbacks are salient.
    """
    df = df.copy()
    
    # Initialize columns
    df["pos_edge_proximity"] = 0.0
    df["pos_act"] = 1.0
    df["pos_within_act"] = 0.0
    df["callback_to_opening"] = 0.0
    df["callback_to_ending"] = 0.0
    
    # Process each movie separately
    for movie_id, movie_group in tqdm(df.groupby("movie_id"), desc="Structural position"):
        movie_df = movie_group.sort_values("scene_index").copy()
        indices = movie_df.index.tolist()
        total_scenes = len(indices)
        
        if total_scenes == 0:
            continue
        
        # Get opening and ending text
        first_text = movie_df.loc[indices[0], text_col]
        last_text = movie_df.loc[indices[-1], text_col]
        first_words = set(w.lower() for w in _tokenize_words(first_text))
        last_words = set(w.lower() for w in _tokenize_words(last_text))
        
        # Process each scene
        for i, idx in enumerate(indices):
            # Normalized position (0 = start, 1 = end)
            pos_norm = i / max(1, total_scenes - 1)
            
            # 1. EDGE PROXIMITY (U-shaped curve)
            # Maximum at edges (0 and 1), minimum at middle (0.5)
            # Formula: 1 - 2*|pos - 0.5|
            edge_proximity = 1.0 - 2.0 * abs(pos_norm - 0.5)
            df.loc[idx, "pos_edge_proximity"] = float(edge_proximity)
            
            # 2. THREE-ACT STRUCTURE
            # Act 1: 0-25% (setup)
            # Act 2: 25-75% (confrontation)
            # Act 3: 75-100% (resolution)
            if pos_norm < 0.25:
                act = 1
                within_act = pos_norm / 0.25
            elif pos_norm < 0.75:
                act = 2
                within_act = (pos_norm - 0.25) / 0.5
            else:
                act = 3
                within_act = (pos_norm - 0.75) / 0.25
            
            df.loc[idx, "pos_act"] = float(act)
            df.loc[idx, "pos_within_act"] = float(within_act)
            
            # 3. THEMATIC CALLBACKS
            # Similarity with opening/ending = structural callbacks
            curr_text = movie_df.loc[idx, text_col]
            curr_words = set(w.lower() for w in _tokenize_words(curr_text))
            
            callback_opening = _jaccard_similarity(curr_words, first_words)
            callback_ending = _jaccard_similarity(curr_words, last_words)
            
            df.loc[idx, "callback_to_opening"] = float(callback_opening)
            df.loc[idx, "callback_to_ending"] = float(callback_ending)
    
    return df


def _jaccard_similarity(set_a: set, set_b: set) -> float:
    """Compute Jaccard similarity between two sets."""
    if not set_a or not set_b:
        return 0.0
    intersection = len(set_a & set_b)
    union = len(set_a | set_b)
    return intersection / union if union > 0 else 0.0

================
File: features/bert_surprisal.py
================
#!/usr/bin/env python3
"""
BERT Surprisal Feature Extractor

Provides feature extraction functions compatible with the MENSA pipeline.
These wrappers extract only scene-level features from BERT surprisal.
"""

import warnings
import numpy as np
import pandas as pd
from typing import Dict, List
import torch

warnings.filterwarnings('ignore')


def add_bert_surprisal_features(df: pd.DataFrame, text_col: str = "scene_text") -> pd.DataFrame:
    """
    Add BERT-large surprisal features.
    
    This extracts only scene-level features. For word-level surprisal data,
    use BERTSurprisalExtractor directly.
    
    Args:
        df: DataFrame with scene texts
        text_col: Column containing text
        
    Returns:
        DataFrame with added bert_* columns
    """
    from feature_extractors import BERTSurprisalExtractor
    
    print("  Loading BERT-large model...")
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    use_fp16 = device == 'cuda'
    extractor = BERTSurprisalExtractor(device=device, use_fp16=use_fp16, preprocess=True)
    
    print(f"  Computing BERT surprisal for {len(df)} scenes...")
    features_list = []
    
    from tqdm.auto import tqdm
    for text in tqdm(df[text_col], desc="  BERT Surprisal", leave=False):
        result = extractor.extract(text)
        # Extract only scene-level features
        scene_features = result['scene_level']
        features_list.append(scene_features)
    
    # Convert to dataframe and concatenate
    features_df = pd.DataFrame(features_list)
    result = pd.concat([df.reset_index(drop=True), features_df.reset_index(drop=True)], axis=1)
    
    return result


def extract_word_level_data(df: pd.DataFrame, text_col: str = "scene_text") -> pd.DataFrame:
    """
    Extract word-level surprisal data from scenes.
    
    This is separate from the main pipeline and produces word-level DataFrames
    suitable for psycholinguistic analysis.
    
    Args:
        df: DataFrame with scene texts and metadata
        text_col: Column containing text
        
    Returns:
        DataFrame with word-level data (one row per word)
    """
    from feature_extractors import BERTSurprisalExtractor
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    extractor = BERTSurprisalExtractor(device=device, preprocess=True)
    
    all_rows = []
    from tqdm.auto import tqdm
    
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="Extracting BERT word-level"):
        text = row[text_col]
        if not text or not text.strip():
            continue
        
        result = extractor.extract(text)
        word_level_data = result['word_level']
        
        # Add scene metadata to each word
        for word_item in word_level_data:
            word_row = {
                'movie_id': row.get('movie_id', idx),
                'scene_index': row.get('scene_index', 0),
                'saliency_score': row.get('label', None),
            }
            word_row.update(word_item)
            all_rows.append(word_row)
    
    return pd.DataFrame(all_rows)

================
File: features/character_arcs.py
================
from __future__ import annotations

from typing import List

import numpy as np
import pandas as pd
from tqdm.auto import tqdm

try:
    import spacy
except Exception:
    spacy = None  # type: ignore


PRONOUNS = {
    "he", "she", "they", "him", "her", "them",
    "his", "hers", "theirs", "himself", "herself", "themselves",
}


def _load_spacy_model():
    if spacy is None:
        return None
    try:
        return spacy.load("en_core_web_sm")
    except Exception:
        try:
            return spacy.load("en_core_web_md")
        except Exception:
            return None


def add_entity_character_features(df: pd.DataFrame, text_col: str = "scene_text") -> pd.DataFrame:
    df = df.copy()
    nlp = _load_spacy_model()
    unique_person_count: List[float] = []
    top_char_rate: List[float] = []
    pronoun_ratio: List[float] = []
    name_repetition_rate: List[float] = []

    texts = df[text_col].tolist()
    if nlp is None:
        # Fallback: zeros when spaCy NER is unavailable
        for t in tqdm(texts, desc="Entity (fallback)"):
            words = t.split()
            wc = max(1, len(words))
            unique_person_count.append(0.0)
            top_char_rate.append(0.0)
            pronoun_ratio.append(float(sum(1 for w in words if w.lower() in PRONOUNS)) / wc)
            name_repetition_rate.append(0.0)
        df["unique_PERSON_count"] = unique_person_count
        df["top_character_mention_rate"] = top_char_rate
        df["pronoun_ratio"] = pronoun_ratio
        df["name_repetition_rate"] = name_repetition_rate
        return df

    for doc in tqdm(nlp.pipe(texts, batch_size=64, disable=[]), total=len(texts), desc="Entity/Character"):
        persons = [ent.text for ent in doc.ents if ent.label_ == "PERSON"]
        total_mentions = len(persons)
        norm_names = [p.strip().lower() for p in persons if p.strip()]
        unique_names = set(norm_names)
        unique_person_count.append(float(len(unique_names)))
        if total_mentions > 0:
            # top character mention rate: max name freq / total mentions
            counts = {}
            for n in norm_names:
                counts[n] = counts.get(n, 0) + 1
            top = max(counts.values())
            top_char_rate.append(float(top) / float(total_mentions))
            name_repetition_rate.append(float(total_mentions - len(unique_names)) / float(total_mentions))
        else:
            top_char_rate.append(0.0)
            name_repetition_rate.append(0.0)

        # pronoun ratio: pronoun tokens / total tokens
        tokens = [t.text for t in doc if not t.is_space]
        wc = max(1, len(tokens))
        prons = sum(1 for t in tokens if t.lower() in PRONOUNS or (t.isalpha() and getattr(getattr(t, "pos_", None), "lower", lambda: "")() == "pron"))
        pronoun_ratio.append(float(prons) / float(wc))

    df["unique_PERSON_count"] = unique_person_count
    df["top_character_mention_rate"] = top_char_rate
    df["pronoun_ratio"] = pronoun_ratio
    df["name_repetition_rate"] = name_repetition_rate
    return df


def add_character_arc_features(df: pd.DataFrame, text_col: str = "scene_text") -> pd.DataFrame:
    """
    Add character transition features across movie timeline.
    
    NEW FEATURES (high impact):
    - char_new_introductions: First appearance in movie (setup)
    - char_returns: Previously seen but not in last scene (payoff!)
    - char_callbacks: Characters from opening scene return
    - char_turnover: How much character lineup changed vs prev scene
    
    These capture: "Oh! That character from Act 1 is back!" moments.
    """
    df = df.copy()
    nlp = _load_spacy_model()
    
    # Initialize columns
    df["char_new_introductions"] = 0.0
    df["char_returns"] = 0.0
    df["char_callbacks"] = 0.0
    df["char_turnover"] = 0.0
    
    if nlp is None:
        return df
    
    # Process each movie separately to track character history
    for movie_id, movie_group in tqdm(df.groupby("movie_id"), desc="Character arcs"):
        # Sort by scene index to process chronologically
        movie_df = movie_group.sort_values("scene_index").copy()
        indices = movie_df.index.tolist()
        
        # Extract all characters for all scenes in this movie
        all_scene_chars = []
        for idx in indices:
            text = movie_df.loc[idx, text_col]
            doc = nlp(text[:1000])  # Limit length for speed
            chars = {ent.text.lower() for ent in doc.ents if ent.label_ == "PERSON"}
            all_scene_chars.append(chars)
        
        # Track all characters seen so far (cumulative)
        all_previous_chars = set()
        first_scene_chars = all_scene_chars[0] if all_scene_chars else set()
        
        # Process each scene
        for i, idx in enumerate(indices):
            current_chars = all_scene_chars[i]
            prev_chars = all_scene_chars[i-1] if i > 0 else set()
            
            # 1. NEW INTRODUCTIONS: Characters appearing for first time
            new_chars = current_chars - all_previous_chars
            df.loc[idx, "char_new_introductions"] = float(len(new_chars))
            
            # 2. RETURNS: Characters seen before but not in previous scene
            # This captures "payoff" moments - character from Act 1 returns!
            returning_chars = current_chars & (all_previous_chars - prev_chars)
            df.loc[idx, "char_returns"] = float(len(returning_chars))
            
            # 3. CALLBACKS: Characters from opening scene
            # Captures structural callbacks to beginning
            callback_chars = current_chars & first_scene_chars
            df.loc[idx, "char_callbacks"] = float(len(callback_chars))
            
            # 4. TURNOVER: How much did character lineup change?
            # High turnover = scene transition / location change
            if prev_chars or current_chars:
                left = len(prev_chars - current_chars)
                joined = len(current_chars - prev_chars)
                total = len(prev_chars | current_chars)
                turnover = (left + joined) / max(1, total)
            else:
                turnover = 0.0
            df.loc[idx, "char_turnover"] = float(turnover)
            
            # Update cumulative set for next iteration
            all_previous_chars |= current_chars
    
    return df

================
File: features/emotional.py
================
from __future__ import annotations

import numpy as np
import pandas as pd
from transformers import pipeline
import torch
from tqdm.auto import tqdm
import nltk

try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")


class EmotionalTrajectoryExtractor:
    def __init__(self):
        # Use RoBERTa-based emotion classifier (6 emotions + neutral)
        # This is universal - no word lists, pure deep learning
        self.emotion_model = pipeline(
            "text-classification",
            model="j-hartmann/emotion-english-distilroberta-base",
            return_all_scores=True,
            device=0 if torch.cuda.is_available() else -1
        )
    
    def compute_trajectory_features(self, text: str) -> dict:
        """Extract emotional trajectory using transformer model."""
        sentences = nltk.sent_tokenize(text)
        if len(sentences) < 2:
            return {
                'emo_shift_magnitude': 0.0,
                'emo_volatility': 0.0,
                'emo_final_vs_initial': 0.0,
                'emo_peak_position': 0.5,
                'emo_dominant_emotion': 0.0
            }
        
        # Get emotion distributions for each sentence
        emotion_sequences = []
        for sent in sentences:
            # Model handles 512 tokens max
            emotions = self.emotion_model(sent[:512])[0]
            # Convert to dict: {anger: 0.1, disgust: 0.05, fear: 0.2, joy: 0.4, ...}
            emotion_dict = {e['label']: e['score'] for e in emotions}
            emotion_sequences.append(emotion_dict)
        
        # Extract trajectory features
        features = {}
        
        # 1. Emotional shift magnitude (most important feature)
        # Compare first third vs last third of scene
        n = len(emotion_sequences)
        first_third = emotion_sequences[:max(1, n//3)]
        last_third = emotion_sequences[-max(1, n//3):]
        
        # Average emotions in each third
        def avg_emotions(emo_list):
            avg = {}
            for emotion in ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']:
                values = [e.get(emotion, 0) for e in emo_list]
                avg[emotion] = np.mean(values)
            return avg
        
        first_avg = avg_emotions(first_third)
        last_avg = avg_emotions(last_third)
        
        # Compute shift as euclidean distance in emotion space
        shift = np.sqrt(sum((last_avg[e] - first_avg[e])**2 
                           for e in first_avg.keys()))
        features['emo_shift_magnitude'] = float(shift)
        
        # 2. Emotional volatility (variance across scene)
        all_values = []
        for emotion in ['anger', 'fear', 'joy', 'sadness']:
            values = [e.get(emotion, 0) for e in emotion_sequences]
            all_values.extend(values)
        features['emo_volatility'] = float(np.std(all_values))
        
        # 3. Final vs Initial emotional state
        # Positive if ending more positive than beginning
        initial_joy = emotion_sequences[0].get('joy', 0)
        final_joy = emotion_sequences[-1].get('joy', 0)
        initial_neg = (emotion_sequences[0].get('anger', 0) + 
                      emotion_sequences[0].get('fear', 0) + 
                      emotion_sequences[0].get('sadness', 0))
        final_neg = (emotion_sequences[-1].get('anger', 0) + 
                    emotion_sequences[-1].get('fear', 0) + 
                    emotion_sequences[-1].get('sadness', 0))
        
        features['emo_final_vs_initial'] = float(
            (final_joy - initial_joy) - (final_neg - initial_neg)
        )
        
        # 4. Peak emotion position (where is the emotional climax)
        max_intensities = []
        for emo_dict in emotion_sequences:
            # Get maximum emotion value for each sentence
            max_intensity = max(emo_dict.values())
            max_intensities.append(max_intensity)
        
        peak_idx = np.argmax(max_intensities)
        features['emo_peak_position'] = float(peak_idx / max(1, n - 1))
        
        # 5. Dominant emotion strength
        all_emotions = {'anger': [], 'fear': [], 'joy': [], 'sadness': []}
        for e_dict in emotion_sequences:
            for emo in all_emotions.keys():
                all_emotions[emo].append(e_dict.get(emo, 0))
        
        avg_per_emotion = {e: np.mean(v) for e, v in all_emotions.items()}
        features['emo_dominant_emotion'] = float(max(avg_per_emotion.values()))
        
        return features


def add_emotional_trajectory_features(df: pd.DataFrame, text_col: str = "scene_text") -> pd.DataFrame:
    """Add emotional trajectory features to dataframe."""
    df = df.copy()
    extractor = EmotionalTrajectoryExtractor()
    
    # Process all texts
    all_features = []
    for text in tqdm(df[text_col].tolist(), desc="Emotional Trajectories"):
        features = extractor.compute_trajectory_features(text)
        all_features.append(features)
    
    # Add to dataframe
    features_df = pd.DataFrame(all_features)
    for col in features_df.columns:
        df[col] = features_df[col].values
    
    return df

================
File: features/gc_academic.py
================
import re
from .gc_base import BaseFeatureExtractor


class AcademicFeatureExtractor(BaseFeatureExtractor):
    """Extract academic and non-fiction specific features"""
    
    def __init__(self):
        super().__init__()
        self.citation_patterns = re.compile(
            r'\([A-Z][^)]+,?\s*\d{4}\)|[\[\(]\d+[\]\)]|\b\d{4}[a-z]?\b|'
            r'\bet\s+al\.|ibid\.|op\.\s*cit\.|cf\.|see\s+also|cited\s+in'
        )
        self.numbers_pattern = re.compile(r'\b\d+(\.\d+)?%?\b')
        self.technical_terms = re.compile(r'\b[A-Z]{2,}\b')
        self.parentheticals = re.compile(r'\([^)]+\)')
    
    def extract(self, text):
        """Extract academic features"""
        features = {}
        total_words = len(text.split())
        
        # Citations and references
        citation_matches = self.citation_patterns.findall(text)
        features['citation_count'] = len(citation_matches)
        features['citation_ratio'] = len(citation_matches) / (total_words + 1)
        
        # Numbers and statistics
        number_matches = self.numbers_pattern.findall(text)
        features['number_count'] = len(number_matches)
        features['number_ratio'] = len(number_matches) / (total_words + 1)
        
        # Technical terms
        technical_matches = self.technical_terms.findall(text)
        # Filter out common words
        technical_matches = [t for t in technical_matches if len(t) > 1 and 
                           t not in ['I', 'A', 'THE', 'AND', 'OR', 'BUT']]
        features['technical_term_count'] = len(technical_matches)
        features['technical_term_ratio'] = len(technical_matches) / (total_words + 1)
        
        # Parentheticals
        parenthetical_matches = self.parentheticals.findall(text)
        features['parenthetical_count'] = len(parenthetical_matches)
        features['parenthetical_ratio'] = len(parenthetical_matches) / (total_words + 1)
        
        return features

================
File: features/gc_base.py
================
"""
Base classes for Genre Classifier features.
Simplified version without external dependencies.
"""

import warnings
import logging
import os

warnings.filterwarnings('ignore')

try:
    import stanza
except ImportError:
    stanza = None

try:
    import spacy
except ImportError:
    spacy = None


class BaseFeatureExtractor:
    """Base class for all GC feature extractors"""
    
    def __init__(self):
        self.nlp_stanza = None
        self.nlp_spacy = None
        self._initialized = False
    
    def initialize_nlp(self):
        """Initialize NLP models (lazy loading)"""
        if not self._initialized:
            if stanza is not None:
                try:
                    logging.getLogger('stanza').setLevel(logging.ERROR)
                    os.environ['STANZA_RESOURCES_DIR'] = os.path.expanduser('~/stanza_resources')
                    self.nlp_stanza = stanza.Pipeline(
                        'en', 
                        processors='tokenize,pos,lemma,depparse',
                        download_method=None,
                        verbose=False,
                        logging_level='ERROR'
                    )
                except Exception as e:
                    print(f"Warning: Could not initialize Stanza: {e}")
                    self.nlp_stanza = None
            
            if spacy is not None:
                try:
                    self.nlp_spacy = spacy.load('en_core_web_sm')
                except Exception as e:
                    print(f"Warning: Could not initialize spaCy: {e}")
                    self.nlp_spacy = None
            
            self._initialized = True
    
    def extract(self, text):
        """Extract features from text. Override in subclasses."""
        raise NotImplementedError("Subclasses must implement extract method")

================
File: features/gc_basic.py
================
import numpy as np
from .gc_base import BaseFeatureExtractor


class BasicFeatureExtractor(BaseFeatureExtractor):
    """Extract basic text statistics"""
    
    def __init__(self):
        super().__init__()
        self.content_pos = {'NOUN', 'VERB', 'ADJ', 'ADV', 'PROPN'}
        self.function_pos = {'DET', 'ADP', 'PRON', 'CONJ', 'SCONJ', 'AUX', 'CCONJ', 'PART'}
    
    def extract(self, text):
        """Extract basic features: sentence length and lexical density"""
        self.initialize_nlp()
        doc = self.nlp_stanza(text)
        
        features = {}
        
        # Sentence length statistics
        sentence_lengths = [len(sent.words) for sent in doc.sentences]
        if sentence_lengths:
            features['avg_sen_len'] = np.mean(sentence_lengths)
            features['std_sen_len'] = np.std(sentence_lengths)
        else:
            features['avg_sen_len'] = 0
            features['std_sen_len'] = 0
        
        # Lexical density
        content_words = 0
        function_words = 0
        
        for sent in doc.sentences:
            for word in sent.words:
                if word.upos in self.content_pos:
                    content_words += 1
                elif word.upos in self.function_pos:
                    function_words += 1
        
        total_words = content_words + function_words
        features['lex_den'] = content_words / total_words if total_words > 0 else 0
        
        return features

================
File: features/gc_char_diversity.py
================
import numpy as np
import math
from collections import Counter
from .gc_base import BaseFeatureExtractor


class CharDiversityFeatureExtractor(BaseFeatureExtractor):
    """Extract character-level diversity measures"""
    
    def extract(self, text):
        """Extract all character diversity features"""
        chars = list(text.replace(' ', '').replace('\n', '').replace('\t', ''))
        
        if not chars:
            return {
                'cd_ttr': 0, 'cd_maas': 0, 'cd_msttr': 0, 'cd_mattr': 0,
                'cd_mtld': 0, 'cd_mtld_ma': 0, 'cd_yulesk': 0, 'cd_vocd': 0
            }
        
        features = {}
        
        # Type-Token Ratio
        unique_chars = set(chars)
        features['cd_ttr'] = len(unique_chars) / len(chars)
        
        # MAAS TTR
        N = len(chars)
        V = len(unique_chars)
        if N > 0 and V > 0:
            features['cd_maas'] = (math.log(N) - math.log(V)) / (math.log(N) ** 2)
        else:
            features['cd_maas'] = 0
        
        # MSTTR (Mean Segmental TTR)
        features['cd_msttr'] = self._calculate_msttr(chars)
        
        # MATTR (Moving Average TTR)
        features['cd_mattr'] = self._calculate_mattr(chars)
        
        # MTLD
        features['cd_mtld'] = self._calculate_mtld(chars)
        features['cd_mtld_ma'] = self._calculate_mtld_ma(chars)
        
        # Yule's K
        features['cd_yulesk'] = self._calculate_yules_k(chars)
        
        # VocD
        features['cd_vocd'] = self._calculate_vocd_simplified(chars)
        
        return features
    
    def _calculate_msttr(self, chars, segment_size=100):
        """Calculate Mean Segmental Type-Token Ratio"""
        segments = [chars[i:i+segment_size] for i in range(0, len(chars), segment_size)]
        segment_ttrs = []
        
        for segment in segments:
            if len(segment) >= 10:
                seg_unique = len(set(segment))
                seg_total = len(segment)
                segment_ttrs.append(seg_unique / seg_total)
        
        return np.mean(segment_ttrs) if segment_ttrs else len(set(chars)) / len(chars)
    
    def _calculate_mattr(self, chars, window_size=50):
        """Calculate Moving Average Type-Token Ratio"""
        window_size = min(window_size, len(chars))
        
        if window_size > 0 and len(chars) >= window_size:
            mattrs = []
            for i in range(len(chars) - window_size + 1):
                window = chars[i:i+window_size]
                window_unique = len(set(window))
                mattrs.append(window_unique / window_size)
            return np.mean(mattrs)
        else:
            return len(set(chars)) / len(chars)
    
    def _calculate_mtld(self, tokens, ttr_threshold=0.72):
        """Calculate Measure of Textual Lexical Diversity"""
        if not tokens:
            return 0
            
        def _compute_factor(tokens, ttr_threshold):
            factors = 0.0
            start = 0
            
            for i in range(1, len(tokens) + 1):
                segment = tokens[start:i]
                unique = len(set(segment))
                total = len(segment)
                ttr = unique / total if total > 0 else 0
                
                if ttr < ttr_threshold and i < len(tokens):
                    continue
                else:
                    factors += 1
                    start = i
                    
            if start < len(tokens):
                remaining_length = len(tokens) - start
                remaining_ttr = len(set(tokens[start:])) / remaining_length
                factors += remaining_ttr / ttr_threshold
                
            return factors
        
        forward_factor = _compute_factor(tokens, ttr_threshold)
        backward_factor = _compute_factor(tokens[::-1], ttr_threshold)
        
        if forward_factor > 0 and backward_factor > 0:
            mtld = len(tokens) / ((forward_factor + backward_factor) / 2)
        else:
            mtld = 0
            
        return mtld
    
    def _calculate_mtld_ma(self, tokens, window_size=50):
        """Calculate moving average MTLD"""
        if len(tokens) < window_size:
            return self._calculate_mtld(tokens)
            
        mtlds = []
        step = max(1, window_size // 2)
        
        for i in range(0, len(tokens) - window_size + 1, step):
            window = tokens[i:i+window_size]
            mtld_value = self._calculate_mtld(window)
            if mtld_value > 0:
                mtlds.append(mtld_value)
                
        return np.mean(mtlds) if mtlds else 0
    
    def _calculate_yules_k(self, chars):
        """Calculate Yule's K statistic"""
        char_freq = Counter(chars)
        M1 = len(chars)
        M2 = sum([freq * freq for freq in char_freq.values()])
        return 10000 * (M2 - M1) / (M1 * M1) if M1 > 0 else 0
    
    def _calculate_vocd_simplified(self, tokens):
        """Simplified VocD calculation"""
        if len(tokens) < 35:
            return 0
            
        sample_sizes = [35, 50, 70, 100]
        ttrs = []
        
        for size in sample_sizes:
            if size <= len(tokens):
                sample_ttrs = []
                for _ in range(10):
                    sample_indices = np.random.choice(len(tokens), size, replace=False)
                    sample = [tokens[i] for i in sample_indices]
                    ttr = len(set(sample)) / len(sample)
                    sample_ttrs.append(ttr)
                ttrs.append(np.mean(sample_ttrs))
        
        if len(ttrs) >= 2:
            vocd = np.mean(ttrs) * 100
        else:
            vocd = 0
            
        return vocd

================
File: features/gc_concreteness.py
================
import numpy as np
from collections import defaultdict
from .gc_base import BaseFeatureExtractor
from huggingface_hub import hf_hub_download


class ConcretenessFeatureExtractor(BaseFeatureExtractor):
    """Extract concreteness features using Brysbaert concreteness ratings"""
    
    def __init__(self, lexicon_path=None):
        """
        Initialize with path to concreteness ratings file.
        
        Parameters:
        -----------
        lexicon_path : str, optional
            Path to the Brysbaert concreteness ratings file.
            If None, downloads from HuggingFace Hub.
        """
        super().__init__()
        
        # Download from HuggingFace if no path provided
        if lexicon_path is None:
            print("Downloading concreteness lexicon from HuggingFace Hub...")
            lexicon_path = hf_hub_download(
                repo_id="Ishaank18/screenplay-lexicons",
                filename="Concreteness_ratings_Brysbaert_et_al_BRM.txt",
                repo_type="dataset"
            )
            print(f"âœ“ Downloaded to: {lexicon_path}")
        
        self.concreteness_lexicon = self._load_lexicon(lexicon_path)
    
    def _load_lexicon(self, path):
        """Load concreteness lexicon from file"""
        lexicon = {}
        try:
            with open(path, 'r', encoding='utf-8') as f:
                # Skip header line
                header = f.readline()
                
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    
                    parts = line.split('\t')
                    # Format: Word, Bigram, Conc.M, Conc.SD, Unknown, Total, Percent_known, SUBTLEX, Dom_Pos
                    if len(parts) >= 9:
                        word = parts[0].strip().lower()
                        bigram = parts[1].strip()
                        
                        try:
                            conc_mean = float(parts[2])
                            conc_sd = float(parts[3])
                            
                            lexicon[word] = {
                                'mean': conc_mean,
                                'sd': conc_sd,
                                'bigram': bigram
                            }
                        except ValueError:
                            continue
            
            print(f"Loaded {len(lexicon)} entries from concreteness lexicon")
        except FileNotFoundError:
            print(f"Warning: Concreteness lexicon file not found at {path}")
            print("Concreteness features will be set to 0")
        except Exception as e:
            print(f"Error loading concreteness lexicon: {e}")
        
        return lexicon
    
    def extract(self, text):
        """Extract concreteness features from text"""
        self.initialize_nlp()
        doc = self.nlp_stanza(text)
        
        features = {}
        
        # Collect concreteness scores
        concreteness_scores = []
        concreteness_sds = []
        coverage_count = 0
        total_words = 0
        
        # Track by POS
        pos_concreteness = defaultdict(list)
        
        for sent in doc.sentences:
            for word in sent.words:
                lemma = word.lemma.lower()
                upos = word.upos
                
                # Only process content words
                if upos in ['NOUN', 'VERB', 'ADJ', 'ADV', 'PROPN']:
                    total_words += 1
                    
                    # Look up in lexicon (try both lemma and original word)
                    word_text = word.text.lower()
                    
                    conc_data = None
                    if lemma in self.concreteness_lexicon:
                        conc_data = self.concreteness_lexicon[lemma]
                    elif word_text in self.concreteness_lexicon:
                        conc_data = self.concreteness_lexicon[word_text]
                    
                    if conc_data:
                        concreteness_scores.append(conc_data['mean'])
                        concreteness_sds.append(conc_data['sd'])
                        coverage_count += 1
                        pos_concreteness[upos].append(conc_data['mean'])
        
        # Overall concreteness statistics
        features['conc_mean'] = np.mean(concreteness_scores) if concreteness_scores else 0
        features['conc_std'] = np.std(concreteness_scores) if concreteness_scores else 0
        features['conc_median'] = np.median(concreteness_scores) if concreteness_scores else 0
        features['conc_max'] = np.max(concreteness_scores) if concreteness_scores else 0
        features['conc_min'] = np.min(concreteness_scores) if concreteness_scores else 0
        features['conc_range'] = features['conc_max'] - features['conc_min']
        
        # Average standard deviation (lexical ambiguity indicator)
        features['conc_sd_mean'] = np.mean(concreteness_sds) if concreteness_sds else 0
        
        # Coverage
        features['conc_coverage'] = coverage_count / (total_words + 1)
        
        # Count highly concrete/abstract words
        if concreteness_scores:
            features['conc_highly_concrete_count'] = sum(1 for s in concreteness_scores if s >= 4.5)
            features['conc_highly_abstract_count'] = sum(1 for s in concreteness_scores if s <= 2.5)
            features['conc_highly_concrete_ratio'] = features['conc_highly_concrete_count'] / len(concreteness_scores)
            features['conc_highly_abstract_ratio'] = features['conc_highly_abstract_count'] / len(concreteness_scores)
        else:
            features['conc_highly_concrete_count'] = 0
            features['conc_highly_abstract_count'] = 0
            features['conc_highly_concrete_ratio'] = 0
            features['conc_highly_abstract_ratio'] = 0
        
        # Concreteness by POS type
        for pos_type in ['NOUN', 'VERB', 'ADJ', 'ADV']:
            if pos_concreteness[pos_type]:
                features[f'conc_{pos_type.lower()}_mean'] = np.mean(pos_concreteness[pos_type])
                features[f'conc_{pos_type.lower()}_std'] = np.std(pos_concreteness[pos_type])
            else:
                features[f'conc_{pos_type.lower()}_mean'] = 0
                features[f'conc_{pos_type.lower()}_std'] = 0
        
        # Percentile features
        if concreteness_scores:
            features['conc_25percentile'] = np.percentile(concreteness_scores, 25)
            features['conc_75percentile'] = np.percentile(concreteness_scores, 75)
        else:
            features['conc_25percentile'] = 0
            features['conc_75percentile'] = 0
        
        return features

================
File: features/gc_dialogue.py
================
import re
from .gc_base import BaseFeatureExtractor


class DialogueFeatureExtractor(BaseFeatureExtractor):
    """Extract dialogue-related features"""
    
    def __init__(self):
        super().__init__()
        self.dialogue_markers = re.compile(r'["\'""'']')
        self.direct_speech = re.compile(r'["\'""''].*?["\'""'']', re.DOTALL)
        self.speech_verbs = re.compile(
            r'\b(said|says|saying|asked|asks|asking|replied|replies|replying|'
            r'whispered|whispers|whispering|shouted|shouts|shouting|muttered|'
            r'mutters|muttering|exclaimed|exclaims|exclaiming|answered|answers|'
            r'answering|told|tells|telling|cried|cries|crying|spoke|speaks|'
            r'speaking|declared|declares|declaring|announced|announces|announcing|'
            r'remarked|remarks|remarking|continued|continues|continuing|added|'
            r'adds|adding|interrupted|interrupts|interrupting|suggested|suggests|'
            r'suggesting|demanded|demands|demanding|insisted|insists|insisting|'
            r'explained|explains|explaining|admitted|admits|admitting|agreed|'
            r'agrees|agreeing|argued|argues|arguing|yelled|yells|yelling)\b', 
            re.IGNORECASE
        )
        self.speech_tags = re.compile(r',\s*["\'""'']|["\'""'']\s*,')
    
    def extract(self, text):
        """Extract dialogue features"""
        features = {}
        total_words = len(text.split())
        
        # Direct speech occurrences
        direct_speeches = self.direct_speech.findall(text)
        features['dialogue_count'] = len(direct_speeches)
        features['dialogue_ratio'] = len(direct_speeches) / (total_words + 1)
        
        # Speech verbs
        speech_verb_matches = self.speech_verbs.findall(text)
        features['speech_verb_count'] = len(speech_verb_matches)
        features['speech_verb_ratio'] = len(speech_verb_matches) / (total_words + 1)
        
        # Speech tags
        speech_tags = self.speech_tags.findall(text)
        features['speech_tag_count'] = len(speech_tags)
        
        # Dialogue diversity
        unique_speech_verbs = len(set([v.lower() for v in speech_verb_matches]))
        features['speech_verb_diversity'] = unique_speech_verbs / (len(speech_verb_matches) + 1)
        
        return features

================
File: features/gc_discourse.py
================
import re
from .gc_base import BaseFeatureExtractor


class DiscourseFeatureExtractor(BaseFeatureExtractor):
    """Extract discourse marker features"""
    
    def __init__(self):
        super().__init__()
        self.causal_markers = re.compile(
            r'\b(because|since|as|therefore|thus|hence|consequently|so|'
            r'accordingly|due to|owing to|as a result|for this reason)\b', 
            re.IGNORECASE
        )
        self.contrast_markers = re.compile(
            r'\b(but|however|although|though|despite|nevertheless|nonetheless|'
            r'yet|still|whereas|while|on the other hand|in contrast|conversely)\b', 
            re.IGNORECASE
        )
        self.addition_markers = re.compile(
            r'\b(and|also|moreover|furthermore|additionally|besides|in addition|'
            r'as well as|not only|similarly|likewise)\b', 
            re.IGNORECASE
        )
    
    def extract(self, text):
        """Extract discourse features"""
        features = {}
        total_words = len(text.split())
        
        # Causal markers
        causal_matches = self.causal_markers.findall(text)
        features['causal_marker_count'] = len(causal_matches)
        features['causal_marker_ratio'] = len(causal_matches) / (total_words + 1)
        
        # Contrast markers
        contrast_matches = self.contrast_markers.findall(text)
        features['contrast_marker_count'] = len(contrast_matches)
        features['contrast_marker_ratio'] = len(contrast_matches) / (total_words + 1)
        
        # Addition markers
        addition_matches = self.addition_markers.findall(text)
        features['addition_marker_count'] = len(addition_matches)
        features['addition_marker_ratio'] = len(addition_matches) / (total_words + 1)
        
        # Total discourse markers
        total_discourse = len(causal_matches) + len(contrast_matches) + len(addition_matches)
        features['discourse_marker_total_ratio'] = total_discourse / (total_words + 1)
        
        return features

================
File: features/gc_extractor.py
================
"""
Main Genre Classifier Feature Extractor
Orchestrates all GC feature extractors without external Genre_Classifier dependency
"""

import pandas as pd
import numpy as np
from typing import List, Optional
from tqdm.auto import tqdm

# Import all feature extractors from features directory
from .gc_basic import BasicFeatureExtractor
from .gc_char_diversity import CharDiversityFeatureExtractor
from .gc_pos import POSFeatureExtractor
from .gc_syntax import SyntaxFeatureExtractor
from .gc_dialogue import DialogueFeatureExtractor
from .gc_pronouns import PronounFeatureExtractor
from .gc_temporal import TemporalFeatureExtractor
from .gc_narrative import NarrativeFeatureExtractor
from .gc_academic import AcademicFeatureExtractor
from .gc_punctuation import PunctuationFeatureExtractor
from .gc_discourse import DiscourseFeatureExtractor
from .gc_readability import ReadabilityFeatureExtractor
from .gc_polarity import PolarityFeatureExtractor
from .gc_concreteness import ConcretenessFeatureExtractor


class GCFeatureExtractor:
    """Main feature extractor that combines all GC feature groups"""
    
    # Map of feature group names to extractor classes
    EXTRACTORS = {
        'basic': BasicFeatureExtractor,
        'char_diversity': CharDiversityFeatureExtractor,
        'pos': POSFeatureExtractor,
        'syntax': SyntaxFeatureExtractor,
        'dialogue': DialogueFeatureExtractor,
        'pronouns': PronounFeatureExtractor,
        'temporal': TemporalFeatureExtractor,
        'narrative': NarrativeFeatureExtractor,
        'academic': AcademicFeatureExtractor,
        'punctuation': PunctuationFeatureExtractor,
        'discourse': DiscourseFeatureExtractor,
        'readability': ReadabilityFeatureExtractor,
        'polarity': PolarityFeatureExtractor,
        'concreteness': ConcretenessFeatureExtractor,
    }
    
    def __init__(self, feature_groups='all', 
                 polarity_lexicon_path=None,
                 concreteness_lexicon_path=None):
        """
        Initialize the feature extractor with specified feature groups.
        
        Parameters:
        -----------
        feature_groups : list or str
            List of feature groups to extract. Use 'all' for all features.
            Available groups: basic, char_diversity, pos, syntax, dialogue,
                            pronouns, temporal, narrative, academic, punctuation,
                            discourse, readability, polarity, concreteness
        polarity_lexicon_path : str, optional
            Path to polarity lexicon file (not required - features will be 0 if missing)
        concreteness_lexicon_path : str, optional
            Path to concreteness lexicon file (not required - features will be 0 if missing)
        """
        # Determine which groups to use
        if feature_groups == 'all':
            self.feature_groups = list(self.EXTRACTORS.keys())
        else:
            self.feature_groups = feature_groups if isinstance(feature_groups, list) else [feature_groups]
        
        # Initialize extractors for selected groups
        self.extractors = {}
        for group in self.feature_groups:
            if group not in self.EXTRACTORS:
                print(f"Warning: Unknown feature group '{group}', skipping")
                continue
            
            # Special handling for lexicon-based extractors
            if group == 'polarity' and polarity_lexicon_path:
                self.extractors[group] = PolarityFeatureExtractor(lexicon_path=polarity_lexicon_path)
            elif group == 'concreteness' and concreteness_lexicon_path:
                self.extractors[group] = ConcretenessFeatureExtractor(lexicon_path=concreteness_lexicon_path)
            else:
                # Use default constructor
                self.extractors[group] = self.EXTRACTORS[group]()
    
    def extract_features_from_dataframe(self, df: pd.DataFrame, text_column: str = 'text') -> pd.DataFrame:
        """
        Extract features from a DataFrame containing text.
        
        Parameters:
        -----------
        df : pd.DataFrame
            DataFrame with text column
        text_column : str
            Name of the column containing text
            
        Returns:
        --------
        pd.DataFrame
            DataFrame with extracted features (one row per input text)
        """
        texts = df[text_column].tolist()
        
        print(f"Extracting features from {len(texts)} texts...")
        print(f"Selected feature groups: {self.feature_groups}")
        
        # Extract features for each text
        all_features = []
        
        for text in tqdm(texts, desc="Genre features"):
            text_features = {}
            
            # Extract from each extractor
            for group_name, extractor in self.extractors.items():
                try:
                    group_features = extractor.extract(text)
                    text_features.update(group_features)
                except Exception as e:
                    print(f"Warning: Error extracting {group_name} features: {e}")
                    # Add zeros for failed extraction
                    pass
            
            all_features.append(text_features)
        
        # Convert to DataFrame
        features_df = pd.DataFrame(all_features)
        
        # Fill any NaN values with 0
        features_df = features_df.fillna(0)
        
        print(f"Extracted {len(features_df.columns)} features from {len(texts)} texts")
        
        return features_df


# Global cache for extractors
_EXTRACTOR_CACHE = {}


def get_gc_extractor(groups=None):
    """Get or create a cached GC feature extractor for given groups."""
    key = tuple(sorted(groups)) if groups else ("__ALL__",)
    if key not in _EXTRACTOR_CACHE:
        _EXTRACTOR_CACHE[key] = GCFeatureExtractor(feature_groups=(groups or "all"))
    return _EXTRACTOR_CACHE[key]


def add_gc_features(df: pd.DataFrame, text_col: str = "scene_text", groups=None):
    """
    Add Genre Classifier features to dataframe.
    
    Args:
        df: DataFrame with text column
        text_col: Name of text column
        groups: List of feature groups or None for all
        
    Returns:
        DataFrame with gc_* columns added
    """
    df = df.copy()
    extractor = get_gc_extractor(groups)
    
    # Run extractor on text
    temp = pd.DataFrame({"text": df[text_col].astype(str).tolist()})
    feats = extractor.extract_features_from_dataframe(temp, text_column="text")
    
    # Prefix columns to avoid collisions
    feats = feats.add_prefix("gc_")
    
    # Align lengths; fill missing
    feats = feats.reindex(range(len(df))).fillna(0)
    
    # Concatenate side-by-side
    df = pd.concat([df.reset_index(drop=True), feats.reset_index(drop=True)], axis=1)
    return df

================
File: features/gc_narrative.py
================
import re
from .gc_base import BaseFeatureExtractor


class NarrativeFeatureExtractor(BaseFeatureExtractor):
    """Extract narrative-specific features"""
    
    def __init__(self):
        super().__init__()
        self.action_verbs = re.compile(
            r'\b(ran|walked|jumped|looked|turned|grabbed|pushed|pulled|opened|'
            r'closed|moved|stepped|reached|touched|lifted|dropped|threw|caught|'
            r'climbed|fell|slipped|danced|kicked|punched|kissed|hugged|smiled|'
            r'frowned|nodded|shook|waved|pointed|leaned|bent|stretched|twisted|'
            r'rolled|spun|crawled|leaped|bounced|slid|swung|rushed|hurried|'
            r'strolled|marched|tip-toed|stumbled|tripped|collapsed)\b', 
            re.IGNORECASE
        )
        self.sensory_words = re.compile(
            r'\b(saw|heard|felt|smelled|tasted|touched|looked|sounded|seemed|'
            r'appeared|watched|listened|observed|noticed|perceived|glimpsed|'
            r'spotted|detected|sensed|recognized|tasted like|smelled like|'
            r'felt like|looked like|sounded like)\b', 
            re.IGNORECASE
        )
        self.emotion_words = re.compile(
            r'\b(happy|sad|angry|afraid|excited|worried|surprised|disappointed|'
            r'confused|frustrated|anxious|nervous|scared|delighted|pleased|upset|'
            r'furious|terrified|thrilled|depressed|miserable|cheerful|content|'
            r'annoyed|irritated|embarrassed|ashamed|proud|jealous|envious|lonely|'
            r'bored|curious|suspicious|disgusted|relieved|grateful|hopeful|'
            r'desperate|calm|relaxed|tense|stressed)\b', 
            re.IGNORECASE
        )
        self.character_names = re.compile(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+){0,2}\b')
        self.place_indicators = re.compile(
            r'\b(in|at|on|near|beside|behind|above|below|between|through|across|'
            r'along|around|inside|outside|within)\s+(?:the\s+)?[A-Z][a-z]+\b'
        )
    
    def extract(self, text):
        """Extract narrative features"""
        features = {}
        total_words = len(text.split())
        
        # Action verbs
        action_matches = self.action_verbs.findall(text)
        features['action_verb_count'] = len(action_matches)
        features['action_verb_ratio'] = len(action_matches) / (total_words + 1)
        features['action_verb_diversity'] = len(set([v.lower() for v in action_matches])) / (len(action_matches) + 1)
        
        # Sensory words
        sensory_matches = self.sensory_words.findall(text)
        features['sensory_word_count'] = len(sensory_matches)
        features['sensory_word_ratio'] = len(sensory_matches) / (total_words + 1)
        
        # Emotion words
        emotion_matches = self.emotion_words.findall(text)
        features['emotion_word_count'] = len(emotion_matches)
        features['emotion_word_ratio'] = len(emotion_matches) / (total_words + 1)
        
        # Character names
        potential_names = self.character_names.findall(text)
        common_non_names = {'The', 'This', 'That', 'These', 'Those', 'Many', 'Some', 'All', 'New', 'Old'}
        names = [n for n in potential_names if n.split()[0] not in common_non_names]
        features['character_name_count'] = len(names)
        features['character_name_ratio'] = len(names) / (total_words + 1)
        
        # Place indicators
        place_matches = self.place_indicators.findall(text)
        features['place_indicator_count'] = len(place_matches)
        
        return features

================
File: features/gc_polarity.py
================
import numpy as np
from collections import defaultdict
from .gc_base import BaseFeatureExtractor
from huggingface_hub import hf_hub_download


class PolarityFeatureExtractor(BaseFeatureExtractor):
    """Extract sentiment polarity features using Distributional Polarity Lexicon"""
    
    def __init__(self, lexicon_path=None):
        """
        Initialize with path to polarity lexicon file.
        
        Parameters:
        -----------
        lexicon_path : str, optional
            Path to the DPLp-EN lexicon file (lemma::pos format).
            If None, downloads from HuggingFace Hub.
        """
        super().__init__()
        
        # Download from HuggingFace if no path provided
        if lexicon_path is None:
            print("Downloading polarity lexicon from HuggingFace Hub...")
            lexicon_path = hf_hub_download(
                repo_id="Ishaank18/screenplay-lexicons",
                filename="DPLp-EN_lrec2016.txt",
                repo_type="dataset"
            )
            print(f"âœ“ Downloaded to: {lexicon_path}")
        
        self.polarity_lexicon = self._load_lexicon(lexicon_path)
    
    def _load_lexicon(self, path):
        """Load polarity lexicon from file"""
        lexicon = {}
        try:
            with open(path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    parts = line.split('\t')
                    if len(parts) == 2:
                        word = parts[0].strip()
                        scores = parts[1].strip().split(',')
                        if len(scores) == 3:
                            try:
                                pos_score = float(scores[0])
                                neg_score = float(scores[1])
                                neu_score = float(scores[2])
                                lexicon[word] = {
                                    'positive': pos_score,
                                    'negative': neg_score,
                                    'neutral': neu_score
                                }
                            except ValueError:
                                continue
            print(f"Loaded {len(lexicon)} entries from polarity lexicon")
        except FileNotFoundError:
            print(f"Warning: Polarity lexicon file not found at {path}")
            print("Polarity features will be set to 0")
        except Exception as e:
            print(f"Error loading polarity lexicon: {e}")
        
        return lexicon
    
    def _get_pos_tag_mapping(self, upos):
        """Map Universal POS tags to lexicon format (::j, ::n, ::v, ::r)"""
        mapping = {
            'ADJ': 'j',
            'NOUN': 'n',
            'PROPN': 'n',
            'VERB': 'v',
            'ADV': 'r'
        }
        return mapping.get(upos, None)
    
    def extract(self, text):
        """Extract polarity features from text"""
        self.initialize_nlp()
        doc = self.nlp_stanza(text)
        
        features = {}
        
        # Collect polarity scores for all words
        positive_scores = []
        negative_scores = []
        neutral_scores = []
        polarity_coverage = 0
        total_content_words = 0
        
        # Track polarity by POS
        pos_polarity = defaultdict(lambda: {'pos': [], 'neg': [], 'neu': []})
        
        for sent in doc.sentences:
            for word in sent.words:
                # Get lemma and POS tag
                lemma = word.lemma.lower()
                upos = word.upos
                
                # Only process content words
                if upos in ['NOUN', 'VERB', 'ADJ', 'ADV', 'PROPN']:
                    total_content_words += 1
                    
                    # Map to lexicon format
                    pos_code = self._get_pos_tag_mapping(upos)
                    if pos_code:
                        lexicon_key = f"{lemma}::{pos_code}"
                        
                        # Look up in lexicon
                        if lexicon_key in self.polarity_lexicon:
                            scores = self.polarity_lexicon[lexicon_key]
                            positive_scores.append(scores['positive'])
                            negative_scores.append(scores['negative'])
                            neutral_scores.append(scores['neutral'])
                            polarity_coverage += 1
                            
                            # Track by POS
                            pos_polarity[upos]['pos'].append(scores['positive'])
                            pos_polarity[upos]['neg'].append(scores['negative'])
                            pos_polarity[upos]['neu'].append(scores['neutral'])
        
        # Overall polarity statistics
        features['pol_positive_mean'] = np.mean(positive_scores) if positive_scores else 0
        features['pol_positive_std'] = np.std(positive_scores) if positive_scores else 0
        features['pol_positive_max'] = np.max(positive_scores) if positive_scores else 0
        features['pol_positive_sum'] = np.sum(positive_scores) if positive_scores else 0
        
        features['pol_negative_mean'] = np.mean(negative_scores) if negative_scores else 0
        features['pol_negative_std'] = np.std(negative_scores) if negative_scores else 0
        features['pol_negative_max'] = np.max(negative_scores) if negative_scores else 0
        features['pol_negative_sum'] = np.sum(negative_scores) if negative_scores else 0
        
        features['pol_neutral_mean'] = np.mean(neutral_scores) if neutral_scores else 0
        features['pol_neutral_std'] = np.std(neutral_scores) if neutral_scores else 0
        
        # Polarity ratios
        if positive_scores and negative_scores:
            features['pol_pos_neg_ratio'] = np.mean(positive_scores) / (np.mean(negative_scores) + 0.01)
        else:
            features['pol_pos_neg_ratio'] = 0
        
        # Coverage
        features['pol_coverage'] = polarity_coverage / (total_content_words + 1)
        
        # Polarity by POS type
        for pos_type in ['NOUN', 'VERB', 'ADJ', 'ADV']:
            if pos_polarity[pos_type]['pos']:
                features[f'pol_{pos_type.lower()}_positive_mean'] = np.mean(pos_polarity[pos_type]['pos'])
                features[f'pol_{pos_type.lower()}_negative_mean'] = np.mean(pos_polarity[pos_type]['neg'])
            else:
                features[f'pol_{pos_type.lower()}_positive_mean'] = 0
                features[f'pol_{pos_type.lower()}_negative_mean'] = 0
        
        # Sentiment orientation (dominance of positive vs negative)
        if positive_scores or negative_scores:
            features['pol_sentiment_orientation'] = (np.sum(positive_scores) - np.sum(negative_scores)) / (len(positive_scores) + len(negative_scores) + 1)
        else:
            features['pol_sentiment_orientation'] = 0
        
        # Count highly polarized words
        features['pol_highly_positive_count'] = sum(1 for s in positive_scores if s > 0.7)
        features['pol_highly_negative_count'] = sum(1 for s in negative_scores if s > 0.7)
        
        return features

================
File: features/gc_pos.py
================
from collections import defaultdict
from .gc_base import BaseFeatureExtractor


class POSFeatureExtractor(BaseFeatureExtractor):
    """Extract Part-of-Speech ratio features"""
    
    def extract(self, text):
        """Extract POS ratio features"""
        self.initialize_nlp()
        doc = self.nlp_stanza(text)
        
        pos_counts = defaultdict(int)
        
        for sent in doc.sentences:
            for word in sent.words:
                pos_counts[word.upos] += 1
        
        # Calculate ratios with add-one smoothing
        ratios = {
            'adverb_noun': pos_counts['ADV'] / (pos_counts['NOUN'] + 1),
            'adverb_pronoun': pos_counts['ADV'] / (pos_counts['PRON'] + 1),
            'adjective_verb': pos_counts['ADJ'] / (pos_counts['VERB'] + 1),
            'noun_verb': pos_counts['NOUN'] / (pos_counts['VERB'] + 1),
            'verb_pronoun': pos_counts['VERB'] / (pos_counts['PRON'] + 1),
            'adverb_adjective': pos_counts['ADV'] / (pos_counts['ADJ'] + 1),
            'adjective_pronoun': pos_counts['ADJ'] / (pos_counts['PRON'] + 1),
            'noun_pronoun': pos_counts['NOUN'] / (pos_counts['PRON'] + 1)
        }
        
        return ratios

================
File: features/gc_pronouns.py
================
import re
from .gc_base import BaseFeatureExtractor


class PronounFeatureExtractor(BaseFeatureExtractor):
    """Extract pronoun usage features"""
    
    def __init__(self):
        super().__init__()
        self.pronouns = {
            'first_singular': re.compile(r'\b(I|me|my|mine|myself)\b', re.IGNORECASE),
            'first_plural': re.compile(r'\b(we|us|our|ours|ourselves)\b', re.IGNORECASE),
            'second_person': re.compile(r'\b(you|your|yours|yourself|yourselves)\b', re.IGNORECASE),
            'third_singular': re.compile(r'\b(he|him|his|himself|she|her|hers|herself|it|its|itself)\b', re.IGNORECASE),
            'third_plural': re.compile(r'\b(they|them|their|theirs|themselves)\b', re.IGNORECASE)
        }
    
    def extract(self, text):
        """Extract pronoun features"""
        features = {}
        total_words = len(text.split())
        
        # Count each pronoun type
        for pronoun_type, pattern in self.pronouns.items():
            matches = pattern.findall(text)
            features[f'pronoun_{pronoun_type}_count'] = len(matches)
            features[f'pronoun_{pronoun_type}_ratio'] = len(matches) / (total_words + 1)
        
        # Overall pronoun density
        total_pronouns = sum(len(pattern.findall(text)) for pattern in self.pronouns.values())
        features['pronoun_total_ratio'] = total_pronouns / (total_words + 1)
        
        # First vs third person ratio
        first_person = len(self.pronouns['first_singular'].findall(text)) + \
                      len(self.pronouns['first_plural'].findall(text))
        third_person = len(self.pronouns['third_singular'].findall(text)) + \
                      len(self.pronouns['third_plural'].findall(text))
        features['first_third_person_ratio'] = first_person / (third_person + 1)
        
        return features

================
File: features/gc_punctuation.py
================
import re
from .gc_base import BaseFeatureExtractor


class PunctuationFeatureExtractor(BaseFeatureExtractor):
    """Extract punctuation-based features"""
    
    def __init__(self):
        super().__init__()
        self.exclamation_pattern = re.compile(r'!')
        self.question_pattern = re.compile(r'\?')
        self.ellipsis_pattern = re.compile(r'\.\.\.')
    
    def extract(self, text):
        """Extract punctuation features"""
        features = {}
        total_words = len(text.split())
        
        # Exclamations
        exclamations = self.exclamation_pattern.findall(text)
        features['exclamation_count'] = len(exclamations)
        
        # Questions
        questions = self.question_pattern.findall(text)
        features['question_count'] = len(questions)
        
        # Ellipses
        ellipses = self.ellipsis_pattern.findall(text)
        features['ellipsis_count'] = len(ellipses)
        
        # Sentence endings ratio
        total_sentences = text.count('.') + text.count('!') + text.count('?')
        if total_sentences > 0:
            features['exclamation_sentence_ratio'] = len(exclamations) / total_sentences
            features['question_sentence_ratio'] = len(questions) / total_sentences
        else:
            features['exclamation_sentence_ratio'] = 0
            features['question_sentence_ratio'] = 0
        
        # Comma density
        features['comma_density'] = text.count(',') / (total_words + 1)
        
        # Semicolon and colon usage
        features['semicolon_count'] = text.count(';')
        features['colon_count'] = text.count(':')
        
        return features

================
File: features/gc_readability.py
================
from .gc_base import BaseFeatureExtractor


class ReadabilityFeatureExtractor(BaseFeatureExtractor):
    """Extract readability scores"""
    
    def extract(self, text):
        """Extract readability features"""
        features = {}
        
        words = text.split()
        sentences = text.count('.') + text.count('!') + text.count('?')
        
        if sentences > 0 and len(words) > 0:
            # Average words per sentence
            features['avg_words_per_sentence'] = len(words) / sentences
            
            # Average syllables per word
            syllable_count = sum(self._count_syllables(word) for word in words)
            features['avg_syllables_per_word'] = syllable_count / len(words)
            
            # Flesch Reading Ease
            features['flesch_reading_ease'] = 206.835 - 1.015 * (len(words) / sentences) - 84.6 * (syllable_count / len(words))
            
            # Gunning Fog Index
            complex_words = sum(1 for word in words if self._count_syllables(word) >= 3)
            features['gunning_fog_index'] = 0.4 * ((len(words) / sentences) + 100 * (complex_words / len(words)))
        else:
            features['avg_words_per_sentence'] = 0
            features['avg_syllables_per_word'] = 0
            features['flesch_reading_ease'] = 0
            features['gunning_fog_index'] = 0
        
        return features
    
    def _count_syllables(self, word):
        """Simple syllable counter"""
        word = word.lower()
        count = 0
        vowels = 'aeiouy'
        previous_was_vowel = False
        
        for char in word:
            is_vowel = char in vowels
            if is_vowel and not previous_was_vowel:
                count += 1
            previous_was_vowel = is_vowel
        
        if word.endswith('e'):
            count -= 1
        if count == 0:
            count = 1
            
        return count

================
File: features/gc_syntax.py
================
import numpy as np
from collections import defaultdict
from .gc_base import BaseFeatureExtractor


class SyntaxFeatureExtractor(BaseFeatureExtractor):
    """Extract syntactic and dependency features"""
    
    def extract(self, text):
        """Extract all syntactic features"""
        self.initialize_nlp()
        doc = self.nlp_stanza(text)
        
        features = {}
        
        # Dependency relations
        features.update(self._extract_dependency_features(doc))
        
        # Argument/Adjunct ratio
        features.update(self._calculate_argument_adjunct_ratio(doc))
        
        # Syntactic complexity
        features.update(self._calculate_syntactic_complexity(doc))
        
        # Dependency bigrams
        features.update(self._extract_dependency_bigrams(doc))
        
        return features
    
    def _extract_dependency_features(self, doc):
        """Extract dependency relation frequencies"""
        dep_counts = defaultdict(int)
        
        for sent in doc.sentences:
            for word in sent.words:
                if word.deprel:
                    dep_counts[word.deprel] += 1
        
        total_deps = sum(dep_counts.values())
        dep_features = {}
        
        important_deps = ['nsubj', 'obj', 'iobj', 'csubj', 'ccomp', 'xcomp', 
                         'obl', 'vocative', 'expl', 'dislocated', 'advcl', 
                         'advmod', 'discourse', 'aux', 'cop', 'mark', 'nmod', 
                         'appos', 'nummod', 'acl', 'amod', 'det', 'clf', 
                         'case', 'conj', 'cc', 'fixed', 'flat', 'compound',
                         'list', 'parataxis', 'orphan', 'goeswith', 'reparandum',
                         'punct', 'root', 'dep']
        
        for dep in important_deps:
            dep_features[f'dep_rel_{dep}'] = dep_counts.get(dep, 0) / (total_deps + 1)
        
        return dep_features
    
    def _calculate_argument_adjunct_ratio(self, doc):
        """Calculate argument/adjunct ratio"""
        arguments = 0
        adjuncts = 0
        
        arg_rels = {'nsubj', 'obj', 'iobj', 'csubj', 'ccomp', 'xcomp'}
        adj_rels = {'obl', 'advmod', 'advcl', 'nmod', 'amod'}
        
        for sent in doc.sentences:
            for word in sent.words:
                if word.deprel in arg_rels:
                    arguments += 1
                elif word.deprel in adj_rels:
                    adjuncts += 1
        
        ratio = arguments / (adjuncts + 1)
        return {'arg_adj_ratio': ratio}
    
    def _calculate_syntactic_complexity(self, doc):
        """Calculate syntactic complexity measures"""
        features = {}
        
        # Tree depth
        max_depths = []
        for sent in doc.sentences:
            depths = self._calculate_tree_depths(sent)
            if depths:
                max_depths.append(max(depths))
        
        features['syn_comp_depth_mean'] = np.mean(max_depths) if max_depths else 0
        features['syn_comp_depth_std'] = np.std(max_depths) if max_depths else 0
        
        # Dependency distances
        dep_distances = []
        for sent in doc.sentences:
            distances = self._calculate_dependency_distances(sent)
            dep_distances.extend(distances)
        
        features['syn_comp_add_mean'] = np.mean(dep_distances) if dep_distances else 0
        features['syn_comp_add_std'] = np.std(dep_distances) if dep_distances else 0
        
        # Index of Syntactic Complexity
        isc_values = []
        for sent in doc.sentences:
            isc = self._calculate_isc(sent)
            isc_values.append(isc)
        
        features['syn_comp_isc_mean'] = np.mean(isc_values) if isc_values else 0
        features['syn_comp_isc_std'] = np.std(isc_values) if isc_values else 0
        
        return features
    
    def _calculate_tree_depths(self, sentence):
        """Calculate depth of each node in dependency tree"""
        depths = []
        
        def get_depth(word_id, current_depth=0):
            depths.append(current_depth)
            for word in sentence.words:
                if word.head == word_id:
                    get_depth(word.id, current_depth + 1)
        
        for word in sentence.words:
            if word.deprel == 'root':
                get_depth(word.id)
                break
                
        return depths
    
    def _calculate_dependency_distances(self, sentence):
        """Calculate dependency distances"""
        distances = []
        
        for word in sentence.words:
            if word.head > 0:
                distance = abs(word.id - word.head)
                distances.append(distance)
                
        return distances
    
    def _calculate_isc(self, sentence):
        """Calculate Index of Syntactic Complexity"""
        subordinating_conj = sum(1 for w in sentence.words if w.upos == 'SCONJ')
        wh_words = sum(1 for w in sentence.words if w.text.lower() in 
                      ['who', 'whom', 'whose', 'which', 'what', 'where', 'when', 'why', 'how'])
        verb_forms = sum(1 for w in sentence.words if w.upos == 'VERB')
        
        total_words = len(sentence.words)
        
        if total_words > 0:
            isc = (subordinating_conj + wh_words + verb_forms) / total_words
        else:
            isc = 0
            
        return isc
    
    def _extract_dependency_bigrams(self, doc):
        """Extract dependency bigrams with word order information"""
        bigram_counts = defaultdict(int)
        
        for sent in doc.sentences:
            for word in sent.words:
                if word.head > 0:
                    head_word = sent.words[word.head - 1]
                    
                    dep_pos = word.upos
                    head_pos = head_word.upos
                    
                    if word.id < word.head:
                        bigram = f'{dep_pos}_before_{head_pos}'
                    else:
                        bigram = f'{dep_pos}_after_{head_pos}'
                    
                    bigram_counts[bigram] += 1
        
        total_bigrams = sum(bigram_counts.values())
        bigram_features = {}
        
        for bigram, count in bigram_counts.items():
            bigram_features[f'dep_big_{bigram}'] = count / (total_bigrams + 1)
        
        return bigram_features

================
File: features/gc_temporal.py
================
import re
from .gc_base import BaseFeatureExtractor


class TemporalFeatureExtractor(BaseFeatureExtractor):
    """Extract temporal and tense-related features"""
    
    def __init__(self):
        super().__init__()
        self.past_tense_verbs = re.compile(
            r'\b(was|were|had|did|went|came|saw|said|thought|felt|knew|took|'
            r'made|got|gave|found|told|asked|became|left|brought|began|kept|'
            r'held|stood|heard|let|meant|set|met|ran|paid|sat|spoke|lay|led|'
            r'read|grew|lost|fell|sent|built|understood|broke|spent|drove|'
            r'wrote|beat|bought|caught|taught|sold|fought|flew|drew|chose|'
            r'rose|threw|dealt)\b', 
            re.IGNORECASE
        )
        self.present_tense_verbs = re.compile(
            r'\b(is|are|am|has|have|does|do|goes|go|comes|come|sees|see|says|'
            r'say|thinks|think|feels|feel|knows|know|takes|take|makes|make|'
            r'gets|get|gives|give|finds|find|tells|tell|asks|ask|becomes|'
            r'become|leaves|leave|brings|bring|begins|begin|keeps|keep|holds|'
            r'hold|stands|stand)\b', 
            re.IGNORECASE
        )
        self.time_adverbs = re.compile(
            r'\b(yesterday|today|tomorrow|now|then|later|earlier|soon|recently|'
            r'lately|afterwards|meanwhile|suddenly|immediately|eventually|finally|'
            r'initially|subsequently|presently|formerly|currently|momentarily)\b', 
            re.IGNORECASE
        )
    
    def extract(self, text):
        """Extract temporal features"""
        features = {}
        total_words = len(text.split())
        
        # Past vs present tense
        past_matches = self.past_tense_verbs.findall(text)
        present_matches = self.present_tense_verbs.findall(text)
        
        features['past_tense_count'] = len(past_matches)
        features['present_tense_count'] = len(present_matches)
        features['past_present_ratio'] = len(past_matches) / (len(present_matches) + 1)
        
        # Time adverbs
        time_adverb_matches = self.time_adverbs.findall(text)
        features['time_adverb_count'] = len(time_adverb_matches)
        features['time_adverb_ratio'] = len(time_adverb_matches) / (total_words + 1)
        
        return features

================
File: features/gc_wrapper.py
================
"""
Simple wrapper to extract GC features from the features/ directory.
This provides a simple interface without needing the external Genre_Classifier library.
"""

from .gc_extractor import add_gc_features, get_gc_extractor

__all__ = ['add_gc_features', 'get_gc_extractor']

================
File: features/ngram_surprisal.py
================
#!/usr/bin/env python3
"""
N-gram Language Model Surprisal Feature Extractor

Computes surprisal features using n-gram language models (KenLM).
Follows the same architecture as surprisal.py (GPT-2) and bert_feature.py (BERT-large).

Features extracted (scene-level):
    - Sentence-level statistics: mean, std, cv, p75, max, slope
    - Perplexity metrics
    - Position-based features: first/last quarter comparison
    - Sentence count

Citation:
    Heafield, K. (2011). "KenLM: Faster and Smaller Language Model Queries." 
    Workshop on Statistical Machine Translation, ACL 2011.
"""

from __future__ import annotations

from typing import List, Dict
import numpy as np
import warnings

warnings.filterwarnings('ignore')

try:
    import nltk
    nltk.data.find("tokenizers/punkt")
except LookupError:
    import ssl
    ssl._create_default_https_context = ssl._create_unverified_context
    nltk.download("punkt")

from nltk.tokenize import sent_tokenize


class NgramSurprisalComputer:
    """
    Extracts n-gram surprisal features at sentence and scene level.
    
    Uses KenLM for efficient n-gram language model scoring.
    Provides a consistent interface with GPT-2 and BERT surprisal extractors.
    
    Scene-level output (85 features = 17 features Ã— 5 n-gram orders):
        For each n-gram order (1-5):
        - Sentence-level: mean, median, std, var, cv
        - Extremes: min, max, range, p25, p75, iqr
        - Perplexity: scene-level and sentence-level perplexity
        - Position: first_quarter_mean, last_quarter_mean, position_diff
        - Count: num_sentences
    """
    
    def __init__(self, model_paths: dict = None, max_order: int = 5):
        """
        Initialize n-gram language models for multiple orders.
        
        Args:
            model_paths: Dict mapping order to KenLM model path, e.g., {3: 'trigram.arpa', 5: '5gram.arpa'}
                        If None, uses fallback for all orders
            max_order: Maximum n-gram order to compute (default: 5)
        """
        self.max_order = max_order
        self.model_paths = model_paths or {}
        self._models = {}
        self._use_kenlm = {}
        
        # Initialize models for each order
        for order in range(1, max_order + 1):
            model_path = self.model_paths.get(order)
            
            if model_path:
                try:
                    import kenlm
                    self._models[order] = kenlm.Model(model_path)
                    self._use_kenlm[order] = True
                    print(f"  Loaded {order}-gram KenLM model: {model_path}")
                except ImportError:
                    if order == 1:
                        warnings.warn("KenLM not available. Using fallback NLTK n-gram model.")
                    self._use_kenlm[order] = False
                except Exception as e:
                    warnings.warn(f"Failed to load {order}-gram KenLM model: {e}. Using fallback.")
                    self._use_kenlm[order] = False
            else:
                self._use_kenlm[order] = False
        
        # Fallback model parameters
        self._smoothing = 1e-10
    
    def _fallback_score(self, sentence: str, order: int) -> float:
        """
        Compute sentence log probability using fallback model.
        Returns negative log probability (surprisal).
        """
        tokens = sentence.lower().split()
        if len(tokens) == 0:
            return 0.0
        
        # Add start/end tokens
        tokens = ['<s>'] * (order - 1) + tokens + ['</s>']
        
        total_log_prob = 0.0
        for i in range(order - 1, len(tokens)):
            # Simple uniform probability as fallback
            prob = 1e-5  # Very small probability for unseen n-grams
            total_log_prob += np.log(prob)
        
        # Return average negative log probability (surprisal)
        return -total_log_prob / max(len(tokens) - (order - 1), 1)
    
    def sentence_surprisal(self, sentence: str, order: int) -> float:
        """
        Compute average surprisal for a sentence using n-gram model of given order.
        
        Args:
            sentence: Input sentence
            order: N-gram order (1-5)
            
        Returns:
            Average surprisal (negative log probability per token)
        """
        cleaned = (sentence or "").strip()
        if not cleaned:
            return 0.0
        
        if self._use_kenlm.get(order, False):
            # KenLM returns log10 probability
            # Convert to nats (natural log) and negate for surprisal
            log10_prob = self._models[order].score(cleaned, bos=True, eos=True)
            # Convert log10 to natural log: ln(x) = log10(x) * ln(10)
            log_prob = log10_prob * np.log(10)
            # Normalize by number of tokens
            num_tokens = len(cleaned.split())
            surprisal = -log_prob / max(num_tokens, 1)
            return float(surprisal)
        else:
            # Use fallback model
            return self._fallback_score(cleaned, order)
        """
        Compute average surprisal for a sentence using n-gram model of given order.
        
        Args:
            sentence: Input sentence
            order: N-gram order (1-5)
            
        Returns:
            Average surprisal (negative log probability per token)
        """
        cleaned = (sentence or "").strip()
        if not cleaned:
            return 0.0
        
        if self._use_kenlm.get(order, False):
            # KenLM returns log10 probability
            # Convert to nats (natural log) and negate for surprisal
            log10_prob = self._models[order].score(cleaned, bos=True, eos=True)
            # Convert log10 to natural log: ln(x) = log10(x) * ln(10)
            log_prob = log10_prob * np.log(10)
            # Normalize by number of tokens
            num_tokens = len(cleaned.split())
            surprisal = -log_prob / max(num_tokens, 1)
            return float(surprisal)
        else:
            # Use fallback model
            return self._fallback_score(cleaned, order)
    
    def _compute_features_for_order(self, text: str, order: int, prefix: str) -> Dict[str, float]:
        """
        Compute surprisal features for a specific n-gram order.
        
        Args:
            text: Scene text
            order: N-gram order (1-5)
            prefix: Feature name prefix (e.g., 'ngram_1gram_')
            
        Returns:
            Dictionary with 17 features for this order
        """
        # Tokenize into sentences
        sentences = sent_tokenize(text or "") or [text or "."]
        sentences = [s for s in sentences if len(s.strip()) > 0]
        
        if not sentences:
            sentences = [text or "."]
        
        # Compute surprisal for each sentence using this order
        values: List[float] = [self.sentence_surprisal(s, order) for s in sentences]
        
        if not values:
            values = [self.sentence_surprisal(text or ".", order)]
        
        arr = np.array(values, dtype=np.float32)
        num_sentences = len(arr)
        
        # Basic statistics
        mean = float(np.mean(arr))
        median = float(np.median(arr))
        std = float(np.std(arr))
        var = float(np.var(arr))
        cv = float(std / mean) if mean != 0 else 0.0
        
        # Extremes and percentiles
        min_val = float(np.min(arr))
        max_val = float(np.max(arr))
        range_val = max_val - min_val
        p25 = float(np.percentile(arr, 25))
        p75 = float(np.percentile(arr, 75))
        iqr = p75 - p25
        
        # Perplexity
        perplexity = float(np.exp(mean))
        sentence_perplexity_mean = float(np.mean(np.exp(arr)))
        
        # Position-based features
        if num_sentences >= 4:
            quarter_size = num_sentences // 4
            first_quarter = arr[:quarter_size]
            last_quarter = arr[-quarter_size:]
            first_quarter_mean = float(np.mean(first_quarter))
            last_quarter_mean = float(np.mean(last_quarter))
            position_diff = last_quarter_mean - first_quarter_mean
        else:
            first_quarter_mean = mean
            last_quarter_mean = mean
            position_diff = 0.0
        
        # Slope across sentence order
        if num_sentences >= 2:
            x = np.arange(num_sentences, dtype=np.float32)
            x_mean = float(np.mean(x))
            y_mean = mean
            num = float(np.sum((x - x_mean) * (arr - y_mean)))
            den = float(np.sum((x - x_mean) ** 2)) or 1.0
            slope = num / den
        else:
            slope = 0.0
        
        return {
            f"{prefix}surprisal_mean": mean,
            f"{prefix}surprisal_median": median,
            f"{prefix}surprisal_std": std,
            f"{prefix}surprisal_var": var,
            f"{prefix}surprisal_cv": cv,
            f"{prefix}surprisal_min": min_val,
            f"{prefix}surprisal_max": max_val,
            f"{prefix}surprisal_range": range_val,
            f"{prefix}surprisal_p25": p25,
            f"{prefix}surprisal_p75": p75,
            f"{prefix}surprisal_iqr": iqr,
            f"{prefix}perplexity": perplexity,
            f"{prefix}sentence_perplexity_mean": sentence_perplexity_mean,
            f"{prefix}surprisal_first_quarter_mean": first_quarter_mean,
            f"{prefix}surprisal_last_quarter_mean": last_quarter_mean,
            f"{prefix}surprisal_position_diff": position_diff,
            f"{prefix}surprisal_slope": float(slope),
        }
    
    def scene_surprisal_features(self, text: str) -> Dict[str, float]:
        """
        Extract scene-level surprisal features for all n-gram orders.
        
        Computes sentence-level surprisals for unigrams through 5-grams
        and aggregates to scene statistics.
        
        Args:
            text: Scene text
            
        Returns:
            Dictionary with 85 scene-level features (17 per order Ã— 5 orders)
        """
        all_features = {}
        
        # Compute features for each n-gram order
        for order in range(1, self.max_order + 1):
            prefix = f"ngram_{order}gram_"
            order_features = self._compute_features_for_order(text, order, prefix)
            all_features.update(order_features)
        
        # Add sentence count (shared across all orders)
        sentences = sent_tokenize(text or "") or [text or "."]
        sentences = [s for s in sentences if len(s.strip()) > 0]
        all_features["ngram_num_sentences"] = float(len(sentences) if sentences else 1)
        
        return all_features
    
    def extract(self, text: str) -> Dict:
        """
        Extract all levels of features (for compatibility with BERT/RST interface).
        
        Args:
            text: Scene text
            
        Returns:
            Dictionary with 'scene_level' and 'sentence_level' keys
        """
        sentences = sent_tokenize(text or "") or [text or "."]
        sentences = [s for s in sentences if len(s.strip()) > 0]
        
        if not sentences:
            sentences = [text or "."]
        
        # Sentence-level data (compute for all orders)
        sentence_level = []
        for i, sent in enumerate(sentences):
            sent_data = {
                'sentence_position': i,
                'sentence': sent,
                'num_tokens': len(sent.split())
            }
            
            # Add surprisal for each order
            for order in range(1, self.max_order + 1):
                surprisal = self.sentence_surprisal(sent, order)
                sent_data[f'{order}gram_surprisal'] = surprisal
                sent_data[f'{order}gram_perplexity'] = float(np.exp(surprisal))
            
            sentence_level.append(sent_data)
        
        # Scene-level features
        scene_level = self.scene_surprisal_features(text)
        
        return {
            'sentence_level': sentence_level,
            'scene_level': scene_level
        }
    
    @staticmethod
    def get_scene_feature_names(max_order: int = 5) -> List[str]:
        """Return list of scene-level feature names."""
        feature_suffixes = [
            "surprisal_mean",
            "surprisal_median", 
            "surprisal_std",
            "surprisal_var",
            "surprisal_cv",
            "surprisal_min",
            "surprisal_max",
            "surprisal_range",
            "surprisal_p25",
            "surprisal_p75",
            "surprisal_iqr",
            "perplexity",
            "sentence_perplexity_mean",
            "surprisal_first_quarter_mean",
            "surprisal_last_quarter_mean",
            "surprisal_position_diff",
            "surprisal_slope",
        ]
        
        names = []
        for order in range(1, max_order + 1):
            for suffix in feature_suffixes:
                names.append(f"ngram_{order}gram_{suffix}")
        
        names.append("ngram_num_sentences")
        return names


def add_ngram_surprisal_features(df, text_col: str = "scene_text", 
                                 model_paths: dict = None, max_order: int = 5):
    """
    Add n-gram surprisal features to a DataFrame.
    
    This is the integration function for the MENSA pipeline,
    matching the pattern of add_bert_surprisal_features() and add_rst_features().
    
    Extracts features for multiple n-gram orders (unigram through max_order).
    
    Args:
        df: DataFrame with scene texts
        text_col: Column containing text
        model_paths: Dict mapping order to KenLM model path (optional)
                    e.g., {3: 'trigram.arpa', 5: '5gram.arpa'}
        max_order: Maximum n-gram order (default: 5)
        
    Returns:
        DataFrame with added ngram_* columns (86 features for max_order=5)
    """
    import pandas as pd
    from tqdm.auto import tqdm
    
    print(f"  Initializing n-gram surprisal computer (orders 1-{max_order})...")
    computer = NgramSurprisalComputer(model_paths=model_paths, max_order=max_order)
    
    print(f"  Computing n-gram surprisal for {len(df)} scenes...")
    features_list = []
    
    for text in tqdm(df[text_col], desc="  N-gram Surprisal", leave=False):
        result = computer.extract(text)
        scene_features = result['scene_level']
        features_list.append(scene_features)
    
    # Convert to dataframe and concatenate
    features_df = pd.DataFrame(features_list)
    result = pd.concat([df.reset_index(drop=True), features_df.reset_index(drop=True)], axis=1)
    
    return result


if __name__ == "__main__":
    # Demo usage
    print("="*80)
    print("N-gram Surprisal Feature Extractor Demo")
    print("="*80)
    
    computer = NgramSurprisalComputer(max_order=5)
    
    # Test sentence
    test_text = """
    The detective entered the dimly lit room. Strange symbols covered the walls.
    Something didn't feel right. He reached for his gun.
    """
    
    result = computer.extract(test_text)
    
    print("\nSentence-level features:")
    print("-" * 80)
    for sent_data in result['sentence_level']:
        print(f"Sentence {sent_data['sentence_position']}: {sent_data['sentence'][:60]}...")
        print(f"  1-gram: {sent_data['1gram_surprisal']:.3f}  ", end="")
        print(f"3-gram: {sent_data['3gram_surprisal']:.3f}  ", end="")
        print(f"5-gram: {sent_data['5gram_surprisal']:.3f}")
    
    print("\nScene-level features (sample - showing 1-gram and 5-gram):")
    print("-" * 80)
    for key, value in sorted(result['scene_level'].items()):
        if '1gram' in key or '5gram' in key or 'num_sentences' in key:
            print(f"  {key:45s}: {value:10.4f}")
    
    print(f"\n  Total scene-level features: {len(result['scene_level'])}")
    
    print("\n" + "="*80)
    print("Feature Comparison")
    print("="*80)
    print("""
N-gram Surprisal (this module):
  â€¢ Model: KenLM n-gram LM (unigram through 5-gram)
  â€¢ Scene-level: 86 features (17 features Ã— 5 orders + count)
  â€¢ Sentence-level: Per-sentence surprisal for all orders
  â€¢ Speed: Very fast (100-1000x faster than BERT/GPT-2)
  â€¢ Use cases:
    - Scene classification (scene-level features)
    - Sentence analysis (sentence-level data)
    - Multi-scale predictability (1-5 gram orders)

GPT-2 Surprisal (surprisal.py):
  â€¢ Model: GPT-2/DistilGPT-2 (autoregressive)
  â€¢ Features: 6 scene-level

BERT Surprisal (bert_feature.py):
  â€¢ Model: BERT-large (masked LM)
  â€¢ Features: 29 scene-level
  â€¢ Word-level: Yes (with subword handling)
  
Combined: 6 + 29 + 86 = 121 surprisal features!
    """)

================
File: features/ngram.py
================
"""
Discriminative N-gram Features for CCG Surface Realization
Based on White & Rajkumar (2009) "Perceptron Reranking for CCG Realization"

Key insight: Count occurrences of each n-gram rather than using 
log-probability as a single feature value.
"""

from collections import defaultdict
from typing import List, Dict, Tuple
import re
import pandas as pd
import nltk
from tqdm.auto import tqdm


class DiscriminativeNgramExtractor:
    """
    Extracts discriminative n-gram features as described in Section 3.
    
    Unlike generative language models that compute log-probabilities,
    discriminative n-gram features count the occurrences of each specific
    n-gram in the candidate realization.
    
    The paper uses a factored language model over:
    - Words
    - Named entity classes (semantic classes)
    - Part-of-speech tags
    - Supertags (CCG categories)
    """
    
    def __init__(self, n: int = 3):
        """
        Args:
            n: Maximum n-gram order (paper uses trigrams, n=3)
        """
        self.n = n
    
    def extract_ngrams(self, sequence: List[str], n: int) -> List[Tuple[str, ...]]:
        """Extract all n-grams of order n from a sequence."""
        if n > len(sequence):
            return []
        return [tuple(sequence[i:i+n]) for i in range(len(sequence) - n + 1)]
    
    def extract_features(self, 
                        words: List[str],
                        pos_tags: List[str] = None,
                        ne_classes: List[str] = None,
                        supertags: List[str] = None) -> Dict[str, int]:
        """
        Extract discriminative n-gram features from a candidate realization.
        
        Args:
            words: List of word tokens
            pos_tags: Part-of-speech tags (optional)
            ne_classes: Named entity classes/semantic classes (optional)
            supertags: CCG lexical categories (optional)
            
        Returns:
            Dictionary mapping feature names to counts
            
        Example:
            words = ["He", "has", "a", "point"]
            pos = ["PRP", "VBZ", "DT", "NN"]
            
            Features include:
            - word_unigram_He: 1
            - word_bigram_He_has: 1
            - word_trigram_He_has_a: 1
            - pos_unigram_PRP: 1
            - pos_bigram_PRP_VBZ: 1
            etc.
        """
        features = defaultdict(int)
        
        # Extract word n-grams (unigrams, bigrams, trigrams)
        for order in range(1, min(self.n + 1, len(words) + 1)):
            ngrams = self.extract_ngrams(words, order)
            for ngram in ngrams:
                feature_name = f"word_{order}gram_{'_'.join(ngram)}"
                features[feature_name] += 1
        
        # Extract POS tag n-grams if provided
        if pos_tags:
            for order in range(1, min(self.n + 1, len(pos_tags) + 1)):
                ngrams = self.extract_ngrams(pos_tags, order)
                for ngram in ngrams:
                    feature_name = f"pos_{order}gram_{'_'.join(ngram)}"
                    features[feature_name] += 1
        
        # Extract named entity class n-grams if provided
        # (Used to handle data sparsity for rare words in same semantic class)
        if ne_classes:
            for order in range(1, min(self.n + 1, len(ne_classes) + 1)):
                ngrams = self.extract_ngrams(ne_classes, order)
                for ngram in ngrams:
                    feature_name = f"ne_{order}gram_{'_'.join(ngram)}"
                    features[feature_name] += 1
        
        # Extract supertag n-grams if provided
        # (CCG lexical categories as fine-grained syntactic labels)
        if supertags:
            for order in range(1, min(self.n + 1, len(supertags) + 1)):
                ngrams = self.extract_ngrams(supertags, order)
                for ngram in ngrams:
                    # Sanitize supertags for feature names (contain special chars)
                    sanitized = '_'.join(self._sanitize(t) for t in ngram)
                    feature_name = f"supertag_{order}gram_{sanitized}"
                    features[feature_name] += 1
        
        return dict(features)
    
    def _sanitize(self, text: str) -> str:
        """Sanitize text for use in feature names."""
        return re.sub(r'[^a-zA-Z0-9]', '', text)


class PerceptronReranker:
    """
    Averaged perceptron model for reranking candidate realizations.
    
    Combines three types of features (as in the paper):
    1. N-gram log probabilities (from generative LM) - baseline features
    2. Syntactic features (from CCG derivation)
    3. Discriminative n-gram features (THIS MODULE)
    """
    
    def __init__(self):
        self.weights = defaultdict(float)
        self.weight_sum = defaultdict(float)  # For averaging
        self.updates = 0
        
    def score(self, features: Dict[str, float]) -> float:
        """Compute score for a candidate using current weights."""
        return sum(self.weights[f] * v for f, v in features.items())
    
    def predict(self, candidates: List[Dict[str, float]]) -> int:
        """Return index of highest-scoring candidate."""
        scores = [self.score(feat) for feat in candidates]
        return max(range(len(scores)), key=lambda i: scores[i])
    
    def update(self, correct_features: Dict[str, float], 
               predicted_features: Dict[str, float]):
        """
        Perceptron update rule:
        Î± = Î± + Î¦(x_i, y_i) - Î¦(x_i, z_i)
        
        where y_i is oracle-best and z_i is predicted
        """
        # Add weight for correct features
        for feat, val in correct_features.items():
            self.weights[feat] += val
            self.weight_sum[feat] += val
            
        # Subtract weight for incorrect prediction
        for feat, val in predicted_features.items():
            self.weights[feat] -= val
            self.weight_sum[feat] -= val
            
        self.updates += 1
    
    def average_weights(self):
        """Compute averaged weights (after training)."""
        if self.updates > 0:
            self.weights = {f: self.weight_sum[f] / self.updates 
                          for f in self.weight_sum}


def add_ngram_features(df: pd.DataFrame, text_col: str = "scene_text", n: int = 3) -> pd.DataFrame:
    """
    Add discriminative n-gram features to a DataFrame.
    
    This function extracts word n-grams and POS tag n-grams from text and
    aggregates them into summary statistics suitable for regression/classification.
    
    Args:
        df: Input DataFrame
        text_col: Column containing text to extract features from
        n: Maximum n-gram order (default: 3 for trigrams)
    
    Returns:
        DataFrame with added n-gram feature columns
    """
    df = df.copy()
    extractor = DiscriminativeNgramExtractor(n=n)
    
    # Extract features for each text
    unique_unigrams_list = []
    unique_bigrams_list = []
    unique_trigrams_list = []
    total_unigrams_list = []
    total_bigrams_list = []
    total_trigrams_list = []
    
    for text in tqdm(df[text_col].tolist(), desc="N-gram features"):
        # Tokenize
        words = nltk.word_tokenize(text.lower())
        
        # Extract features
        features = extractor.extract_features(words)
        
        # Count unique and total n-grams of each order
        unigrams = {k: v for k, v in features.items() if k.startswith("word_1gram_")}
        bigrams = {k: v for k, v in features.items() if k.startswith("word_2gram_")}
        trigrams = {k: v for k, v in features.items() if k.startswith("word_3gram_")}
        
        unique_unigrams_list.append(len(unigrams))
        unique_bigrams_list.append(len(bigrams))
        unique_trigrams_list.append(len(trigrams))
        total_unigrams_list.append(sum(unigrams.values()))
        total_bigrams_list.append(sum(bigrams.values()))
        total_trigrams_list.append(sum(trigrams.values()))
    
    # Add as columns
    df["ngram_unique_unigrams"] = unique_unigrams_list
    df["ngram_unique_bigrams"] = unique_bigrams_list
    df["ngram_unique_trigrams"] = unique_trigrams_list
    df["ngram_total_unigrams"] = total_unigrams_list
    df["ngram_total_bigrams"] = total_bigrams_list
    df["ngram_total_trigrams"] = total_trigrams_list
    
    # Compute diversity ratios
    df["ngram_unigram_diversity"] = df["ngram_unique_unigrams"] / df["ngram_total_unigrams"].replace(0, 1)
    df["ngram_bigram_diversity"] = df["ngram_unique_bigrams"] / df["ngram_total_bigrams"].replace(0, 1)
    df["ngram_trigram_diversity"] = df["ngram_unique_trigrams"] / df["ngram_total_trigrams"].replace(0, 1)
    
    return df


# Example usage demonstrating the key difference
def compare_feature_types():
    """
    Demonstrates the difference between:
    1. Log-probability features (single feature with LM score)
    2. Discriminative n-gram features (one feature per n-gram)
    """
    
    extractor = DiscriminativeNgramExtractor(n=3)
    
    # Example sentence from the paper
    words = ["He", "has", "a", "point", "he", "wants", "to", "make"]
    pos = ["PRP", "VBZ", "DT", "NN", "PRP", "VBZ", "TO", "VB"]
    
    print("=" * 70)
    print("DISCRIMINATIVE N-GRAM FEATURES")
    print("=" * 70)
    print("\nExample sentence:", " ".join(words))
    print()
    
    # Extract discriminative n-gram features
    features = extractor.extract_features(words, pos_tags=pos)
    
    print(f"Total features extracted: {len(features)}")
    print("\nSample word features:")
    for feat, count in list(features.items())[:10]:
        print(f"  {feat}: {count}")
    
    print("\n" + "=" * 70)
    print("KEY DIFFERENCES FROM LOG-PROBABILITY FEATURES")
    print("=" * 70)
    print("""
1. LOG-PROBABILITY FEATURES (baseline):
   - Single feature: "lm_logprob" = -45.23
   - One value representing overall sequence probability
   
2. DISCRIMINATIVE N-GRAM FEATURES (this implementation):
   - Separate feature for EACH n-gram: 
     * "word_1gram_He": 1
     * "word_2gram_He_has": 1  
     * "word_3gram_He_has_a": 1
     * ... (100s-1000s of features)
   - Counts allow learning specific weights for each n-gram
   
3. BENEFIT (from paper):
   "Discriminative training with n-gram features has the potential 
   to learn to negatively weight n-grams that appear in some of the 
   GEN(x_i) candidates, but which never appear in the naturally 
   occurring corpus used to train a standard, generative language model."
   
   Example: The model can learn that "video-viewing small" is bad
            even if the LM assigns it reasonable probability.
    """)
    
    return features


def demonstration_with_reranking():
    """
    Demonstrates how discriminative n-gram features are used in reranking.
    """
    print("\n" + "=" * 70)
    print("RERANKING EXAMPLE")
    print("=" * 70)
    
    extractor = DiscriminativeNgramExtractor(n=2)  # Using bigrams for clarity
    
    # Two candidate realizations for the same logical form
    # (from Table 6 in the paper)
    candidate1_words = ["Taipei", "'s", "growing", "number", "of", 
                       "video-viewing", "small", "parlors"]
    candidate2_words = ["Taipei", "'s", "growing", "number", "of", 
                       "small", "video-viewing", "parlors"]
    
    # Extract features for both
    feat1 = extractor.extract_features(candidate1_words)
    feat2 = extractor.extract_features(candidate2_words)
    
    print("\nCandidate 1 (incorrect word order):")
    print(" ".join(candidate1_words))
    print(f"Features: {len(feat1)}")
    
    print("\nCandidate 2 (correct word order):")
    print(" ".join(candidate2_words))
    print(f"Features: {len(feat2)}")
    
    # Show discriminating features
    print("\nKey discriminating bigrams:")
    print("  Candidate 1 has: 'word_2gram_video-viewing_small'")
    print("  Candidate 2 has: 'word_2gram_small_video-viewing'")
    print("\n  â†’ Perceptron can learn negative weight for the first,")
    print("    positive weight for the second")


if __name__ == "__main__":
    # Run demonstrations
    features = compare_feature_types()
    demonstration_with_reranking()
    
    print("\n" + "=" * 70)
    print("IMPLEMENTATION NOTES")
    print("=" * 70)
    print("""
This implementation demonstrates discriminative n-gram features from:
White & Rajkumar (2009), Section 3, pages 5-6

Key points from the paper:
1. Features count n-gram occurrences rather than using log-probs
2. Applied to factored LM: words, NE classes, POS tags, supertags
3. Combined with log-prob features and syntactic features
4. Trained using averaged perceptron (Collins 2002)
5. Achieved 0.8506 BLEU on CCGbank Section 23

Training details (Table 2):
- Full model: 576,176 features (from 2.4M alphabet)
- 10 iterations, ~9 hours training time
- 96.40% training accuracy
    """)

================
File: features/plot_shifts.py
================
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Any, List

import numpy as np
import pandas as pd

import nltk
from tqdm.auto import tqdm

try:
    from nltk.corpus import stopwords
except LookupError:
    nltk.download("stopwords")
    from nltk.corpus import stopwords

# Ensure punkt for sentence tokenization used later for surprisal consistency
try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")

EN_STOPWORDS = set(stopwords.words("english"))


def _tokenize_words(text: str) -> List[str]:
    return [w for w in nltk.word_tokenize(text) if any(c.isalpha() for c in w)]


def type_token_ratio(text: str, exclude_function_words: bool = False) -> float:
    tokens = _tokenize_words(text)
    if exclude_function_words:
        tokens = [t for t in tokens if t.lower() not in EN_STOPWORDS]
    if not tokens:
        return 0.0
    unique = set(t.lower() for t in tokens)
    return float(len(unique)) / float(len(tokens))


def add_ttr_features(df: pd.DataFrame, text_col: str = "scene_text") -> pd.DataFrame:
    df = df.copy()
    df["ttr"] = [
        type_token_ratio(x, exclude_function_words=False)
        for x in tqdm(df[text_col].tolist(), desc="TTR")
    ]
    # Drop ttr_no_func per pruning suggestion
    return df


def _sentence_tokenize(text: str) -> List[str]:
    try:
        return nltk.sent_tokenize(text)
    except Exception:
        return [text]


def add_length_structure_features(df: pd.DataFrame, text_col: str = "scene_text") -> pd.DataFrame:
    df = df.copy()
    texts = df[text_col].tolist()
    sent_counts: List[int] = []
    token_counts: List[int] = []
    avg_sent_len: List[float] = []
    var_sent_len: List[float] = []
    exclaim_rate: List[float] = []
    question_rate: List[float] = []
    uppercase_ratio: List[float] = []
    dialogue_ratio: List[float] = []
    for t in tqdm(texts, desc="Len/Struct"):
        sents = _sentence_tokenize(t or "")
        sents = [s for s in sents if s.strip()]
        sent_counts.append(len(sents))
        words = _tokenize_words(t or "")
        token_counts.append(len(words))
        slens = [len(_tokenize_words(s)) for s in sents] or [0]
        avg_sent_len.append(float(np.mean(slens)))
        var_sent_len.append(float(np.var(slens)))
        text_len = max(1, len(t))
        exclaim_rate.append(t.count("!") / text_len)
        question_rate.append(t.count("?") / text_len)
        upper = sum(1 for c in t if c.isupper())
        letters = sum(1 for c in t if c.isalpha())
        uppercase_ratio.append((upper / letters) if letters else 0.0)
        # simple dialogue heuristic: lines starting with uppercase word + colon, or quoted text
        lines = [ln.strip() for ln in t.splitlines() if ln.strip()]
        dialogue_lines = [ln for ln in lines if (":" in ln.split(" ")[0] if ln.split(" ") else False) or ("\"" in ln or "'" in ln)]
        dialogue_ratio.append((len(dialogue_lines) / len(lines)) if lines else 0.0)
    df["sentence_count"] = sent_counts
    df["token_count"] = token_counts
    df["avg_sentence_len"] = avg_sent_len
    df["var_sentence_len"] = var_sent_len
    df["exclaim_rate"] = exclaim_rate
    df["question_rate"] = question_rate
    df["uppercase_ratio"] = uppercase_ratio
    df["dialogue_ratio"] = dialogue_ratio
    return df


def add_position_overlap_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    # position
    if "scene_index" in df.columns:
        by_movie = df.groupby("movie_id")["scene_index"].transform("max").replace(0, 1)
        df["scene_index_norm"] = df["scene_index"] / by_movie
    else:
        df["scene_index_norm"] = 0.0
    # lexical overlap with neighbors (Jaccard)
    def jaccard(a: set, b: set) -> float:
        if not a and not b:
            return 0.0
        return float(len(a & b)) / float(len(a | b))
    prev_overlap: List[float] = []
    next_overlap: List[float] = []
    for i in tqdm(range(len(df)), desc="Overlap"):
        cur = set(w.lower() for w in _tokenize_words(df.loc[i, "scene_text"]) if w)
        # find prev/next within same movie
        prev_set: set = set()
        next_set: set = set()
        if i > 0 and df.loc[i - 1, "movie_id"] == df.loc[i, "movie_id"]:
            prev_set = set(w.lower() for w in _tokenize_words(df.loc[i - 1, "scene_text"]))
        if i + 1 < len(df) and df.loc[i + 1, "movie_id"] == df.loc[i, "movie_id"]:
            next_set = set(w.lower() for w in _tokenize_words(df.loc[i + 1, "scene_text"]))
        prev_overlap.append(jaccard(cur, prev_set))
        next_overlap.append(jaccard(cur, next_set))
    df["overlap_prev"] = prev_overlap
    df["overlap_next"] = next_overlap
    return df


def add_similarity_change_features(df: pd.DataFrame, text_col: str = "scene_text") -> pd.DataFrame:
    """
    Detect plot shifts through changes in similarity patterns.
    
    NEW FEATURES:
    - sim_change_magnitude: How different is this vs the trend?
    - vocab_novelty: New words appearing (reveals/plot twists)
    - dialogue_shift: Sudden change in dialogue amount
    
    Key insight: Salient scenes break from the pattern.
    """
    df = df.copy()
    
    # Initialize columns
    df["sim_change_magnitude"] = 0.0
    df["vocab_novelty"] = 0.0
    df["dialogue_shift"] = 0.0
    
    # Process each movie separately
    for movie_id, movie_group in tqdm(df.groupby("movie_id"), desc="Similarity changes"):
        movie_df = movie_group.sort_values("scene_index").copy()
        indices = movie_df.index.tolist()
        
        # Pre-tokenize all texts
        all_texts = []
        all_words = []
        all_dialogue_counts = []
        
        for idx in indices:
            text = movie_df.loc[idx, text_col]
            words = set(w.lower() for w in _tokenize_words(text))
            dialogue_count = text.count('"') + text.count("'")
            
            all_texts.append(text)
            all_words.append(words)
            all_dialogue_counts.append(dialogue_count)
        
        # Process each scene with context
        for i, idx in enumerate(indices):
            curr_words = all_words[i]
            curr_text = all_texts[i]
            curr_dialogue = all_dialogue_counts[i]
            
            # 1. SIMILARITY CHANGE MAGNITUDE
            # Compare current similarity with baseline trend
            if i >= 2:
                prev_words = all_words[i-1]
                prev_prev_words = all_words[i-2]
                
                # What was the "normal" similarity between prev scenes?
                baseline_sim = _jaccard_similarity(prev_words, prev_prev_words)
                
                # How similar is current to previous?
                current_sim = _jaccard_similarity(curr_words, prev_words)
                
                # Big change = plot shift / reveal moment
                change = abs(current_sim - baseline_sim)
                df.loc[idx, "sim_change_magnitude"] = float(change)
            
            # 2. VOCABULARY NOVELTY
            # New words = new concepts = reveals/twists
            if i > 0:
                prev_words = all_words[i-1]
                new_words = curr_words - prev_words
                novelty = len(new_words) / max(1, len(curr_words))
                df.loc[idx, "vocab_novelty"] = float(novelty)
            
            # 3. DIALOGUE SHIFT
            # Sudden change in dialogue amount
            if i > 0:
                prev_dialogue = all_dialogue_counts[i-1]
                text_length = max(1, len(curr_text.split()))
                shift = abs(curr_dialogue - prev_dialogue) / text_length
                df.loc[idx, "dialogue_shift"] = float(shift)
    
    return df


def add_structural_position_features(df: pd.DataFrame, text_col: str = "scene_text") -> pd.DataFrame:
    """
    Add three-act structure and thematic callback features.
    
    NEW FEATURES:
    - pos_edge_proximity: U-shaped importance (beginning/end salient)
    - pos_act: Which act (1=setup, 2=conflict, 3=resolution)
    - pos_within_act: Position within current act
    - callback_to_opening: Thematic return to opening
    - callback_to_ending: Foreshadowing of ending
    
    Key insight: Position matters. Edges and callbacks are salient.
    """
    df = df.copy()
    
    # Initialize columns
    df["pos_edge_proximity"] = 0.0
    df["pos_act"] = 1.0
    df["pos_within_act"] = 0.0
    df["callback_to_opening"] = 0.0
    df["callback_to_ending"] = 0.0
    
    # Process each movie separately
    for movie_id, movie_group in tqdm(df.groupby("movie_id"), desc="Structural position"):
        movie_df = movie_group.sort_values("scene_index").copy()
        indices = movie_df.index.tolist()
        total_scenes = len(indices)
        
        if total_scenes == 0:
            continue
        
        # Get opening and ending text
        first_text = movie_df.loc[indices[0], text_col]
        last_text = movie_df.loc[indices[-1], text_col]
        first_words = set(w.lower() for w in _tokenize_words(first_text))
        last_words = set(w.lower() for w in _tokenize_words(last_text))
        
        # Process each scene
        for i, idx in enumerate(indices):
            # Normalized position (0 = start, 1 = end)
            pos_norm = i / max(1, total_scenes - 1)
            
            # 1. EDGE PROXIMITY (U-shaped curve)
            # Maximum at edges (0 and 1), minimum at middle (0.5)
            # Formula: 1 - 2*|pos - 0.5|
            edge_proximity = 1.0 - 2.0 * abs(pos_norm - 0.5)
            df.loc[idx, "pos_edge_proximity"] = float(edge_proximity)
            
            # 2. THREE-ACT STRUCTURE
            # Act 1: 0-25% (setup)
            # Act 2: 25-75% (confrontation)
            # Act 3: 75-100% (resolution)
            if pos_norm < 0.25:
                act = 1
                within_act = pos_norm / 0.25
            elif pos_norm < 0.75:
                act = 2
                within_act = (pos_norm - 0.25) / 0.5
            else:
                act = 3
                within_act = (pos_norm - 0.75) / 0.25
            
            df.loc[idx, "pos_act"] = float(act)
            df.loc[idx, "pos_within_act"] = float(within_act)
            
            # 3. THEMATIC CALLBACKS
            # Similarity with opening/ending = structural callbacks
            curr_text = movie_df.loc[idx, text_col]
            curr_words = set(w.lower() for w in _tokenize_words(curr_text))
            
            callback_opening = _jaccard_similarity(curr_words, first_words)
            callback_ending = _jaccard_similarity(curr_words, last_words)
            
            df.loc[idx, "callback_to_opening"] = float(callback_opening)
            df.loc[idx, "callback_to_ending"] = float(callback_ending)
    
    return df


def _jaccard_similarity(set_a: set, set_b: set) -> float:
    """Compute Jaccard similarity between two sets."""
    if not set_a or not set_b:
        return 0.0
    intersection = len(set_a & set_b)
    union = len(set_a | set_b)
    return intersection / union if union > 0 else 0.0

================
File: features/rst.py
================
#!/usr/bin/env python3
"""
RST (Rhetorical Structure Theory) Feature Extractor

Provides feature extraction functions compatible with the MENSA pipeline.
These wrappers extract only scene-level features from the RST extractor.
"""

import warnings
import numpy as np
import pandas as pd
from typing import Dict, List
import torch

warnings.filterwarnings('ignore')


def add_rst_features(df: pd.DataFrame, text_col: str = "scene_text") -> pd.DataFrame:
    """
    Add RST (Rhetorical Structure Theory) structural features.
    
    This extracts only scene-level features. For word-level (EDU) data,
    use RSTFeatureExtractor directly.
    
    Args:
        df: DataFrame with scene texts
        text_col: Column containing text
        
    Returns:
        DataFrame with added rst_* columns
    """
    from feature_extractors import RSTFeatureExtractor
    
    print("  Initializing RST parser...")
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    extractor = RSTFeatureExtractor(device=device, preprocess=True)
    
    print(f"  Extracting RST features from {len(df)} scenes...")
    features_list = []
    
    from tqdm.auto import tqdm
    for text in tqdm(df[text_col], desc="  RST", leave=False):
        result = extractor.extract(text)
        # Extract only scene-level features
        scene_features = result['scene_level']
        features_list.append(scene_features)
    
    # Convert to dataframe and concatenate
    features_df = pd.DataFrame(features_list)
    result = pd.concat([df.reset_index(drop=True), features_df.reset_index(drop=True)], axis=1)
    
    return result


def extract_word_level_data(df: pd.DataFrame, text_col: str = "scene_text") -> pd.DataFrame:
    """
    Extract word-level EDU data from scenes.
    
    This is separate from the main pipeline and produces word-level DataFrames
    suitable for psycholinguistic analysis.
    
    Args:
        df: DataFrame with scene texts and metadata
        text_col: Column containing text
        
    Returns:
        DataFrame with EDU-level data (one row per EDU)
    """
    from feature_extractors import RSTFeatureExtractor
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    extractor = RSTFeatureExtractor(device=device, preprocess=True)
    
    all_rows = []
    from tqdm.auto import tqdm
    
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="Extracting RST EDU-level"):
        text = row[text_col]
        if not text or not text.strip():
            continue
        
        result = extractor.extract(text)
        word_level_data = result['word_level']
        
        # Add scene metadata to each EDU
        for word_item in word_level_data:
            word_row = {
                'movie_id': row.get('movie_id', idx),
                'scene_index': row.get('scene_index', 0),
                'saliency_score': row.get('label', None),
            }
            word_row.update(word_item)
            all_rows.append(word_row)
    
    return pd.DataFrame(all_rows)

================
File: features/structure.py
================
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Any, List

import numpy as np
import pandas as pd

import nltk
from tqdm.auto import tqdm

try:
    from nltk.corpus import stopwords
except LookupError:
    nltk.download("stopwords")
    from nltk.corpus import stopwords

# Ensure punkt for sentence tokenization used later for surprisal consistency
try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")

EN_STOPWORDS = set(stopwords.words("english"))


def _tokenize_words(text: str) -> List[str]:
    return [w for w in nltk.word_tokenize(text) if any(c.isalpha() for c in w)]


def type_token_ratio(text: str, exclude_function_words: bool = False) -> float:
    tokens = _tokenize_words(text)
    if exclude_function_words:
        tokens = [t for t in tokens if t.lower() not in EN_STOPWORDS]
    if not tokens:
        return 0.0
    unique = set(t.lower() for t in tokens)
    return float(len(unique)) / float(len(tokens))


def add_ttr_features(df: pd.DataFrame, text_col: str = "scene_text") -> pd.DataFrame:
    df = df.copy()
    df["ttr"] = [
        type_token_ratio(x, exclude_function_words=False)
        for x in tqdm(df[text_col].tolist(), desc="TTR")
    ]
    # Drop ttr_no_func per pruning suggestion
    return df


def _sentence_tokenize(text: str) -> List[str]:
    try:
        return nltk.sent_tokenize(text)
    except Exception:
        return [text]


def add_length_structure_features(df: pd.DataFrame, text_col: str = "scene_text") -> pd.DataFrame:
    df = df.copy()
    texts = df[text_col].tolist()
    sent_counts: List[int] = []
    token_counts: List[int] = []
    avg_sent_len: List[float] = []
    var_sent_len: List[float] = []
    exclaim_rate: List[float] = []
    question_rate: List[float] = []
    uppercase_ratio: List[float] = []
    dialogue_ratio: List[float] = []
    for t in tqdm(texts, desc="Len/Struct"):
        sents = _sentence_tokenize(t or "")
        sents = [s for s in sents if s.strip()]
        sent_counts.append(len(sents))
        words = _tokenize_words(t or "")
        token_counts.append(len(words))
        slens = [len(_tokenize_words(s)) for s in sents] or [0]
        avg_sent_len.append(float(np.mean(slens)))
        var_sent_len.append(float(np.var(slens)))
        text_len = max(1, len(t))
        exclaim_rate.append(t.count("!") / text_len)
        question_rate.append(t.count("?") / text_len)
        upper = sum(1 for c in t if c.isupper())
        letters = sum(1 for c in t if c.isalpha())
        uppercase_ratio.append((upper / letters) if letters else 0.0)
        # simple dialogue heuristic: lines starting with uppercase word + colon, or quoted text
        lines = [ln.strip() for ln in t.splitlines() if ln.strip()]
        dialogue_lines = [ln for ln in lines if (":" in ln.split(" ")[0] if ln.split(" ") else False) or ("\"" in ln or "'" in ln)]
        dialogue_ratio.append((len(dialogue_lines) / len(lines)) if lines else 0.0)
    df["sentence_count"] = sent_counts
    df["token_count"] = token_counts
    df["avg_sentence_len"] = avg_sent_len
    df["var_sentence_len"] = var_sent_len
    df["exclaim_rate"] = exclaim_rate
    df["question_rate"] = question_rate
    df["uppercase_ratio"] = uppercase_ratio
    df["dialogue_ratio"] = dialogue_ratio
    return df


def add_position_overlap_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    # position
    if "scene_index" in df.columns:
        by_movie = df.groupby("movie_id")["scene_index"].transform("max").replace(0, 1)
        df["scene_index_norm"] = df["scene_index"] / by_movie
    else:
        df["scene_index_norm"] = 0.0
    # lexical overlap with neighbors (Jaccard)
    def jaccard(a: set, b: set) -> float:
        if not a and not b:
            return 0.0
        return float(len(a & b)) / float(len(a | b))
    prev_overlap: List[float] = []
    next_overlap: List[float] = []
    for i in tqdm(range(len(df)), desc="Overlap"):
        cur = set(w.lower() for w in _tokenize_words(df.loc[i, "scene_text"]) if w)
        # find prev/next within same movie
        prev_set: set = set()
        next_set: set = set()
        if i > 0 and df.loc[i - 1, "movie_id"] == df.loc[i, "movie_id"]:
            prev_set = set(w.lower() for w in _tokenize_words(df.loc[i - 1, "scene_text"]))
        if i + 1 < len(df) and df.loc[i + 1, "movie_id"] == df.loc[i, "movie_id"]:
            next_set = set(w.lower() for w in _tokenize_words(df.loc[i + 1, "scene_text"]))
        prev_overlap.append(jaccard(cur, prev_set))
        next_overlap.append(jaccard(cur, next_set))
    df["overlap_prev"] = prev_overlap
    df["overlap_next"] = next_overlap
    return df


def add_similarity_change_features(df: pd.DataFrame, text_col: str = "scene_text") -> pd.DataFrame:
    """
    Detect plot shifts through changes in similarity patterns.
    
    NEW FEATURES:
    - sim_change_magnitude: How different is this vs the trend?
    - vocab_novelty: New words appearing (reveals/plot twists)
    - dialogue_shift: Sudden change in dialogue amount
    
    Key insight: Salient scenes break from the pattern.
    """
    df = df.copy()
    
    # Initialize columns
    df["sim_change_magnitude"] = 0.0
    df["vocab_novelty"] = 0.0
    df["dialogue_shift"] = 0.0
    
    # Process each movie separately
    for movie_id, movie_group in tqdm(df.groupby("movie_id"), desc="Similarity changes"):
        movie_df = movie_group.sort_values("scene_index").copy()
        indices = movie_df.index.tolist()
        
        # Pre-tokenize all texts
        all_texts = []
        all_words = []
        all_dialogue_counts = []
        
        for idx in indices:
            text = movie_df.loc[idx, text_col]
            words = set(w.lower() for w in _tokenize_words(text))
            dialogue_count = text.count('"') + text.count("'")
            
            all_texts.append(text)
            all_words.append(words)
            all_dialogue_counts.append(dialogue_count)
        
        # Process each scene with context
        for i, idx in enumerate(indices):
            curr_words = all_words[i]
            curr_text = all_texts[i]
            curr_dialogue = all_dialogue_counts[i]
            
            # 1. SIMILARITY CHANGE MAGNITUDE
            # Compare current similarity with baseline trend
            if i >= 2:
                prev_words = all_words[i-1]
                prev_prev_words = all_words[i-2]
                
                # What was the "normal" similarity between prev scenes?
                baseline_sim = _jaccard_similarity(prev_words, prev_prev_words)
                
                # How similar is current to previous?
                current_sim = _jaccard_similarity(curr_words, prev_words)
                
                # Big change = plot shift / reveal moment
                change = abs(current_sim - baseline_sim)
                df.loc[idx, "sim_change_magnitude"] = float(change)
            
            # 2. VOCABULARY NOVELTY
            # New words = new concepts = reveals/twists
            if i > 0:
                prev_words = all_words[i-1]
                new_words = curr_words - prev_words
                novelty = len(new_words) / max(1, len(curr_words))
                df.loc[idx, "vocab_novelty"] = float(novelty)
            
            # 3. DIALOGUE SHIFT
            # Sudden change in dialogue amount
            if i > 0:
                prev_dialogue = all_dialogue_counts[i-1]
                text_length = max(1, len(curr_text.split()))
                shift = abs(curr_dialogue - prev_dialogue) / text_length
                df.loc[idx, "dialogue_shift"] = float(shift)
    
    return df


def add_structural_position_features(df: pd.DataFrame, text_col: str = "scene_text") -> pd.DataFrame:
    """
    Add three-act structure and thematic callback features.
    
    NEW FEATURES:
    - pos_edge_proximity: U-shaped importance (beginning/end salient)
    - pos_act: Which act (1=setup, 2=conflict, 3=resolution)
    - pos_within_act: Position within current act
    - callback_to_opening: Thematic return to opening
    - callback_to_ending: Foreshadowing of ending
    
    Key insight: Position matters. Edges and callbacks are salient.
    """
    df = df.copy()
    
    # Initialize columns
    df["pos_edge_proximity"] = 0.0
    df["pos_act"] = 1.0
    df["pos_within_act"] = 0.0
    df["callback_to_opening"] = 0.0
    df["callback_to_ending"] = 0.0
    
    # Process each movie separately
    for movie_id, movie_group in tqdm(df.groupby("movie_id"), desc="Structural position"):
        movie_df = movie_group.sort_values("scene_index").copy()
        indices = movie_df.index.tolist()
        total_scenes = len(indices)
        
        if total_scenes == 0:
            continue
        
        # Get opening and ending text
        first_text = movie_df.loc[indices[0], text_col]
        last_text = movie_df.loc[indices[-1], text_col]
        first_words = set(w.lower() for w in _tokenize_words(first_text))
        last_words = set(w.lower() for w in _tokenize_words(last_text))
        
        # Process each scene
        for i, idx in enumerate(indices):
            # Normalized position (0 = start, 1 = end)
            pos_norm = i / max(1, total_scenes - 1)
            
            # 1. EDGE PROXIMITY (U-shaped curve)
            # Maximum at edges (0 and 1), minimum at middle (0.5)
            # Formula: 1 - 2*|pos - 0.5|
            edge_proximity = 1.0 - 2.0 * abs(pos_norm - 0.5)
            df.loc[idx, "pos_edge_proximity"] = float(edge_proximity)
            
            # 2. THREE-ACT STRUCTURE
            # Act 1: 0-25% (setup)
            # Act 2: 25-75% (confrontation)
            # Act 3: 75-100% (resolution)
            if pos_norm < 0.25:
                act = 1
                within_act = pos_norm / 0.25
            elif pos_norm < 0.75:
                act = 2
                within_act = (pos_norm - 0.25) / 0.5
            else:
                act = 3
                within_act = (pos_norm - 0.75) / 0.25
            
            df.loc[idx, "pos_act"] = float(act)
            df.loc[idx, "pos_within_act"] = float(within_act)
            
            # 3. THEMATIC CALLBACKS
            # Similarity with opening/ending = structural callbacks
            curr_text = movie_df.loc[idx, text_col]
            curr_words = set(w.lower() for w in _tokenize_words(curr_text))
            
            callback_opening = _jaccard_similarity(curr_words, first_words)
            callback_ending = _jaccard_similarity(curr_words, last_words)
            
            df.loc[idx, "callback_to_opening"] = float(callback_opening)
            df.loc[idx, "callback_to_ending"] = float(callback_ending)
    
    return df


def _jaccard_similarity(set_a: set, set_b: set) -> float:
    """Compute Jaccard similarity between two sets."""
    if not set_a or not set_b:
        return 0.0
    intersection = len(set_a & set_b)
    union = len(set_a | set_b)
    return intersection / union if union > 0 else 0.0

================
File: features/surprisal.py
================
from __future__ import annotations

from typing import List

import numpy as np
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import nltk

try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")

from nltk.tokenize import sent_tokenize


class SurprisalComputer:
    def __init__(self, model_name: str = "gpt2", device: str | None = None) -> None:
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = device
        # Ensure a pad token exists to avoid shape issues
        if self.tokenizer.pad_token is None and getattr(self.tokenizer, "eos_token", None) is not None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        self.model.to(self.device)
        self.model.eval()

    @torch.no_grad()
    def text_nll(self, text: str, max_length: int = 512) -> float:
        # Guard against empty/whitespace text
        cleaned = (text or "").strip()
        if not cleaned:
            cleaned = "."
        enc = self.tokenizer(
            cleaned,
            return_tensors="pt",
            truncation=True,
            max_length=max_length,
        )
        input_ids = enc["input_ids"].to(self.device)
        attn = enc["attention_mask"].to(self.device)
        # Fallback if tokenizer produced empty sequence
        if input_ids.numel() == 0 or input_ids.shape[-1] == 0:
            enc = self.tokenizer(".", return_tensors="pt")
            input_ids = enc["input_ids"].to(self.device)
            attn = enc.get("attention_mask", torch.ones_like(input_ids)).to(self.device)
        if input_ids.shape[-1] == 0:
            return 0.0
        outputs = self.model(input_ids, attention_mask=attn, labels=input_ids)
        # outputs.loss is mean cross-entropy per token (nats if model uses natural log)
        return float(outputs.loss.detach().cpu().item())

    def scene_surprisal_features(self, text: str) -> dict:
        # Sentence-level NLL as proxy for surprisal; compute mean and std
        sentences = sent_tokenize(text or "") or [text or "."]
        values: List[float] = [self.text_nll(s) for s in sentences if len(s.strip()) > 0]
        if not values:
            values = [self.text_nll(text or ".")]
        arr = np.array(values, dtype=np.float32)
        mean = float(np.mean(arr))
        std = float(np.std(arr))
        cv = float(std / mean) if mean != 0 else 0.0
        p75 = float(np.percentile(arr, 75))
        maxv = float(np.max(arr))
        # simple slope across sentence order
        if len(arr) >= 2:
            x = np.arange(len(arr), dtype=np.float32)
            x_mean = float(np.mean(x)); y_mean = mean
            num = float(np.sum((x - x_mean) * (arr - y_mean)))
            den = float(np.sum((x - x_mean) ** 2)) or 1.0
            slope = num / den
        else:
            slope = 0.0
        return {
            "surprisal_mean": mean,
            "surprisal_std": std,
            "surprisal_cv": cv,
            "surprisal_p75": p75,
            "surprisal_max": maxv,
            "surprisal_slope": float(slope),
        }

================
File: data.py
================
from __future__ import annotations

from dataclasses import dataclass
from typing import List, Dict, Any, Tuple

import pandas as pd
from datasets import load_dataset


@dataclass
class MensaSplit:
    name: str
    scenes: List[str]
    labels: List[int]
    movie_ids: List[int]
    scene_indices: List[int]


def load_mensa_split(split: str = "train") -> MensaSplit:
    """Load MENSA from Hugging Face and flatten to scene-level rows.

    Expects the MENSA dataset schema as used in the existing repo loader:
    item["scenes"] -> list[str]
    item["labels"] -> list[int]
    item["name"] or item["title"] (optional)
    """
    # The existing codebase uses this identifier successfully
    ds = load_dataset("rohitsaxena/MENSA", split=split)
    all_scenes: List[str] = []
    all_labels: List[int] = []
    movie_index: List[int] = []
    scene_indices: List[int] = []
    for movie_idx in range(len(ds)):
        item = ds[movie_idx]
        scenes = item.get("scenes") or item.get("script_scenes", [])
        labels = item.get("labels", [0] * len(scenes))
        for i, sc in enumerate(scenes):
            all_scenes.append(sc)
            all_labels.append(int(labels[i]))
            movie_index.append(movie_idx)
            scene_indices.append(i)

    return MensaSplit(name=split, scenes=all_scenes, labels=all_labels, movie_ids=movie_index, scene_indices=scene_indices)


def load_mensa_dataframe(split: str = "train") -> pd.DataFrame:
    ms = load_mensa_split(split)
    return pd.DataFrame({
        "scene_text": ms.scenes,
        "label": ms.labels,
        "movie_id": ms.movie_ids,
        "scene_index": ms.scene_indices,
    })

================
File: pca_ensemble.py
================
"""
Ensemble methods for MENSA scene saliency classification with PCA support.

This module provides ensemble learning utilities that combine multiple models
including both PCA-transformed and original feature spaces:
- Weighted soft voting ensemble
- Dirichlet-based weight search
- Stacking meta-learner with out-of-fold predictions
- Multiple model types (LR, SVM, RF, HGB, ExtraTrees, LightGBM)
"""

from __future__ import annotations

from typing import List, Dict, Tuple, Any
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.calibration import CalibratedClassifierCV
from sklearn.ensemble import (
    RandomForestClassifier,
    HistGradientBoostingClassifier,
    ExtraTreesClassifier,
)
from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone
from sklearn.metrics import f1_score, roc_auc_score, accuracy_score

try:
    from lightgbm import LGBMClassifier
    HAS_LGBM = True
except ImportError:
    HAS_LGBM = False


def create_base_models(
    random_state: int = 0,
    include_lgbm: bool = True,
    pos_weight_ratio: float | None = None,
) -> List[Tuple[str, Any]]:
    """
    Create a list of diverse base models for ensemble.
    
    Args:
        random_state: Random seed for reproducibility
        include_lgbm: Whether to include LightGBM (if available)
        pos_weight_ratio: Ratio for LightGBM scale_pos_weight (neg/pos counts)
        
    Returns:
        List of (name, model) tuples
    """
    models = [
        ("lr", LogisticRegression(
            max_iter=1000,
            class_weight="balanced",
            C=0.3,
            random_state=random_state
        )),
        ("svc", CalibratedClassifierCV(
            LinearSVC(class_weight="balanced", C=1.0, random_state=random_state),
            method="sigmoid",
            cv=2
        )),
        ("rf", RandomForestClassifier(
            n_estimators=300,
            min_samples_leaf=2,
            class_weight="balanced_subsample",
            n_jobs=-1,
            random_state=random_state
        )),
        ("hgb", HistGradientBoostingClassifier(
            learning_rate=0.06,
            max_leaf_nodes=31,
            max_iter=250,
            random_state=random_state
        )),
        ("extratrees", ExtraTreesClassifier(
            n_estimators=400,
            min_samples_leaf=2,
            class_weight="balanced_subsample",
            n_jobs=-1,
            random_state=random_state
        )),
    ]
    
    if include_lgbm and HAS_LGBM:
        lgbm = LGBMClassifier(
            n_estimators=400,
            learning_rate=0.05,
            num_leaves=31,
            subsample=0.8,
            colsample_bytree=0.8,
            scale_pos_weight=pos_weight_ratio or 1.0,
            random_state=random_state,
            verbose=-1,
        )
        models.append(("lgbm", lgbm))
    
    return models


def weighted_ensemble_predict(
    model_probs: np.ndarray,
    weights: np.ndarray,
) -> np.ndarray:
    """
    Make ensemble predictions using weighted soft voting.
    
    Args:
        model_probs: Array of shape (n_models, n_samples) with model probabilities
        weights: Array of shape (n_models,) with model weights (should sum to 1)
        
    Returns:
        Ensemble probabilities of shape (n_samples,)
    """
    return weights @ model_probs


def search_ensemble_weights_simplex(
    model_probs: np.ndarray,
    y_true: np.ndarray,
    threshold_finder_func,
    step: float = 0.1,
    metric: str = "macro",
) -> Dict[str, Any]:
    """
    Search for optimal ensemble weights using simplex grid search.
    
    Args:
        model_probs: Array of shape (n_models, n_samples)
        y_true: True labels
        threshold_finder_func: Function to find optimal threshold
        step: Grid step size (default: 0.1)
        metric: F1 averaging metric
        
    Returns:
        Dictionary with optimal weights, threshold, and scores
    """
    n_models = model_probs.shape[0]
    
    if n_models == 2:
        # Binary case: just vary w0, w1 = 1 - w0
        best = {"weights": None, "threshold": 0.5, "f1_macro": -1.0, "roc_auc": -1.0}
        for w0 in np.arange(0.0, 1.0 + step/2, step):
            w1 = 1.0 - w0
            weights = np.array([w0, w1])
            ens_probs = weighted_ensemble_predict(model_probs, weights)
            threshold, f1 = threshold_finder_func(y_true, ens_probs, metric=metric)
            auc = roc_auc_score(y_true, ens_probs)
            if f1 > best["f1_macro"] or (np.isclose(f1, best["f1_macro"]) and auc > best["roc_auc"]):
                best = {
                    "weights": weights,
                    "threshold": float(threshold),
                    "f1_macro": float(f1),
                    "roc_auc": float(auc),
                }
        return best
    
    elif n_models == 3:
        # Ternary case
        best = {"weights": None, "threshold": 0.5, "f1_macro": -1.0, "roc_auc": -1.0}
        for w0 in np.arange(0.0, 1.0 + step/2, step):
            for w1 in np.arange(0.0, 1.0 - w0 + step/2, step):
                w2 = 1.0 - w0 - w1
                if w2 < -1e-9:
                    continue
                weights = np.array([w0, w1, max(0.0, w2)])
                if not np.isclose(weights.sum(), 1.0, atol=1e-6):
                    continue
                ens_probs = weighted_ensemble_predict(model_probs, weights)
                threshold, f1 = threshold_finder_func(y_true, ens_probs, metric=metric)
                auc = roc_auc_score(y_true, ens_probs)
                if f1 > best["f1_macro"] or (np.isclose(f1, best["f1_macro"]) and auc > best["roc_auc"]):
                    best = {
                        "weights": weights,
                        "threshold": float(threshold),
                        "f1_macro": float(f1),
                        "roc_auc": float(auc),
                    }
        return best
    
    elif n_models == 4:
        # Quaternary case (from original code)
        best = {"weights": None, "threshold": 0.5, "f1_macro": -1.0, "roc_auc": -1.0}
        for w0 in np.arange(0.0, 1.0 + step/2, step):
            for w1 in np.arange(0.0, 1.0 - w0 + step/2, step):
                for w2 in np.arange(0.0, 1.0 - w0 - w1 + step/2, step):
                    w3 = 1.0 - (w0 + w1 + w2)
                    if w3 < -1e-9:
                        continue
                    weights = np.array([w0, w1, w2, max(0.0, w3)])
                    if not np.isclose(weights.sum(), 1.0, atol=1e-6):
                        continue
                    ens_probs = weighted_ensemble_predict(model_probs, weights)
                    threshold, f1 = threshold_finder_func(y_true, ens_probs, metric=metric)
                    auc = roc_auc_score(y_true, ens_probs)
                    if f1 > best["f1_macro"] or (np.isclose(f1, best["f1_macro"]) and auc > best["roc_auc"]):
                        best = {
                            "weights": weights,
                            "threshold": float(threshold),
                            "f1_macro": float(f1),
                            "roc_auc": float(auc),
                        }
        return best
    
    else:
        # Fall back to Dirichlet search for >4 models
        return search_ensemble_weights_dirichlet(
            model_probs, y_true, threshold_finder_func,
            n_trials=800, metric=metric, random_state=0
        )


def search_ensemble_weights_dirichlet(
    model_probs: np.ndarray,
    y_true: np.ndarray,
    threshold_finder_func,
    n_trials: int = 800,
    metric: str = "macro",
    random_state: int = 0,
) -> Dict[str, Any]:
    """
    Search for optimal ensemble weights using Dirichlet distribution sampling.
    
    Args:
        model_probs: Array of shape (n_models, n_samples)
        y_true: True labels
        threshold_finder_func: Function to find optimal threshold
        n_trials: Number of random weight combinations to try
        metric: F1 averaging metric
        random_state: Random seed
        
    Returns:
        Dictionary with optimal weights, threshold, and scores
    """
    n_models = model_probs.shape[0]
    rng = np.random.default_rng(random_state)
    
    best = {"weights": None, "threshold": 0.5, "f1_macro": -1.0, "roc_auc": -1.0}
    
    for _ in range(n_trials):
        weights = rng.dirichlet(np.ones(n_models))
        ens_probs = weighted_ensemble_predict(model_probs, weights)
        threshold, f1 = threshold_finder_func(y_true, ens_probs, metric=metric)
        auc = roc_auc_score(y_true, ens_probs)
        
        if f1 > best["f1_macro"] or (np.isclose(f1, best["f1_macro"]) and auc > best["roc_auc"]):
            best = {
                "weights": weights,
                "threshold": float(threshold),
                "f1_macro": float(f1),
                "roc_auc": float(auc),
            }
    
    return best


def train_pca_ensemble(
    X_train_std: np.ndarray,
    X_train_pca: np.ndarray,
    y_train: np.ndarray,
    X_val_std: np.ndarray,
    X_val_pca: np.ndarray,
    y_val: np.ndarray,
    threshold_finder_func,
    n_pca_components: int | None = None,
    random_state: int = 0,
    search_method: str = "dirichlet",
    verbose: bool = True,
) -> Dict[str, Any]:
    """
    Train ensemble combining models on both standard and PCA features.
    
    Args:
        X_train_std: Standardized training features
        X_train_pca: PCA-transformed training features
        y_train: Training labels
        X_val_std: Standardized validation features
        X_val_pca: PCA-transformed validation features
        y_val: Validation labels
        threshold_finder_func: Function to find optimal threshold (e.g., from pca_utils)
        n_pca_components: Number of PCA components to use (default: all)
        random_state: Random seed
        search_method: "dirichlet" or "simplex"
        verbose: Print progress
        
    Returns:
        Dictionary with ensemble results including models, weights, and metrics
    """
    if verbose:
        print("\n" + "="*80)
        print("TRAINING PCA ENSEMBLE")
        print("="*80)
    
    # Prepare PCA features
    if n_pca_components is not None:
        X_train_pca = X_train_pca[:, :n_pca_components]
        X_val_pca = X_val_pca[:, :n_pca_components]
    
    # Calculate class weights for LightGBM
    pos_count = np.sum(y_train == 1)
    neg_count = np.sum(y_train == 0)
    pos_weight = neg_count / max(1.0, pos_count)
    
    # Create models
    if verbose:
        print(f"\nTraining {4 if not HAS_LGBM else 5} base models on standard features...")
    models_std = create_base_models(random_state, include_lgbm=True, pos_weight_ratio=pos_weight)
    
    if verbose:
        print(f"Training {4 if not HAS_LGBM else 5} base models on PCA features ({X_train_pca.shape[1]} components)...")
    models_pca = create_base_models(random_state, include_lgbm=True, pos_weight_ratio=pos_weight)
    
    # Train models
    probs_dict = {}
    model_dict = {}
    
    for name, model in models_std:
        model.fit(X_train_std, y_train)
        probs_dict[f"{name}_std"] = model.predict_proba(X_val_std)[:, 1]
        model_dict[f"{name}_std"] = model
    
    for name, model in models_pca:
        model.fit(X_train_pca, y_train)
        probs_dict[f"{name}_pca"] = model.predict_proba(X_val_pca)[:, 1]
        model_dict[f"{name}_pca"] = model
    
    # Stack probabilities
    model_names = list(probs_dict.keys())
    model_probs = np.vstack([probs_dict[name] for name in model_names])
    
    if verbose:
        print(f"\nSearching optimal weights for {len(model_names)} models using {search_method}...")
    
    # Search for optimal weights
    if search_method == "simplex" and len(model_names) <= 4:
        results = search_ensemble_weights_simplex(
            model_probs, y_val, threshold_finder_func,
            step=0.1, metric="macro"
        )
    else:
        results = search_ensemble_weights_dirichlet(
            model_probs, y_val, threshold_finder_func,
            n_trials=800, metric="macro", random_state=random_state
        )
    
    results["model_names"] = model_names
    results["models"] = model_dict
    results["n_models"] = len(model_names)
    
    # Compute validation predictions
    ens_probs = weighted_ensemble_predict(model_probs, results['weights'])
    y_pred = (ens_probs >= results['threshold']).astype(int)
    
    # Add validation metrics
    from sklearn.metrics import accuracy_score
    results["y_proba"] = ens_probs
    results["y_pred"] = y_pred
    results["accuracy"] = float(accuracy_score(y_val, y_pred))
    results["f1_binary"] = float(f1_score(y_val, y_pred, average='binary'))
    # f1_macro already exists from the search
    
    if verbose:
        print(f"\n{'='*80}")
        print("ENSEMBLE RESULTS")
        print(f"{'='*80}")
        print(f"Models: {model_names}")
        print(f"Optimal weights: {np.round(results['weights'], 3).tolist()}")
        print(f"Optimal threshold: {results['threshold']:.4f}")
        print(f"Accuracy: {results['accuracy']:.4f}")
        print(f"F1 (binary): {results['f1_binary']:.4f}")
        print(f"F1 (macro): {results['f1_macro']:.4f}")
        print(f"ROC-AUC: {results['roc_auc']:.4f}")
        print(f"{'='*80}\n")
    
    return results


def train_stacking_ensemble(
    X_train_std: np.ndarray,
    y_train: np.ndarray,
    X_val_std: np.ndarray,
    y_val: np.ndarray,
    threshold_finder_func,
    n_folds: int = 5,
    random_state: int = 0,
    verbose: bool = True,
) -> Dict[str, Any]:
    """
    Train stacking meta-learner using out-of-fold predictions.
    
    Args:
        X_train_std: Standardized training features
        y_train: Training labels
        X_val_std: Standardized validation features
        y_val: Validation labels
        threshold_finder_func: Function to find optimal threshold
        n_folds: Number of cross-validation folds for OOF predictions
        random_state: Random seed
        verbose: Print progress
        
    Returns:
        Dictionary with stacking results
    """
    if verbose:
        print("\n" + "="*80)
        print("TRAINING STACKING ENSEMBLE")
        print("="*80)
    
    # Calculate class weights
    pos_count = np.sum(y_train == 1)
    neg_count = np.sum(y_train == 0)
    pos_weight = neg_count / max(1.0, pos_count)
    
    # Create base models
    base_models = create_base_models(random_state, include_lgbm=True, pos_weight_ratio=pos_weight)
    
    if verbose:
        print(f"\nGenerating out-of-fold predictions with {n_folds} folds...")
    
    # Generate OOF predictions
    oof_preds = np.zeros((len(y_train), len(base_models)))
    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)
    
    for model_idx, (name, model) in enumerate(base_models):
        if verbose:
            print(f"  {name}...", end=" ", flush=True)
        
        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_train_std, y_train)):
            model_clone = clone(model)
            model_clone.fit(X_train_std[train_idx], y_train[train_idx])
            
            if hasattr(model_clone, "predict_proba"):
                oof_preds[val_idx, model_idx] = model_clone.predict_proba(X_train_std[val_idx])[:, 1]
            else:
                # For models without predict_proba, use decision_function and scale
                scores = model_clone.decision_function(X_train_std[val_idx])
                scores = (scores - scores.min()) / (max(1e-9, scores.max() - scores.min()))
                oof_preds[val_idx, model_idx] = scores
        
        if verbose:
            print("âœ“")
    
    # Train meta-learner
    if verbose:
        print("\nTraining meta-learner (Logistic Regression)...")
    
    meta_model = LogisticRegression(max_iter=1000, class_weight=None, random_state=random_state)
    meta_model.fit(oof_preds, y_train)
    
    # Generate validation meta-features
    val_meta_features = np.zeros((len(y_val), len(base_models)))
    
    for model_idx, (name, model) in enumerate(base_models):
        # Refit on full training data
        model_clone = clone(model)
        model_clone.fit(X_train_std, y_train)
        
        if hasattr(model_clone, "predict_proba"):
            val_meta_features[:, model_idx] = model_clone.predict_proba(X_val_std)[:, 1]
        else:
            scores = model_clone.decision_function(X_val_std)
            scores = (scores - scores.min()) / (max(1e-9, scores.max() - scores.min()))
            val_meta_features[:, model_idx] = scores
    
    # Meta-model predictions
    ens_probs = meta_model.predict_proba(val_meta_features)[:, 1]
    threshold, f1 = threshold_finder_func(y_val, ens_probs, metric="macro")
    auc = roc_auc_score(y_val, ens_probs)
    y_pred = (ens_probs >= threshold).astype(int)
    
    results = {
        "meta_model": meta_model,
        "base_models": base_models,
        "threshold": float(threshold),
        "f1_macro": float(f1),
        "f1_binary": float(f1_score(y_val, y_pred, average='binary')),
        "roc_auc": float(auc),
        "accuracy": float(accuracy_score(y_val, y_pred)),
        "y_pred": y_pred,
        "y_proba": ens_probs,
    }
    
    if verbose:
        print(f"\n{'='*80}")
        print("STACKING RESULTS")
        print(f"{'='*80}")
        print(f"Base models: {[name for name, _ in base_models]}")
        print(f"Optimal threshold: {threshold:.4f}")
        print(f"Accuracy: {results['accuracy']:.4f}")
        print(f"F1 (binary): {results['f1_binary']:.4f}")
        print(f"F1 (macro): {f1:.4f}")
        print(f"ROC-AUC: {auc:.4f}")
        print(f"{'='*80}\n")
    
    return results

================
File: pca_utils.py
================
"""
PCA Analysis utilities for MENSA scene saliency classification.

This module provides functions for:
- PCA-based dimensionality reduction
- Feature variance analysis
- Component loadings visualization
- Threshold optimization for macro F1
"""

from __future__ import annotations

from typing import Tuple, List
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.metrics import f1_score


def fit_pca(
    X_train: np.ndarray,
    X_val: np.ndarray | None = None,
    n_components: int | float = 0.95,
    random_state: int = 0,
) -> Tuple[PCA, np.ndarray, np.ndarray | None]:
    """
    Fit PCA on training data and transform both train and validation sets.
    
    Args:
        X_train: Training feature matrix (samples x features)
        X_val: Optional validation feature matrix
        n_components: Number of components or variance ratio to preserve (default: 0.95)
        random_state: Random seed for reproducibility
        
    Returns:
        Tuple of (fitted PCA object, transformed train data, transformed val data)
    """
    # If n_components is a float between 0 and 1, it's interpreted as variance ratio
    # Otherwise use min of n_components and n_features
    if isinstance(n_components, float) and 0 < n_components < 1:
        pca = PCA(n_components=n_components, random_state=random_state)
    else:
        max_components = min(int(n_components), X_train.shape[1])
        pca = PCA(n_components=max_components, random_state=random_state)
    
    X_train_pca = pca.fit_transform(X_train)
    X_val_pca = pca.transform(X_val) if X_val is not None else None
    
    return pca, X_train_pca, X_val_pca


def get_explained_variance_info(pca: PCA, target_variance: float = 0.95) -> dict:
    """
    Get information about explained variance from a fitted PCA.
    
    Args:
        pca: Fitted PCA object
        target_variance: Target cumulative variance ratio (default: 0.95)
        
    Returns:
        Dictionary with variance statistics
    """
    explained_var = pca.explained_variance_ratio_
    cumulative_var = np.cumsum(explained_var)
    
    # Find number of components for target variance
    n_components_target = int(np.searchsorted(cumulative_var, target_variance) + 1)
    
    return {
        "explained_variance_ratio": explained_var,
        "cumulative_variance": cumulative_var,
        "n_components": len(explained_var),
        "n_components_for_target": n_components_target,
        "target_variance": target_variance,
        "variance_explained_first_10": explained_var[:10].tolist() if len(explained_var) >= 10 else explained_var.tolist(),
        "cumulative_variance_first_10": cumulative_var[:10].tolist() if len(cumulative_var) >= 10 else cumulative_var.tolist(),
    }


def get_component_loadings(
    pca: PCA,
    feature_names: List[str],
    n_components: int | None = None,
) -> pd.DataFrame:
    """
    Extract component loadings (feature contributions to each PC).
    
    Args:
        pca: Fitted PCA object
        feature_names: List of feature names
        n_components: Number of components to include (default: all)
        
    Returns:
        DataFrame with features as rows and components as columns
    """
    n_components = n_components or pca.n_components_
    n_components = min(n_components, pca.n_components_)
    
    loadings = pd.DataFrame(
        pca.components_[:n_components].T,
        index=feature_names,
        columns=[f"PC{i+1}" for i in range(n_components)]
    )
    
    return loadings


def get_top_loadings(
    loadings: pd.DataFrame,
    n_top: int = 10,
    by_component: str | None = None,
) -> pd.DataFrame:
    """
    Get features with highest absolute loadings.
    
    Args:
        loadings: DataFrame from get_component_loadings
        n_top: Number of top features to return per component
        by_component: Specific component to analyze (e.g., "PC1"), or None for all
        
    Returns:
        DataFrame with top features and their loadings
    """
    if by_component:
        if by_component not in loadings.columns:
            raise ValueError(f"Component {by_component} not found in loadings")
        
        top = loadings[by_component].abs().sort_values(ascending=False).head(n_top)
        result = pd.DataFrame({
            "feature": top.index,
            "loading": loadings.loc[top.index, by_component].values,
            "abs_loading": top.values,
            "component": by_component,
        })
        return result.reset_index(drop=True)
    
    # Get top features for all components
    rows = []
    for pc in loadings.columns:
        top = loadings[pc].abs().sort_values(ascending=False).head(n_top)
        for feat in top.index:
            rows.append({
                "component": pc,
                "feature": feat,
                "loading": loadings.loc[feat, pc],
                "abs_loading": abs(loadings.loc[feat, pc]),
            })
    
    result = pd.DataFrame(rows)
    return result


def find_optimal_threshold(
    y_true: np.ndarray,
    y_proba: np.ndarray,
    metric: str = "macro",
    n_thresholds: int = 100,
) -> Tuple[float, float]:
    """
    Find optimal probability threshold for classification by maximizing F1 score.
    
    Args:
        y_true: True binary labels
        y_proba: Predicted probabilities for positive class
        metric: F1 averaging method ("macro", "binary", "weighted")
        n_thresholds: Number of thresholds to try
        
    Returns:
        Tuple of (optimal threshold, best F1 score)
    """
    # Try both uniform grid and actual probability values
    thresholds = np.unique(np.concatenate([
        np.linspace(0, 1, n_thresholds),
        y_proba
    ]))
    
    best_f1 = -1.0
    best_threshold = 0.5
    
    for threshold in thresholds:
        y_pred = (y_proba >= threshold).astype(int)
        f1 = f1_score(y_true, y_pred, average=metric, zero_division=0)
        
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
    
    return float(best_threshold), float(best_f1)


def analyze_pca_classification(
    pca: PCA,
    X_train_pca: np.ndarray,
    X_val_pca: np.ndarray,
    y_train: np.ndarray,
    y_val: np.ndarray,
    feature_names: List[str],
    n_components_target: int | None = None,
    verbose: bool = True,
) -> dict:
    """
    Comprehensive PCA analysis including variance, loadings, and classification performance.
    
    Args:
        pca: Fitted PCA object
        X_train_pca: PCA-transformed training data
        X_val_pca: PCA-transformed validation data
        y_train: Training labels
        y_val: Validation labels
        feature_names: Original feature names
        n_components_target: Number of components to use (default: all)
        verbose: Whether to print analysis results
        
    Returns:
        Dictionary with analysis results
    """
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
    
    # Variance analysis
    variance_info = get_explained_variance_info(pca)
    
    # Use target components or all
    if n_components_target is None:
        n_components_target = variance_info["n_components_for_target"]
    n_components_target = min(n_components_target, X_train_pca.shape[1])
    
    # Train classifier on PCA features
    clf = LogisticRegression(max_iter=1000, class_weight=None, random_state=0)
    clf.fit(X_train_pca[:, :n_components_target], y_train)
    
    # Predictions
    y_pred = clf.predict(X_val_pca[:, :n_components_target])
    y_proba = clf.predict_proba(X_val_pca[:, :n_components_target])[:, 1]
    
    # Find optimal threshold
    opt_threshold, opt_f1 = find_optimal_threshold(y_val, y_proba, metric="macro")
    y_pred_opt = (y_proba >= opt_threshold).astype(int)
    
    # Component loadings
    loadings = get_component_loadings(pca, feature_names, n_components=10)
    top_loadings = get_top_loadings(loadings, n_top=10, by_component="PC1")
    
    results = {
        "variance_info": variance_info,
        "n_components_used": n_components_target,
        "classification_metrics": {
            "accuracy": float(accuracy_score(y_val, y_pred)),
            "f1_binary": float(f1_score(y_val, y_pred, average="binary", zero_division=0)),
            "f1_macro": float(f1_score(y_val, y_pred, average="macro", zero_division=0)),
            "f1_weighted": float(f1_score(y_val, y_pred, average="weighted", zero_division=0)),
            "roc_auc": float(roc_auc_score(y_val, y_proba)),
        },
        "optimal_threshold": {
            "threshold": opt_threshold,
            "f1_macro": opt_f1,
            "accuracy": float(accuracy_score(y_val, y_pred_opt)),
            "f1_binary": float(f1_score(y_val, y_pred_opt, average="binary", zero_division=0)),
        },
        "loadings": loadings,
        "top_features_pc1": top_loadings,
    }
    
    if verbose:
        print("\n" + "="*80)
        print("PCA ANALYSIS RESULTS")
        print("="*80)
        print(f"\nVariance explained by first 10 components:")
        for i, var in enumerate(variance_info["variance_explained_first_10"][:10], 1):
            cum_var = variance_info["cumulative_variance_first_10"][i-1]
            print(f"  PC{i}: {var:.4f} (cumulative: {cum_var:.4f})")
        
        print(f"\nComponents for 95% variance: {variance_info['n_components_for_target']}")
        print(f"Components used: {n_components_target}")
        
        print(f"\nClassification performance (default threshold=0.5):")
        metrics = results["classification_metrics"]
        print(f"  Accuracy:   {metrics['accuracy']:.4f}")
        print(f"  F1 (binary): {metrics['f1_binary']:.4f}")
        print(f"  F1 (macro):  {metrics['f1_macro']:.4f}")
        print(f"  ROC-AUC:     {metrics['roc_auc']:.4f}")
        
        print(f"\nOptimal threshold analysis:")
        opt = results["optimal_threshold"]
        print(f"  Best threshold: {opt['threshold']:.4f}")
        print(f"  F1 (macro):  {opt['f1_macro']:.4f}")
        print(f"  F1 (binary): {opt['f1_binary']:.4f}")
        print(f"  Accuracy:    {opt['accuracy']:.4f}")
        
        print(f"\nTop 10 features for PC1 (by absolute loading):")
        for _, row in results["top_features_pc1"].head(10).iterrows():
            print(f"  {row['feature']}: {row['loading']:.4f}")
        
        print("="*80 + "\n")
    
    return results

================
File: README.md
================
# MENSA Scene Saliency Classification

A comprehensive machine learning pipeline for classifying screenplay scene saliency using the MENSA dataset. This repository implements PCA ensemble methods and RFECV-based feature selection with extensive linguistic and narrative features.

## Overview

This project tackles the binary classification problem of identifying salient (memorable/important) scenes in movie screenplays. It extracts diverse linguistic, narrative, emotional, and structural features from screenplay text and employs advanced ensemble methods to achieve state-of-the-art performance.

## Features

### Feature Groups (30+ feature extractors)

The pipeline supports 30+ feature extraction groups organized into categories:

**Core Features:**
- `base`: Basic scene-level features (word count, sentence count, etc.)
- `structure`: Structural features (scene position, act structure)
- `character_arcs`: Character presence and interaction patterns
- `emotional`: Emotional trajectory features
- `plot_shifts`: Narrative shift detection

**Linguistic Complexity (GC Features):**
- `gc_basic`: Basic text statistics
- `gc_academic`: Academic vocabulary usage
- `gc_char_diversity`: Character and lexical diversity
- `gc_concreteness`: Concreteness ratings
- `gc_dialogue`: Dialogue-specific metrics
- `gc_discourse`: Discourse markers and connectives
- `gc_narrative`: Narrative tense and perspective
- `gc_polarity`: Sentiment and polarity
- `gc_pos`: Part-of-speech distributions
- `gc_pronouns`: Pronoun usage patterns
- `gc_punctuation`: Punctuation statistics
- `gc_readability`: Readability scores
- `gc_syntax`: Syntactic complexity
- `gc_temporal`: Temporal references

**Advanced NLP Features:**
- `ngram`: N-gram statistics
- `ngram_surprisal`: N-gram based surprisal scores
- `bert_surprisal`: BERT-based surprisal (requires GPU)
- `surprisal`: GPT-2 surprisal scores
- `rst`: Rhetorical Structure Theory features

All features are pre-extracted and cached on HuggingFace Hub for efficient loading.

## Installation

### Requirements

- Python 3.10+
- CUDA-compatible GPU (optional, for BERT/GPT features)

### Setup

1. Clone the repository:
```bash
git clone <repository-url>
cd lr_saliency
```

2. Create conda environment:
```bash
conda create -n saliency python=3.10
conda activate saliency
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Download additional NLP models:
```bash
# Spacy model
python -m spacy download en_core_web_sm

# NLTK data (downloads automatically on first use)
# Stanza models (downloads automatically on first use)
```

## Data

The pipeline uses the MENSA dataset from HuggingFace:
- Dataset: `rohitsaxena/MENSA`
- Cached features: `Ishaank18/screenplay-features`

Features are automatically downloaded from HuggingFace Hub when running training scripts.

## Usage

### Training Scripts

The repository includes two main training approaches:

#### 1. PCA Ensemble (`train_pca_ensemble.py`)

Trains an ensemble of PCA-transformed feature groups with optimized voting or stacking.

**Key Parameters:**
- `--groups`: Feature groups to use
- `--method`: Ensemble method (`voting` or `stacking`)
- `--search`: Weight optimization (`dirichlet` or `simplex`)
- `--pca_n_components`: PCA variance threshold (0.0-1.0)
- `--oversample`: Enable SMOTE oversampling
- `--undersample`: Enable undersampling
- `--undersample_method`: Method (`cluster_random` or `random`)
- `--undersample_clusters`: Number of clusters for cluster-based undersampling
- `--prefer_gc_overlap`: Prioritize overlapping GC features
- `--report_features`: Print detailed feature importance

#### 2. RFECV Feature Selection (`train_lr_rfecv.py`)

Trains logistic regression with recursive feature elimination and cross-validation.

**Key Parameters:**
- `--groups`: Feature groups to use
- `--rfecv_step`: Feature elimination step size
- `--rfecv_cv`: Cross-validation folds
- `--rfecv_scoring`: Scoring metric for RFECV
- `--lr_C`: Logistic regression regularization
- `--oversample`, `--undersample`: Same as PCA ensemble

### Pre-configured Run Scripts

The repository includes 5 pre-configured SLURM scripts for different feature configurations:

#### 1. **Full Feature Set** (`run_lr.sh`)
Uses all 30 feature groups including BERT and GPT surprisal:
```bash
sbatch run_lr.sh
```

**Features:** base, bert_surprisal, character_arcs, emotional, all GC features, ngram, ngram_surprisal, plot_shifts, rst, structure, surprisal

**Use case:** Maximum performance with all available features

#### 2. **Base Features** (`run_lr_base.sh`)
Excludes neural surprisal features (BERT, GPT):
```bash
sbatch run_lr_base.sh
```

**Features:** base, character_arcs, emotional, all GC features, ngram, plot_shifts, rst, structure

**Use case:** Fast training without GPU-intensive surprisal computation

#### 3. **BERT Variant** (`run_lr_bert.sh`)
Includes BERT surprisal but excludes GPT surprisal:
```bash
sbatch run_lr_bert.sh
```

**Features:** base, bert_surprisal, character_arcs, emotional, all GC features, ngram, ngram_surprisal, plot_shifts, rst, structure

**Use case:** BERT-based surprisal analysis

#### 4. **GPT Variant** (`run_lr_gpt.sh`)
Includes GPT surprisal and n-gram surprisal but excludes BERT:
```bash
sbatch run_lr_gpt.sh
```

**Features:** base, character_arcs, emotional, all GC features, ngram, ngram_surprisal, plot_shifts, rst, structure, surprisal

**Use case:** GPT-based surprisal analysis

#### 5. **RFECV Feature Selection** (`run_lr_rfecv.sh`)
Full feature set with RFECV for optimal feature selection:
```bash
sbatch run_lr_rfecv.sh
```

**Features:** Same as full feature set

**Use case:** Automatic feature selection and model simplification

### Common Configuration

All scripts use these default settings:
- Oversampling: SMOTE (enabled)
- Undersampling: Cluster-based with 20 clusters (enabled)
- PCA variance threshold: 0.95
- Ensemble method: Voting with Dirichlet weight optimization
- Prefer GC overlap: Enabled
- Output: `/scratch/ishaan.karan/pca_ensemble.pkl` (or `lr_rfecv.pkl` for RFECV)

### Local Execution

For local execution or custom configurations:

```bash
# PCA Ensemble with custom groups
python train_pca_ensemble.py \
    --groups base gc_polarity emotional ngram \
    --method voting \
    --search dirichlet \
    --pca_n_components 0.95 \
    --oversample \
    --undersample \
    --undersample_method cluster_random \
    --undersample_clusters 20 \
    --output models/my_model.pkl

# RFECV with specific groups
python train_lr_rfecv.py \
    --groups base gc_polarity emotional \
    --oversample \
    --undersample \
    --output models/rfecv_model.pkl
```

## Project Structure

```
lr_saliency/
â”œâ”€â”€ data.py                    # MENSA dataset loading
â”œâ”€â”€ train_pca_ensemble.py      # PCA ensemble training
â”œâ”€â”€ train_lr_rfecv.py          # RFECV feature selection training
â”œâ”€â”€ pca_ensemble.py            # Ensemble weight optimization
â”œâ”€â”€ pca_utils.py               # PCA utilities and threshold finding
â”œâ”€â”€ requirements.txt           # Python dependencies
â”‚
â”œâ”€â”€ feature_cache/             # Feature loading infrastructure
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ load_hf.py            # HuggingFace feature loading
â”‚   â””â”€â”€ extract.py            # Feature extraction registry
â”‚
â”œâ”€â”€ features/                  # Feature extractors (30+ modules)
â”‚   â”œâ”€â”€ base.py               # Basic features
â”‚   â”œâ”€â”€ character_arcs.py     # Character features
â”‚   â”œâ”€â”€ emotional.py          # Emotional features
â”‚   â”œâ”€â”€ structure.py          # Structural features
â”‚   â”œâ”€â”€ plot_shifts.py        # Plot shift detection
â”‚   â”œâ”€â”€ bert_surprisal.py     # BERT surprisal
â”‚   â”œâ”€â”€ surprisal.py          # GPT-2 surprisal
â”‚   â”œâ”€â”€ ngram*.py             # N-gram features
â”‚   â”œâ”€â”€ rst.py                # RST features
â”‚   â”œâ”€â”€ gc_*.py               # 14 GC feature modules
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ hf_feature_cache/          # Local HuggingFace cache
â”‚
â””â”€â”€ run_*.sh                   # SLURM job scripts (5 configs)
```

## Methodology

### Data Preprocessing

1. **Feature Loading**: Features loaded from HuggingFace Hub cache
2. **Feature Alignment**: Ensures consistent features across train/val/test splits
3. **Imputation**: Missing values filled with median strategy
4. **Class Balancing**:
   - SMOTE oversampling of minority class
   - Cluster-based undersampling of majority class (20 clusters)
5. **Standardization**: Features scaled with StandardScaler

### PCA Ensemble Method

1. **Group-wise PCA**: Each feature group transformed independently
2. **Variance Preservation**: Retains components explaining 95% variance
3. **Ensemble Training**: Individual classifiers per PCA-transformed group
4. **Weight Optimization**: 
   - Dirichlet sampling for voting weights
   - Bayesian optimization over weight simplex
5. **Threshold Tuning**: Optimal decision threshold found on validation set

### RFECV Method

1. **Recursive Elimination**: Iteratively removes least important features
2. **Cross-Validation**: 5-fold CV for robust feature ranking
3. **Scoring**: F1-macro score optimization
4. **Automatic Selection**: Determines optimal feature count

## Output

Both training scripts produce pickle files containing:
- Trained model(s)
- Feature importance/weights
- Selected features (RFECV)
- PCA transformers (PCA ensemble)
- Preprocessing pipeline
- Performance metrics

## Performance Metrics

The pipeline reports:
- **Accuracy**: Overall classification accuracy
- **F1 Score**: Macro-averaged F1 (primary metric)
- **Precision/Recall**: Per-class performance
- **ROC AUC**: Area under ROC curve
- **Confusion Matrix**: Classification breakdown

================
File: requirements.txt
================
# Comprehensive Requirements for LR Saliency Project
# MENSA Scene Saliency Classification

# Core scientific computing
numpy==1.26.4
pandas==2.2.2
scipy>=1.11.0

# Machine learning
scikit-learn==1.5.1
imbalanced-learn==0.12.4
lightgbm==4.6.0

# Deep learning (for BERT and neural features)
torch==2.5.0
transformers==4.46.3

# NLP tools
nltk==3.9.1
datasets==3.1.0
huggingface_hub>=0.20.0

# Progress bars and utilities
tqdm==4.66.4

# Data handling
pyarrow>=14.0.0
fastparquet>=2023.10.0

# NLP libraries (for advanced linguistic features)
spacy>=3.0.0
stanza>=1.4.0
https://github.com/kpu/kenlm/archive/master.zip

# Installation:
# pip install -r requirements.txt

# For conda environment:
# conda create -n saliency python=3.10
# conda activate saliency
# pip install -r requirements.txt

# Post-installation:
# - Spacy model: python -m spacy download en_core_web_sm
# - Stanza will auto-download English models on first use

================
File: run_lr_base.sh
================
#!/bin/bash
#SBATCH --job-name=run_lr_base
#SBATCH --output=run_lr_base.out
#SBATCH --error=run_lr_base.err
#SBATCH --partition=u22
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=9
#SBATCH --mem=20G
#SBATCH --time=96:00:00
#SBATCH -A research
#SBATCH --qos=medium
#SBATCH -w gnode063

# --- Environment setup ---
source ~/.bashrc
conda activate pyg


python train_pca_ensemble.py \
            --groups base character_arcs emotional \
                     gc_academic gc_basic gc_char_diversity gc_concreteness gc_dialogue \
                     gc_discourse gc_narrative gc_polarity gc_pos gc_pronouns \
                     gc_punctuation gc_readability gc_syntax gc_temporal \
                     ngram plot_shifts rst structure \
            --oversample \
            --undersample \
            --undersample_method cluster_random \
            --undersample_clusters 20 \
            --prefer_gc_overlap \
            --report_features \
            --method voting \
            --search dirichlet \
            --pca_n_components 0.95 \
            --output /scratch/ishaan.karan/pca_ensemble.pkl

================
File: run_lr_bert.sh
================
#!/bin/bash
#SBATCH --job-name=run_lr_bert
#SBATCH --output=run_lr_bert.out
#SBATCH --error=run_lr_bert.err
#SBATCH --partition=u22
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=9
#SBATCH --mem=20G
#SBATCH --time=96:00:00
#SBATCH -A research
#SBATCH --qos=medium
#SBATCH -w gnode063

# --- Environment setup ---
source ~/.bashrc
conda activate pyg


python train_pca_ensemble.py \
            --groups base bert_surprisal character_arcs emotional \
                     gc_academic gc_basic gc_char_diversity gc_concreteness gc_dialogue \
                     gc_discourse gc_narrative gc_polarity gc_pos gc_pronouns \
                     gc_punctuation gc_readability gc_syntax gc_temporal \
                     ngram ngram_surprisal plot_shifts rst structure \
            --oversample \
            --undersample \
            --undersample_method cluster_random \
            --undersample_clusters 20 \
            --prefer_gc_overlap \
            --report_features \
            --method voting \
            --search dirichlet \
            --pca_n_components 0.95 \
            --output /scratch/ishaan.karan/pca_ensemble.pkl

================
File: run_lr_gpt.sh
================
#!/bin/bash
#SBATCH --job-name=run_lr_gpt
#SBATCH --output=run_lr_gpt.out
#SBATCH --error=run_lr_gpt.err
#SBATCH --partition=u22
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=9
#SBATCH --mem=20G
#SBATCH --time=96:00:00
#SBATCH -A research
#SBATCH --qos=medium
#SBATCH -w gnode063

# --- Environment setup ---
source ~/.bashrc
conda activate pyg


python train_pca_ensemble.py \
            --groups base character_arcs emotional \
                     gc_academic gc_basic gc_char_diversity gc_concreteness gc_dialogue \
                     gc_discourse gc_narrative gc_polarity gc_pos gc_pronouns \
                     gc_punctuation gc_readability gc_syntax gc_temporal \
                     ngram ngram_surprisal plot_shifts rst structure surprisal \
            --oversample \
            --undersample \
            --undersample_method cluster_random \
            --undersample_clusters 20 \
            --prefer_gc_overlap \
            --report_features \
            --method voting \
            --search dirichlet \
            --pca_n_components 0.95 \
            --output /scratch/ishaan.karan/pca_ensemble.pkl

================
File: run_lr_rfecv.sh
================
#!/bin/bash
#SBATCH --job-name=run_lr_rfecv
#SBATCH --output=run_lr_rfecv.out
#SBATCH --error=run_lr_rfecv.err
#SBATCH --partition=u22
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=9
#SBATCH --mem=20G
#SBATCH --time=96:00:00
#SBATCH -A research
#SBATCH --qos=medium
#SBATCH -w gnode063

# --- Environment setup ---
source ~/.bashrc
conda activate pyg

python train_lr_rfecv.py \
            --groups base bert_surprisal character_arcs emotional \
                     gc_academic gc_basic gc_char_diversity gc_concreteness gc_dialogue \
                     gc_discourse gc_narrative gc_polarity gc_pos gc_pronouns \
                     gc_punctuation gc_readability gc_syntax gc_temporal \
                     ngram ngram_surprisal plot_shifts rst structure surprisal \
            --oversample \
            --undersample \
            --undersample_method cluster_random \
            --undersample_clusters 20 \
            --prefer_gc_overlap \
            --output /scratch/ishaan.karan/lr_rfecv.pkl

================
File: run_lr.sh
================
#!/bin/bash
#SBATCH --job-name=run_lr
#SBATCH --output=run_lr.out
#SBATCH --error=run_lr.err
#SBATCH --partition=u22
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=9
#SBATCH --mem=20G
#SBATCH --time=96:00:00
#SBATCH -A research
#SBATCH --qos=medium
#SBATCH -w gnode063

# --- Environment setup ---
source ~/.bashrc
conda activate pyg


python train_pca_ensemble.py \
            --groups base bert_surprisal character_arcs emotional \
                     gc_academic gc_basic gc_char_diversity gc_concreteness gc_dialogue \
                     gc_discourse gc_narrative gc_polarity gc_pos gc_pronouns \
                     gc_punctuation gc_readability gc_syntax gc_temporal \
                     ngram ngram_surprisal plot_shifts rst structure surprisal \
            --oversample \
            --undersample \
            --undersample_method cluster_random \
            --undersample_clusters 20 \
            --prefer_gc_overlap \
            --report_features \
            --method voting \
            --search dirichlet \
            --pca_n_components 0.95 \
            --output /scratch/ishaan.karan/pca_ensemble.pkl

================
File: train_lr_rfecv.py
================
#!/usr/bin/env python3
"""
Train Logistic Regression with RFECV (Recursive Feature Elimination with Cross-Validation)
for MENSA scene saliency classification.

Uses same data loading and balancing as train_pca_ensemble.py but with RFECV feature selection.
"""

from __future__ import annotations

import argparse
import pickle
import numpy as np
import pandas as pd

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.cluster import KMeans
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFECV
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, f1_score, accuracy_score, roc_auc_score

from feature_cache import load_feature_matrix
from pca_utils import find_optimal_threshold

try:
    from imblearn.over_sampling import SMOTE
    from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids
    HAS_IMBLEARN = True
except ImportError:
    HAS_IMBLEARN = False
    print("âš  Warning: imbalanced-learn not installed. Install with: pip install imbalanced-learn")


def apply_custom_cluster_undersampling(X_train, y_train, n_clusters=50, random_state=42):
    """
    Custom cluster-based undersampling matching train.py logic.
    Clusters majority class and samples proportionally from each cluster.
    """
    print(f"\nApplying custom cluster-based undersampling (cluster_random method):")
    print(f"  Strategy: K-Means clustering ({n_clusters} clusters) + random sampling within clusters")
    
    num_pos = int((y_train == 1).sum())
    num_neg = int((y_train == 0).sum())
    
    print(f"  Original distribution: {num_neg} non-salient, {num_pos} salient")
    
    if num_pos == 0 or num_neg == 0:
        return X_train, y_train
    
    # Determine minority and majority
    if num_pos <= num_neg:
        minority_label = 1
        majority_label = 0
        target_majority = num_pos
    else:
        minority_label = 0
        majority_label = 1
        target_majority = num_neg
    
    maj_idx = np.where(y_train == majority_label)[0]
    min_idx = np.where(y_train == minority_label)[0]
    X_maj = X_train[maj_idx]
    
    print(f"  Target: Reduce majority class from {len(maj_idx)} to {target_majority} (match minority)")
    
    # Cluster majority class
    k = int(max(1, min(n_clusters, len(maj_idx), target_majority)))
    print(f"\n  Step 1: K-Means clustering of {len(maj_idx)} majority scenes into {k} clusters...")
    
    try:
        km = KMeans(n_clusters=k, random_state=random_state, n_init=10)
        labels = km.fit_predict(X_maj)
        print(f"  âœ“ Clustering complete")
    except Exception:
        labels = np.zeros(len(maj_idx), dtype=int)
        k = 1
        print(f"  âš  Clustering failed, using single cluster")
    
    # Compute sampling quota per cluster
    cluster_sizes = np.bincount(labels, minlength=k).astype(np.int64)
    total_maj = int(len(maj_idx))
    
    print(f"\n  Step 2: Compute proportional sampling quota for each cluster...")
    print(f"    Cluster sizes (min/mean/max): {cluster_sizes.min()}/{cluster_sizes.mean():.0f}/{cluster_sizes.max()}")
    
    if total_maj > 0:
        raw_quota = (cluster_sizes / max(1, total_maj)) * float(target_majority)
        base_quota = np.floor(raw_quota).astype(int)
        remainder = int(target_majority - int(base_quota.sum()))
        frac = raw_quota - base_quota
        order = np.argsort(-frac)
        for i in range(min(remainder, len(order))):
            base_quota[order[i]] += 1
        
        print(f"    Sampling quotas per cluster (min/mean/max): {base_quota.min()}/{base_quota.mean():.0f}/{base_quota.max()}")
        
        # Sample from each cluster
        print(f"\n  Step 3: Random sampling within each cluster...")
        chosen = []
        rng = np.random.RandomState(random_state)
        for c in range(k):
            members = np.where(labels == c)[0]
            take = int(min(base_quota[c], len(members)))
            if take > 0:
                chosen_members = rng.choice(members, size=take, replace=False)
                chosen.append(chosen_members)
        
        if len(chosen):
            chosen_idx_rel = np.concatenate(chosen, axis=0)
            chosen_idx_abs = maj_idx[chosen_idx_rel]
            keep_idx = np.concatenate([min_idx, chosen_idx_abs], axis=0)
            X_train = X_train[keep_idx]
            y_train = y_train[keep_idx]
            
            final_neg = (y_train == 0).sum()
            final_pos = (y_train == 1).sum()
            print(f"  âœ“ Sampling complete: {final_neg} non-salient, {final_pos} salient")
            print(f"    Reduction: {len(maj_idx)} â†’ {final_neg} ({100*final_neg/len(maj_idx):.1f}%)")
    
    return X_train, y_train


def apply_sampling(X_train, y_train, args):
    """
    Apply data balancing matching train.py exactly.
    
    For cluster_random: Undersample first (custom), then SMOTE
    For other methods: SMOTE first, then undersample (via pipeline behavior)
    """
    if not HAS_IMBLEARN:
        print("\nâš  Cannot apply sampling: imbalanced-learn not installed")
        return X_train, y_train
    
    original_counts = np.bincount(y_train.astype(int))
    print(f"\nOriginal class distribution: {original_counts}")
    
    # Special handling for cluster_random (matches train.py)
    if args.undersample and args.undersample_method == "cluster_random":
        X_train, y_train = apply_custom_cluster_undersampling(
            X_train, y_train,
            n_clusters=args.undersample_clusters,
            random_state=args.random_state
        )
        after_under = np.bincount(y_train.astype(int))
        print(f"\nâœ“ After cluster_random undersampling: {after_under}")
        
        # Then optionally oversample
        if args.oversample:
            print(f"\nApplying SMOTE oversampling...")
            smote = SMOTE(random_state=args.random_state)
            X_train, y_train = smote.fit_resample(X_train, y_train)
            after_over = np.bincount(y_train.astype(int))
            print(f"âœ“ After SMOTE: {after_over}")
    
    # For other methods: SMOTE first, then undersample (matches train.py pipeline)
    else:
        # Oversample first (if requested)
        if args.oversample:
            print(f"\nApplying SMOTE oversampling...")
            smote = SMOTE(random_state=args.random_state)
            X_train, y_train = smote.fit_resample(X_train, y_train)
            after_over = np.bincount(y_train.astype(int))
            print(f"âœ“ After SMOTE: {after_over}")
        
        # Undersample second (if requested)
        if args.undersample:
            method_desc = {
                'random': 'Random undersampling (RandomUnderSampler)',
                'cluster': 'Cluster centroids (ClusterCentroids via K-Means)'
            }
            print(f"\nApplying undersampling (method: {args.undersample_method})...")
            print(f"  {method_desc.get(args.undersample_method, args.undersample_method)}")
            
            if args.undersample_method == "random":
                undersampler = RandomUnderSampler(random_state=args.random_state)
            elif args.undersample_method == "cluster":
                undersampler = ClusterCentroids(
                    estimator=KMeans(n_clusters=args.undersample_clusters, random_state=args.random_state, n_init=10),
                    random_state=args.random_state
                )
            else:
                print(f"âš  Unknown undersample method: {args.undersample_method}, skipping")
                return X_train, y_train
            
            X_train, y_train = undersampler.fit_resample(X_train, y_train)
            after_under = np.bincount(y_train.astype(int))
            print(f"âœ“ After undersampling: {after_under}")
    
    return X_train, y_train


def main():
    parser = argparse.ArgumentParser(description="Train LR with RFECV for scene saliency classification")
    
    # Feature groups
    parser.add_argument("--groups", nargs="+", required=True,
                       help="Feature groups to load")
    parser.add_argument("--hf_repo", default="Ishaank18/screenplay-features",
                       help="Hugging Face repo for features (default: Ishaank18/screenplay-features)")
    
    # Output
    parser.add_argument("--output", type=str, required=True,
                       help="Output file for results")
    
    # RFECV parameters
    parser.add_argument("--rfecv_step", type=float, default=0.05,
                       help="Step size for RFECV (fraction of features to remove at each iteration)")
    parser.add_argument("--rfecv_cv", type=int, default=5,
                       help="Number of CV folds for RFECV")
    parser.add_argument("--rfecv_scoring", type=str, default="f1_macro",
                       help="Scoring metric for RFECV")
    parser.add_argument("--rfecv_min_features", type=int, default=10,
                       help="Minimum number of features to select")
    
    # Logistic Regression parameters
    parser.add_argument("--lr_C", type=float, default=0.3,
                       help="Inverse regularization strength for LR")
    parser.add_argument("--lr_max_iter", type=int, default=1000,
                       help="Maximum iterations for LR")
    
    # Sampling
    parser.add_argument("--oversample", action="store_true",
                       help="Use SMOTE oversampling")
    parser.add_argument("--undersample", action="store_true",
                       help="Use undersampling")
    parser.add_argument("--undersample_method", type=str, default="cluster_random",
                       choices=["random", "cluster", "cluster_random"],
                       help="Undersampling method")
    parser.add_argument("--undersample_clusters", type=int, default=50,
                       help="Number of clusters for cluster-based undersampling")
    
    # Feature preferences
    parser.add_argument("--prefer_gc_overlap", action="store_true",
                       help="Prefer Genre Classifier overlap features (for feature selection)")
    
    # Other
    parser.add_argument("--random_state", type=int, default=42,
                       help="Random state for reproducibility")
    
    args = parser.parse_args()
    
    # Data source
    hf_repo = args.hf_repo
    data_source = f"Hugging Face ({hf_repo})"
    
    print("\n" + "="*80)
    print("LOGISTIC REGRESSION WITH RFECV")
    print("="*80)
    print(f"Groups: {', '.join(args.groups)}")
    print(f"Source: {data_source}")
    print(f"RFECV: step={args.rfecv_step}, cv={args.rfecv_cv}, scoring={args.rfecv_scoring}")
    print(f"LR: C={args.lr_C}, max_iter={args.lr_max_iter}")
    print(f"Oversample: {args.oversample}")
    print(f"Undersample: {args.undersample} (method: {args.undersample_method})")
    if args.undersample:
        print(f"Undersample clusters: {args.undersample_clusters}")
    print(f"Prefer GC overlap: {args.prefer_gc_overlap}")
    print("="*80)
    
    # Load data
    print("\nLoading training data...")
    X_train, y_train = load_feature_matrix(args.groups, "train", hf_repo)
    
    print("Loading validation data...")
    X_val, y_val = load_feature_matrix(args.groups, "validation", hf_repo)
    
    print("Loading test data...")
    X_test, y_test = load_feature_matrix(args.groups, "test", hf_repo)
    
    print(f"\nâœ“ Training set: {len(X_train)} samples, {X_train.shape[1]} features")
    print(f"âœ“ Validation set: {len(X_val)} samples, {X_val.shape[1]} features")
    print(f"âœ“ Test set: {len(X_test)} samples, {X_test.shape[1]} features")
    
    # Align features
    train_cols = set(X_train.columns)
    val_cols = set(X_val.columns)
    test_cols = set(X_test.columns)
    common_cols = sorted(train_cols & val_cols & test_cols)
    
    if len(common_cols) < len(train_cols):
        print(f"\nâš  Feature mismatch detected:")
        print(f"  Train: {len(train_cols)} features")
        print(f"  Val: {len(val_cols)} features")
        print(f"  Test: {len(test_cols)} features")
        print(f"  Common: {len(common_cols)} features")
        print(f"  Using only common features across all splits")
        X_train = X_train[common_cols]
        X_val = X_val[common_cols]
        X_test = X_test[common_cols]
        print(f"âœ“ Aligned to {len(common_cols)} common features")
    
    # Apply prefer_gc_overlap filter
    if args.prefer_gc_overlap:
        print("\nApplying prefer_gc_overlap filter...")
        # Identify duplicate features (same across gc_ prefixes)
        gc_overlap_features = set()
        base_to_gc = {}
        
        for col in common_cols:
            if col.startswith('gc_'):
                # Extract base name (e.g., gc_polarity_mean -> polarity_mean)
                parts = col.split('_', 2)
                if len(parts) >= 3:
                    base_name = parts[2]
                    if base_name not in base_to_gc:
                        base_to_gc[base_name] = []
                    base_to_gc[base_name].append(col)
        
        # For duplicates, keep only the first gc_ version
        features_to_remove = set()
        for base_name, gc_versions in base_to_gc.items():
            if len(gc_versions) > 1:
                # Keep first, remove others
                features_to_remove.update(gc_versions[1:])
        
        if features_to_remove:
            print(f"  Removing {len(features_to_remove)} duplicate GC features")
            common_cols = [c for c in common_cols if c not in features_to_remove]
            X_train = X_train[common_cols]
            X_val = X_val[common_cols]
            X_test = X_test[common_cols]
            print(f"  âœ“ {len(common_cols)} features after deduplication")
    
    feature_names = common_cols
    
    # Original class distribution
    print("\nOriginal class distribution:")
    print(f"  Train: {np.bincount(y_train.astype(int))}")
    print(f"  Val:   {np.bincount(y_val.astype(int))}")
    print(f"  Test:  {np.bincount(y_test.astype(int))}")
    
    # Preprocessing (imputation + scaling)
    print("\nPreprocessing (imputation + scaling)...")
    preproc = Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler()),
    ])
    
    X_train_std = preproc.fit_transform(X_train)
    X_val_std = preproc.transform(X_val)
    X_test_std = preproc.transform(X_test)
    
    print("âœ“ Preprocessing complete")
    
    # Apply sampling
    X_train_std, y_train = apply_sampling(X_train_std, y_train, args)
    
    print(f"\nFinal training set: {len(X_train_std)} samples")
    print(f"Final class distribution: {np.bincount(y_train.astype(int))}")
    
    # Train LR with RFECV
    print("\n" + "="*80)
    print("TRAINING LOGISTIC REGRESSION WITH RFECV")
    print("="*80)
    
    print(f"\nBase model: LogisticRegression(C={args.lr_C}, max_iter={args.lr_max_iter}, class_weight='balanced')")
    print(f"RFECV settings:")
    print(f"  - CV folds: {args.rfecv_cv}")
    print(f"  - Scoring: {args.rfecv_scoring}")
    print(f"  - Step: {args.rfecv_step}")
    print(f"  - Min features: {args.rfecv_min_features}")
    
    # Create base estimator
    base_lr = LogisticRegression(
        C=args.lr_C,
        max_iter=args.lr_max_iter,
        class_weight="balanced",
        random_state=args.random_state
    )
    
    # Create RFECV
    rfecv = RFECV(
        estimator=base_lr,
        step=args.rfecv_step,
        cv=StratifiedKFold(n_splits=args.rfecv_cv, shuffle=True, random_state=args.random_state),
        scoring=args.rfecv_scoring,
        min_features_to_select=args.rfecv_min_features,
        n_jobs=-1,
        verbose=1
    )
    
    print(f"\nRunning RFECV on {X_train_std.shape[1]} features...")
    print("This may take a while...\n")
    
    rfecv.fit(X_train_std, y_train)
    
    print(f"\nâœ“ RFECV complete!")
    print(f"  Optimal number of features: {rfecv.n_features_}")
    print(f"  Selected features: {rfecv.support_.sum()}")
    print(f"  Best CV score ({args.rfecv_scoring}): {rfecv.cv_results_['mean_test_score'].max():.4f}")
    
    # Get selected features
    selected_features = [f for f, selected in zip(feature_names, rfecv.support_) if selected]
    print(f"\nTop 20 selected features:")
    for i, feat in enumerate(selected_features[:20], 1):
        print(f"  {i:2d}. {feat}")
    
    # Transform data
    X_train_selected = rfecv.transform(X_train_std)
    X_val_selected = rfecv.transform(X_val_std)
    X_test_selected = rfecv.transform(X_test_std)
    
    # Get final model (refitted on all training data with selected features)
    final_lr = rfecv.estimator_
    
    # Find optimal threshold on validation set
    print("\nFinding optimal threshold on validation set...")
    val_proba = final_lr.predict_proba(X_val_selected)[:, 1]
    optimal_threshold, best_f1 = find_optimal_threshold(
        y_val, val_proba, metric="macro"
    )
    print(f"âœ“ Optimal threshold: {optimal_threshold:.4f} (macro F1: {best_f1:.4f})")
    
    # Validation metrics
    val_pred = (val_proba >= optimal_threshold).astype(int)
    val_accuracy = accuracy_score(y_val, val_pred)
    val_f1_binary = f1_score(y_val, val_pred, average='binary')
    val_f1_macro = f1_score(y_val, val_pred, average='macro')
    val_roc_auc = roc_auc_score(y_val, val_proba)
    
    print("\n" + "="*80)
    print("VALIDATION SET RESULTS")
    print("="*80)
    print(f"Accuracy:    {val_accuracy:.4f}")
    print(f"F1 (binary): {val_f1_binary:.4f}")
    print(f"F1 (macro):  {val_f1_macro:.4f} â† OPTIMIZED")
    print(f"ROC-AUC:     {val_roc_auc:.4f}")
    
    # Test metrics
    print("\nEvaluating on test set...")
    test_proba = final_lr.predict_proba(X_test_selected)[:, 1]
    test_pred = (test_proba >= optimal_threshold).astype(int)
    
    test_accuracy = accuracy_score(y_test, test_pred)
    test_f1_binary = f1_score(y_test, test_pred, average='binary')
    test_f1_macro = f1_score(y_test, test_pred, average='macro')
    test_roc_auc = roc_auc_score(y_test, test_proba)
    
    print("\n" + "="*80)
    print("TEST SET RESULTS")
    print("="*80)
    print(f"Accuracy:    {test_accuracy:.4f}")
    print(f"F1 (binary): {test_f1_binary:.4f}")
    print(f"F1 (macro):  {test_f1_macro:.4f} â† OPTIMIZED")
    print(f"ROC-AUC:     {test_roc_auc:.4f}")
    
    # Detailed classification reports
    print("\n" + "-"*80)
    print("VALIDATION SET - DETAILED CLASSIFICATION REPORT")
    print("-"*80)
    print(classification_report(y_val, val_pred, target_names=["Non-salient", "Salient"]))
    
    print("\n" + "-"*80)
    print("TEST SET - DETAILED CLASSIFICATION REPORT")
    print("-"*80)
    print(classification_report(y_test, test_pred, target_names=["Non-salient", "Salient"]))
    
    # Feature coefficients
    print("\n" + "="*80)
    print("MODEL COEFFICIENTS (Selected Features)")
    print("="*80)
    
    coefs = final_lr.coef_[0]
    coef_abs = np.abs(coefs)
    top_idx = np.argsort(coef_abs)[::-1][:20]
    
    print("\nTop 20 features by absolute coefficient:")
    for i, idx in enumerate(top_idx, 1):
        print(f"  {i:2d}. {selected_features[idx]:50s}: {coefs[idx]:+.6f}")
    
    # Save results
    results = {
        'rfecv': rfecv,
        'model': final_lr,
        'preprocessor': preproc,
        'selected_features': selected_features,
        'feature_names': feature_names,
        'n_features_selected': rfecv.n_features_,
        'support': rfecv.support_,
        'cv_results': rfecv.cv_results_,
        'threshold': optimal_threshold,
        'val_accuracy': val_accuracy,
        'val_f1_binary': val_f1_binary,
        'val_f1_macro': val_f1_macro,
        'val_roc_auc': val_roc_auc,
        'test_accuracy': test_accuracy,
        'test_f1_binary': test_f1_binary,
        'test_f1_macro': test_f1_macro,
        'test_roc_auc': test_roc_auc,
        'val_y_pred': val_pred,
        'val_y_proba': val_proba,
        'test_y_pred': test_pred,
        'test_y_proba': test_proba,
        'coefficients': dict(zip(selected_features, coefs)),
        'args': vars(args),
    }
    
    print(f"\nSaving results to: {args.output}")
    with open(args.output, 'wb') as f:
        pickle.dump(results, f)
    print("âœ“ Results saved")
    
    # Save selected features to text file
    features_output = args.output.replace('.pkl', '_selected_features.txt')
    with open(features_output, 'w') as f:
        f.write(f"# Selected Features (n={len(selected_features)})\n")
        f.write(f"# RFECV settings: step={args.rfecv_step}, cv={args.rfecv_cv}, scoring={args.rfecv_scoring}\n")
        f.write(f"# Best CV score: {rfecv.cv_results_['mean_test_score'].max():.4f}\n\n")
        for feat in selected_features:
            f.write(f"{feat}\n")
    print(f"âœ“ Selected features saved to: {features_output}")
    
    # Save coefficients
    coef_output = args.output.replace('.pkl', '_coefficients.txt')
    with open(coef_output, 'w') as f:
        f.write("Feature\tCoefficient\n")
        for feat, coef in sorted(zip(selected_features, coefs), key=lambda x: abs(x[1]), reverse=True):
            f.write(f"{feat}\t{coef:+.8f}\n")
    print(f"âœ“ Coefficients saved to: {coef_output}")
    
    print("\n" + "="*80)
    print("TRAINING COMPLETE")
    print("="*80)


if __name__ == "__main__":
    main()

================
File: train_pca_ensemble.py
================
#!/usr/bin/env python
"""
PCA Ensemble Training with Full Feature Control

Combines:
- Cached feature loading
- Oversampling (SMOTE)
- Undersampling (cluster-based or random)
- PCA ensemble (voting or stacking)
- Feature selection and reporting

Usage:
    # Basic usage
    python train_pca_ensemble.py --groups base gc_all emotional ngram
    
    # With sampling
    python train_pca_ensemble.py \
        --groups base gc_all emotional ngram \
        --oversample --undersample --undersample_method cluster_random
    
    # Stacking instead of voting
    python train_pca_ensemble.py --groups base gc_all --method stacking
"""

import argparse
import pickle
import numpy as np
from pathlib import Path
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.cluster import KMeans
from sklearn.metrics import classification_report, f1_score, accuracy_score, roc_auc_score

from feature_cache.load_hf import load_feature_matrix
from pca_utils import fit_pca, find_optimal_threshold
from pca_ensemble import search_ensemble_weights_dirichlet, search_ensemble_weights_simplex

try:
    from imblearn.over_sampling import SMOTE
    from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids
    HAS_IMBLEARN = True
except ImportError:
    HAS_IMBLEARN = False
    print("âš  Warning: imbalanced-learn not installed. Install with: pip install imbalanced-learn")


def apply_custom_cluster_undersampling(X_train, y_train, n_clusters=50, random_state=42):
    """
    Custom cluster-based undersampling matching train.py logic.
    Clusters majority class and samples proportionally from each cluster.
    """
    print(f"\nApplying custom cluster-based undersampling (cluster_random method):")
    print(f"  Strategy: K-Means clustering ({n_clusters} clusters) + random sampling within clusters")
    
    num_pos = int((y_train == 1).sum())
    num_neg = int((y_train == 0).sum())
    
    print(f"  Original distribution: {num_neg} non-salient, {num_pos} salient")
    
    if num_pos == 0 or num_neg == 0:
        return X_train, y_train
    
    # Determine minority and majority
    if num_pos <= num_neg:
        minority_label = 1
        majority_label = 0
        target_majority = num_pos
    else:
        minority_label = 0
        majority_label = 1
        target_majority = num_neg
    
    maj_idx = np.where(y_train == majority_label)[0]
    min_idx = np.where(y_train == minority_label)[0]
    X_maj = X_train[maj_idx]
    
    print(f"  Target: Reduce majority class from {len(maj_idx)} to {target_majority} (match minority)")
    
    # Cluster majority class
    k = int(max(1, min(n_clusters, len(maj_idx), target_majority)))
    print(f"\n  Step 1: K-Means clustering of {len(maj_idx)} majority scenes into {k} clusters...")
    
    try:
        km = KMeans(n_clusters=k, random_state=random_state, n_init=10)
        labels = km.fit_predict(X_maj)
        print(f"  âœ“ Clustering complete")
    except Exception:
        labels = np.zeros(len(maj_idx), dtype=int)
        k = 1
        print(f"  âš  Clustering failed, using single cluster")
    
    # Compute sampling quota per cluster
    cluster_sizes = np.bincount(labels, minlength=k).astype(np.int64)
    total_maj = int(len(maj_idx))
    
    print(f"\n  Step 2: Compute proportional sampling quota for each cluster...")
    print(f"    Cluster sizes (min/mean/max): {cluster_sizes.min()}/{cluster_sizes.mean():.0f}/{cluster_sizes.max()}")
    
    if total_maj > 0:
        raw_quota = (cluster_sizes / max(1, total_maj)) * float(target_majority)
        base_quota = np.floor(raw_quota).astype(int)
        remainder = int(target_majority - int(base_quota.sum()))
        frac = raw_quota - base_quota
        order = np.argsort(-frac)
        for i in range(min(remainder, len(order))):
            base_quota[order[i]] += 1
        
        print(f"    Sampling quotas per cluster (min/mean/max): {base_quota.min()}/{base_quota.mean():.0f}/{base_quota.max()}")
        
        # Sample from each cluster
        print(f"\n  Step 3: Random sampling within each cluster...")
        chosen = []
        rng = np.random.RandomState(random_state)
        for c in range(k):
            members = np.where(labels == c)[0]
            take = int(min(base_quota[c], len(members)))
            if take > 0:
                chosen_members = rng.choice(members, size=take, replace=False)
                chosen.append(chosen_members)
        
        if len(chosen):
            chosen_idx_rel = np.concatenate(chosen, axis=0)
            chosen_idx_abs = maj_idx[chosen_idx_rel]
            keep_idx = np.concatenate([min_idx, chosen_idx_abs], axis=0)
            X_train = X_train[keep_idx]
            y_train = y_train[keep_idx]
            
            final_neg = (y_train == 0).sum()
            final_pos = (y_train == 1).sum()
            print(f"  âœ“ Sampling complete: {final_neg} non-salient, {final_pos} salient")
            print(f"    Reduction: {len(maj_idx)} â†’ {final_neg} ({100*final_neg/len(maj_idx):.1f}%)")
    
    return X_train, y_train


def apply_sampling(X_train, y_train, args):
    """
    Apply data balancing matching train.py exactly.
    
    For cluster_random: Undersample first (custom), then SMOTE
    For other methods: SMOTE first, then undersample (via pipeline behavior)
    """
    if not HAS_IMBLEARN:
        print("\nâš  Cannot apply sampling: imbalanced-learn not installed")
        return X_train, y_train
    
    original_counts = np.bincount(y_train.astype(int))
    print(f"\nOriginal class distribution: {original_counts}")
    
    # Special handling for cluster_random (matches train.py)
    if args.undersample and args.undersample_method == "cluster_random":
        X_train, y_train = apply_custom_cluster_undersampling(
            X_train, y_train,
            n_clusters=args.undersample_clusters,
            random_state=args.random_state
        )
        after_under = np.bincount(y_train.astype(int))
        print(f"\nâœ“ After cluster_random undersampling: {after_under}")
        
        # Then optionally oversample
        if args.oversample:
            print(f"\nApplying SMOTE oversampling...")
            smote = SMOTE(random_state=args.random_state)
            X_train, y_train = smote.fit_resample(X_train, y_train)
            after_over = np.bincount(y_train.astype(int))
            print(f"âœ“ After SMOTE: {after_over}")
    
    # For other methods: SMOTE first, then undersample (matches train.py pipeline)
    else:
        # Oversample first (if requested)
        if args.oversample:
            print(f"\nApplying SMOTE oversampling...")
            smote = SMOTE(random_state=args.random_state)
            X_train, y_train = smote.fit_resample(X_train, y_train)
            after_over = np.bincount(y_train.astype(int))
            print(f"âœ“ After SMOTE: {after_over}")
        
        # Undersample second (if requested)
        if args.undersample:
            method_desc = {
                'random': 'Random undersampling (RandomUnderSampler)',
                'cluster': 'Cluster centroids (ClusterCentroids via K-Means)'
            }
            print(f"\nApplying undersampling (method: {args.undersample_method})...")
            print(f"  {method_desc.get(args.undersample_method, args.undersample_method)}")
            
            if args.undersample_method == "random":
                undersampler = RandomUnderSampler(random_state=args.random_state)
            elif args.undersample_method == "cluster":
                undersampler = ClusterCentroids(
                    estimator=KMeans(n_clusters=args.undersample_clusters, random_state=args.random_state, n_init=10),
                    random_state=args.random_state
                )
            else:
                print(f"âš  Unknown undersample method: {args.undersample_method}, skipping")
                return X_train, y_train
            
            X_train, y_train = undersampler.fit_resample(X_train, y_train)
            after_under = np.bincount(y_train.astype(int))
            print(f"âœ“ After undersampling: {after_under}")
    
    return X_train, y_train


def main():
    parser = argparse.ArgumentParser(
        description="Advanced PCA Ensemble Training with Cached Features"
    )
    # Feature groups
    parser.add_argument("--groups", nargs="+", required=True,
                       help="Feature groups to use (e.g., base gc_all emotional)")
    parser.add_argument("--hf_repo", default="Ishaank18/screenplay-features",
                       help="Hugging Face repo for features (default: Ishaank18/screenplay-features)")
    
    # Output
    parser.add_argument("--output", type=str,
                       default="/scratch/ishaan.karan/pca_ensemble_advanced.pkl",
                       help="Output file for results")
    
    # Ensemble method
    parser.add_argument("--method", type=str, default="voting",
                       choices=["voting", "stacking"],
                       help="Ensemble method: voting (weighted) or stacking (meta-learner)")
    
    # PCA settings
    parser.add_argument("--pca_n_components", type=float, default=0.95,
                       help="PCA components (float for variance ratio, int for exact number)")
    parser.add_argument("--search", type=str, default="dirichlet",
                       choices=["dirichlet", "simplex"],
                       help="Weight search method for voting ensemble")
    parser.add_argument("--stacking_folds", type=int, default=5,
                       help="Number of folds for stacking ensemble")
    
    # Sampling
    parser.add_argument("--oversample", action="store_true",
                       help="Use SMOTE oversampling")
    parser.add_argument("--undersample", action="store_true",
                       help="Use undersampling")
    parser.add_argument("--undersample_method", type=str, default="cluster_random",
                       choices=["random", "cluster", "cluster_random"],
                       help="Undersampling method")
    parser.add_argument("--undersample_clusters", type=int, default=50,
                       help="Number of clusters for cluster-based undersampling")
    
    # Feature preferences
    parser.add_argument("--prefer_gc_overlap", action="store_true",
                       help="Prefer Genre Classifier overlap features (for feature selection)")
    
    # General settings
    parser.add_argument("--random_state", type=int, default=42,
                       help="Random seed")
    parser.add_argument("--report_features", action="store_true",
                       help="Report feature importance")
    
    args = parser.parse_args()
    
    # Data source
    hf_repo = args.hf_repo
    data_source = f"Hugging Face ({hf_repo})"
    
    print("\n" + "="*80)
    print("ADVANCED PCA ENSEMBLE TRAINING")
    print("="*80)
    print(f"Groups: {', '.join(args.groups)}")
    print(f"Source: {data_source}")
    print(f"Method: {args.method}")
    print(f"PCA components: {args.pca_n_components}")
    print(f"Oversample: {args.oversample}")
    print(f"Undersample: {args.undersample} (method: {args.undersample_method})")
    if args.undersample:
        print(f"Undersample clusters: {args.undersample_clusters}")
    print(f"Prefer GC overlap: {args.prefer_gc_overlap}")
    print("="*80 + "\n")
    
    # Load cached features
    print("Loading training data...")
    X_train, y_train = load_feature_matrix(args.groups, "train", hf_repo)
    
    print("Loading validation data...")
    X_val, y_val = load_feature_matrix(args.groups, "validation", hf_repo)
    
    print("Loading test data...")
    X_test, y_test = load_feature_matrix(args.groups, "test", hf_repo)
    
    print(f"\nâœ“ Training set: {len(X_train)} samples, {len(X_train.columns)} features")
    print(f"âœ“ Validation set: {len(X_val)} samples, {len(X_val.columns)} features")
    print(f"âœ“ Test set: {len(X_test)} samples, {len(X_test.columns)} features")
    
    # Align features across splits (use intersection of columns)
    train_cols = set(X_train.columns)
    val_cols = set(X_val.columns)
    test_cols = set(X_test.columns)
    
    common_cols = train_cols & val_cols & test_cols
    
    if len(common_cols) < len(train_cols):
        print(f"\nâš  Feature mismatch detected:")
        print(f"  Train: {len(train_cols)} features")
        print(f"  Val: {len(val_cols)} features")
        print(f"  Test: {len(test_cols)} features")
        print(f"  Common: {len(common_cols)} features")
        print(f"  Using only common features across all splits")
        
        # Keep only common columns in same order
        common_cols = sorted(common_cols)
        X_train = X_train[common_cols]
        X_val = X_val[common_cols]
        X_test = X_test[common_cols]
        
        print(f"âœ“ Aligned to {len(common_cols)} common features")
    
    # Save feature names
    feature_names = X_train.columns.tolist()
    
    # Remove duplicate features (GC versions that duplicate non-GC features)
    duplicate_gc_features = [
        'gc_flesch_reading_ease',      # Duplicate of flesch_reading_ease
        'gc_gunning_fog_index',         # Duplicate of gunning_fog
    ]
    
    features_before = len(feature_names)
    feature_names = [f for f in feature_names if f not in duplicate_gc_features]
    
    if features_before > len(feature_names):
        removed = features_before - len(feature_names)
        print(f"\nâœ“ Removed {removed} duplicate GC features")
        X_train = X_train[[f for f in X_train.columns if f not in duplicate_gc_features]]
        X_val = X_val[[f for f in X_val.columns if f not in duplicate_gc_features]]
        X_test = X_test[[f for f in X_test.columns if f not in duplicate_gc_features]]
    
    # Feature filtering based on prefer_gc_overlap
    if args.prefer_gc_overlap:
        print("\nApplying prefer_gc_overlap filter...")
        gc_overlap_cols = [c for c in feature_names if "overlap" in c.lower() and c.startswith("gc_")]
        if gc_overlap_cols:
            print(f"Found {len(gc_overlap_cols)} GC overlap features")
            # Keep GC overlap features and all non-GC features
            keep_cols = gc_overlap_cols + [c for c in feature_names if not c.startswith("gc_")]
            X_train = X_train[keep_cols]
            X_val = X_val[keep_cols]
            X_test = X_test[keep_cols]
            feature_names = keep_cols
            print(f"Filtered to {len(feature_names)} features")
    
    # Convert to numpy
    X_train = X_train.values
    X_val = X_val.values
    X_test = X_test.values
    y_train = y_train.values
    y_val = y_val.values
    y_test = y_test.values
    
    print(f"\nOriginal class distribution:")
    print(f"  Train: {np.bincount(y_train.astype(int))}")
    print(f"  Val:   {np.bincount(y_val.astype(int))}")
    print(f"  Test:  {np.bincount(y_test.astype(int))}")
    
    # Preprocess
    print("\nPreprocessing (imputation + scaling)...")
    preproc = Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler()),
    ])
    
    X_train_std = preproc.fit_transform(X_train)
    X_val_std = preproc.transform(X_val)
    X_test_std = preproc.transform(X_test)
    
    print("âœ“ Preprocessing complete")
    
    # Apply sampling
    X_train_std, y_train = apply_sampling(X_train_std, y_train, args)
    
    print(f"\nFinal training set: {len(X_train_std)} samples")
    print(f"Final class distribution: {np.bincount(y_train.astype(int))}")
    
    if args.method == "voting":
        # Fit PCA
        print(f"\nFitting PCA (target: {args.pca_n_components})...")
        from sklearn.decomposition import PCA
        from sklearn.calibration import CalibratedClassifierCV
        
        pca = PCA(n_components=args.pca_n_components, random_state=args.random_state)
        X_train_pca = pca.fit_transform(X_train_std)
        X_val_pca = pca.transform(X_val_std)
        X_test_pca = pca.transform(X_test_std)
        
        print(f"âœ“ PCA fitted with {pca.n_components_} components")
        print(f"âœ“ Explained variance: {pca.explained_variance_ratio_.sum():.4f}")
        
        # Train 4-model ensemble matching pca_analysis.py
        print(f"\nTraining 4-model ensemble (matching pca_analysis.py)...")
        print("=" * 80)
        print("Models:")
        print("  1. Logistic Regression (standard features, C=0.3)")
        print("  2. Logistic Regression (PCA features, C=0.1)")
        print("  3. Calibrated LinearSVC (standard features, C=1.0)")
        print("  4. Random Forest (standard features, 300 trees)")
        print("=" * 80)
        
        # Define 4 models (exactly as in pca_analysis.py)
        from sklearn.linear_model import LogisticRegression
        from sklearn.svm import LinearSVC
        from sklearn.ensemble import RandomForestClassifier
        
        base_lr = LogisticRegression(max_iter=1000, class_weight="balanced", C=0.3, random_state=args.random_state)
        pca_lr = LogisticRegression(max_iter=1000, class_weight="balanced", C=0.1, random_state=args.random_state)
        svc = LinearSVC(class_weight="balanced", C=1.0, random_state=args.random_state)
        svc_cal = CalibratedClassifierCV(svc, method="sigmoid", cv=2)
        rf = RandomForestClassifier(
            n_estimators=300,
            max_depth=None,
            min_samples_split=2,
            class_weight="balanced_subsample",
            n_jobs=-1,
            random_state=args.random_state,
        )
        
        # Train models
        print("\nTraining base_lr (standard features)...")
        base_lr.fit(X_train_std, y_train)
        
        print("Training pca_lr (PCA features)...")
        pca_lr.fit(X_train_pca, y_train)
        
        print("Training svc (calibrated, standard features)...")
        svc_cal.fit(X_train_std, y_train)
        
        print("Training rf (standard features)...")
        rf.fit(X_train_std, y_train)
        
        # Get validation probabilities
        print("\nComputing validation probabilities...")
        val_probs = {
            "base_lr": base_lr.predict_proba(X_val_std)[:, 1],
            "pca_lr": pca_lr.predict_proba(X_val_pca)[:, 1],
            "svc": svc_cal.predict_proba(X_val_std)[:, 1],
            "rf": rf.predict_proba(X_val_std)[:, 1],
        }
        
        model_names = list(val_probs.keys())
        P = np.vstack([val_probs[n] for n in model_names])
        
        # Search for optimal weights using specified method
        print(f"\nSearching optimal weights ({args.search} method)...")
        print(f"Optimizing for: macro F1 (both classes weighted equally)")
        
        if args.search == "dirichlet":
            search_results = search_ensemble_weights_dirichlet(
                P, y_val, 
                threshold_finder_func=find_optimal_threshold,
                n_trials=500, 
                metric="macro",
                random_state=args.random_state
            )
            best_weights = search_results['weights']
            best_threshold = search_results['threshold']
        else:  # simplex
            search_results = search_ensemble_weights_simplex(
                P, y_val,
                threshold_finder_func=find_optimal_threshold,
                step=0.1,
                metric="macro"
            )
            best_weights = search_results['weights']
            best_threshold = search_results['threshold']
        
        # Compute validation metrics with best weights
        val_proba = (best_weights @ P)
        val_pred = (val_proba >= best_threshold).astype(int)
        
        val_accuracy = accuracy_score(y_val, val_pred)
        val_f1_binary = f1_score(y_val, val_pred, average='binary')
        val_f1_macro = f1_score(y_val, val_pred, average='macro')
        val_roc_auc = roc_auc_score(y_val, val_proba)
        
        print("\n" + "=" * 80)
        print("ENSEMBLE RESULTS (OPTIMIZED FOR MACRO F1)")
        print("=" * 80)
        print(f"Models: {model_names}")
        print(f"Optimal weights: {[f'{w:.3f}' for w in best_weights]}")
        print(f"Optimal threshold: {best_threshold:.4f} (maximizes macro F1)")
        print(f"Accuracy: {val_accuracy:.4f}")
        print(f"F1 (binary): {val_f1_binary:.4f}")
        print(f"F1 (macro): {val_f1_macro:.4f} â† OPTIMIZED FOR THIS")
        print(f"ROC-AUC: {val_roc_auc:.4f}")
        print("=" * 80 + "\n")
        
        # Evaluate on test set
        print(f"\nEvaluating on test set...")
        test_probs = {
            "base_lr": base_lr.predict_proba(X_test_std)[:, 1],
            "pca_lr": pca_lr.predict_proba(X_test_pca)[:, 1],
            "svc": svc_cal.predict_proba(X_test_std)[:, 1],
            "rf": rf.predict_proba(X_test_std)[:, 1],
        }
        
        P_test = np.vstack([test_probs[n] for n in model_names])
        test_proba = (best_weights @ P_test)
        test_pred = (test_proba >= best_threshold).astype(int)
        
        test_accuracy = accuracy_score(y_test, test_pred)
        test_f1_binary = f1_score(y_test, test_pred, average='binary')
        test_f1_macro = f1_score(y_test, test_pred, average='macro')
        test_roc_auc = roc_auc_score(y_test, test_proba)
        
        # Package results
        results = {
            'models': {
                'base_lr': base_lr,
                'pca_lr': pca_lr,
                'svc': svc_cal,
                'rf': rf,
            },
            'model_names': model_names,
            'weights': best_weights,
            'threshold': best_threshold,
            'pca': pca,
            'method': 'voting',
            'search_method': args.search,
            'accuracy': val_accuracy,
            'f1_binary': val_f1_binary,
            'f1_macro': val_f1_macro,
            'roc_auc': val_roc_auc,
            'y_pred': val_pred,
            'y_proba': val_proba,
            'test_accuracy': test_accuracy,
            'test_f1_binary': test_f1_binary,
            'test_f1_macro': test_f1_macro,
            'test_roc_auc': test_roc_auc,
            'test_y_pred': test_pred,
            'test_y_proba': test_proba,
        }
    
    # Save results
    results["preprocessor"] = preproc
    results["feature_names"] = feature_names
    results["feature_groups"] = args.groups
    results["hf_repo"] = args.hf_repo
    results["sampling"] = {
        "oversample": args.oversample,
        "undersample": args.undersample,
        "undersample_method": args.undersample_method,
        "undersample_clusters": args.undersample_clusters,
    }
    results["args"] = vars(args)
    
    print(f"\nSaving results to: {args.output}")
    with open(args.output, "wb") as f:
        pickle.dump(results, f)
    
    print("âœ“ Results saved")
    
    # Print final summary
    print("\n" + "="*80)
    print("TRAINING COMPLETE - RESULTS SUMMARY")
    print("="*80)
    print(f"Method: {args.method}")
    print(f"Optimization: macro F1 (both classes weighted equally)")
    print(f"Optimal threshold: {results['threshold']:.4f}")
    
    print("\n" + "-"*80)
    print("VALIDATION SET RESULTS")
    print("-"*80)
    print(f"Accuracy:    {results.get('accuracy', 0.0):.4f}")
    print(f"F1 (binary): {results.get('f1_binary', 0.0):.4f}")
    print(f"F1 (macro):  {results.get('f1_macro', 0.0):.4f} â† OPTIMIZED")
    print(f"ROC-AUC:     {results.get('roc_auc', 0.0):.4f}")
    
    print("\n" + "-"*80)
    print("TEST SET RESULTS")
    print("-"*80)
    print(f"Accuracy:    {results.get('test_accuracy', 0.0):.4f}")
    print(f"F1 (binary): {results.get('test_f1_binary', 0.0):.4f}")
    print(f"F1 (macro):  {results.get('test_f1_macro', 0.0):.4f} â† OPTIMIZED")
    print(f"ROC-AUC:     {results.get('test_roc_auc', 0.0):.4f}")
    
    if args.method == "voting" and args.report_features:
        print("\n" + "-"*80)
        print("ENSEMBLE MODEL WEIGHTS")
        print("-"*80)
        for model_name, weight in zip(results['model_names'], results['weights']):
            print(f"  {model_name:30s}: {weight:.4f}")
    
    # Detailed classification reports
    print("\n" + "-"*80)
    print("VALIDATION SET - DETAILED CLASSIFICATION REPORT")
    print("-"*80)
    y_pred = results.get('y_pred')
    if y_pred is not None:
        print(classification_report(y_val, y_pred, target_names=["Non-salient", "Salient"]))
    else:
        print("Predictions not available")
    
    print("\n" + "-"*80)
    print("TEST SET - DETAILED CLASSIFICATION REPORT")
    print("-"*80)
    y_test_pred = results.get('test_y_pred')
    if y_test_pred is not None:
        print(classification_report(y_test, y_test_pred, target_names=["Non-salient", "Salient"]))
    else:
        print("Predictions not available")
    
    print("="*80)
    
    # Feature importance for PCA (if voting method)
    if args.method == "voting" and args.report_features and len(feature_names) <= 200:
        print("\nTop 20 Features by PCA Component Importance:")
        print("-" * 80)
        
        # Get top features from first few PCA components
        pca_obj = results.get("pca")
        if pca_obj is not None:
            components = pca_obj.components_[:min(5, pca_obj.n_components_)]
            
            # Calculate feature importance as sum of absolute loadings
            importance = np.abs(components).sum(axis=0)
            top_idx = np.argsort(importance)[-20:][::-1]
            
            for i, idx in enumerate(top_idx, 1):
                print(f"  {i:2d}. {feature_names[idx]:50s}: {importance[idx]:.4f}")
    
    # Extract and display model coefficients
    print("\n" + "="*80)
    print("MODEL COEFFICIENTS")
    print("="*80)
    
    if args.method == "voting":
        model_dict = results['models']
        model_names = results['model_names']
        weights = results['weights']
        pca_obj = results.get("pca")
        
        # Extract coefficients from each model
        for model_name, weight in zip(model_names, weights):
            model = model_dict[model_name]
            print(f"\n{model_name} (weight: {weight:.4f}):")
            print("-" * 80)
            
            if hasattr(model, 'coef_'):
                coefs = model.coef_[0] if len(model.coef_.shape) > 1 else model.coef_
                
                # If PCA model, transform coefficients back to original feature space
                if '_pca' in model_name and pca_obj is not None:
                    # Transform PCA coefficients back to original space
                    # PCA inverse_transform needs 2D array, so reshape
                    coefs_2d = coefs.reshape(1, -1)
                    original_coefs = pca_obj.inverse_transform(coefs_2d).flatten()
                    
                    # Show top features by absolute coefficient value
                    top_idx = np.argsort(np.abs(original_coefs))[-20:][::-1]
                    print(f"Top 20 features (transformed from PCA space):")
                    for i, idx in enumerate(top_idx, 1):
                        print(f"  {i:2d}. {feature_names[idx]:50s}: {original_coefs[idx]:+.6f}")
                else:
                    # Direct feature space coefficients
                    top_idx = np.argsort(np.abs(coefs))[-20:][::-1]
                    print(f"Top 20 features:")
                    for i, idx in enumerate(top_idx, 1):
                        print(f"  {i:2d}. {feature_names[idx]:50s}: {coefs[idx]:+.6f}")
            else:
                print(f"  Model {model_name} does not have coefficients (non-linear model)")
        
        # Compute weighted ensemble coefficients (for linear models)
        print("\n" + "-"*80)
        print("WEIGHTED ENSEMBLE COEFFICIENTS")
        print("-"*80)
        
        ensemble_coefs = np.zeros(len(feature_names))
        total_weight = 0.0
        
        for model_name, weight in zip(model_names, weights):
            model = model_dict[model_name]
            if hasattr(model, 'coef_'):
                coefs = model.coef_[0] if len(model.coef_.shape) > 1 else model.coef_
                
                if '_pca' in model_name and pca_obj is not None:
                    # Transform PCA coefficients back to original space
                    # PCA inverse_transform needs 2D array, so reshape
                    coefs_2d = coefs.reshape(1, -1)
                    original_coefs = pca_obj.inverse_transform(coefs_2d).flatten()
                    ensemble_coefs += weight * original_coefs
                else:
                    # Ensure coefs matches feature_names length
                    if len(coefs) == len(feature_names):
                        ensemble_coefs += weight * coefs
                    else:
                        print(f"  Warning: {model_name} has {len(coefs)} coefficients, expected {len(feature_names)}, skipping")
                
                total_weight += weight
        
        if total_weight > 0:
            # Show top features by absolute ensemble coefficient value
            top_idx = np.argsort(np.abs(ensemble_coefs))[-20:][::-1]
            print(f"Top 20 features by weighted ensemble coefficient:")
            for i, idx in enumerate(top_idx, 1):
                print(f"  {i:2d}. {feature_names[idx]:50s}: {ensemble_coefs[idx]:+.6f}")
            
            # Save ensemble coefficients to file
            # Add GPU/process ID prefix to avoid conflicts when running on multiple GPUs
            import os
            gpu_id = os.environ.get('CUDA_VISIBLE_DEVICES', os.environ.get('GPU_ID', '0'))
            base_output = args.output.replace('.pkl', '')
            coef_output = f"{base_output}_gpu{gpu_id}_coefficients.txt"
            print(f"\nSaving all ensemble coefficients to: {coef_output}")
            with open(coef_output, 'w') as f:
                f.write("Feature\tCoefficient\n")
                for fname, coef in sorted(zip(feature_names, ensemble_coefs), 
                                         key=lambda x: abs(x[1]), reverse=True):
                    f.write(f"{fname}\t{coef:+.8f}\n")
            print("âœ“ Coefficients saved")
            
            # Add to results dict
            results['ensemble_coefficients'] = dict(zip(feature_names, ensemble_coefs))
        else:
            print("No linear models in ensemble - cannot compute ensemble coefficients")
    
    print("\n" + "="*80 + "\n")


if __name__ == "__main__":
    main()
